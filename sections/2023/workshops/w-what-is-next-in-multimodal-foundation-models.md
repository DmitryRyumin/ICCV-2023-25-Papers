# ICCVW-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/workshops/visual-inductive-priors-for-data-efficient-dl-w.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/workshops/w-and-challenge-on-deepfake-analysis-and-detection.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## What is Next in Multimodal Foundation Models?

![Section Papers](https://img.shields.io/badge/Section%20Papers-9-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-5-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-1-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Coarse to Fine Frame Selection for Online Open-Ended Video Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Nuthalapati_Coarse_to_Fine_Frame_Selection_for_Online_Open-Ended_Video_Question_ICCVW_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Qw81xaGh-O0) |
| Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Pan_Retrieving-to-Answer_Zero-Shot_Video_Question_Answering_with_Frozen_Large_Language_Models_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11732-b31b1b.svg)](https://arxiv.org/abs/2306.11732) | :heavy_minus_sign: |
| Video-and-Language (VidL) Models and their Cognitive Relevance | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Zonneveld_Video-and-Language_VidL_models_and_their_cognitive_relevance_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Video Attribute Prototype Network: A New Perspective for Zero-Shot Video Classification | [![GitHub](https://img.shields.io/github/stars/bobo199830/VAPNet?style=flat)](https://github.com/bobo199830/VAPNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Wang_Video_Attribute_Prototype_Network_A_New_Perspective_for_Zero-Shot_Video_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Interaction-Aware Prompting for Zero-Shot Spatio-Temporal Action Detection | [![GitHub](https://img.shields.io/github/stars/webber2933/iCLIP?style=flat)](https://github.com/webber2933/iCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Huang_Interaction-Aware_Prompting_for_Zero-Shot_Spatio-Temporal_Action_Detection_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04688-b31b1b.svg)](https://arxiv.org/abs/2304.04688) | :heavy_minus_sign: |
| ClipCrop: Conditioned Cropping Driven by Vision-Language Model | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Zhong_ClipCrop_Conditioned_Cropping_Driven_by_Vision-Language_Model_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11492-b31b1b.svg)](https://arxiv.org/abs/2211.11492) | :heavy_minus_sign: |
| Towards an Exhaustive Evaluation of Vision-Language Foundation Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Salin_Towards_an_Exhaustive_Evaluation_of_Vision-Language_Foundation_Models_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts | [![GitHub](https://img.shields.io/github/stars/mayug/VDT-Adapter?style=flat)](https://github.com/mayug/VDT-Adapter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Maniparambil_Enhancing_CLIP_with_GPT-4_Harnessing_Visual_Descriptions_as_Prompts_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11661-b31b1b.svg)](https://arxiv.org/abs/2307.11661) | :heavy_minus_sign: |
| Painter: Teaching Auto-Regressive Language Models to Draw Sketches | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/MMFM/papers/Pourreza_Painter_Teaching_Auto-Regressive_Language_Models_to_Draw_Sketches_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08520-b31b1b.svg)](https://arxiv.org/abs/2308.08520) | :heavy_minus_sign: |
