# ICCVW-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/workshops/w-on-new-ideas-in-vision-transformers.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/workshops/w-to-nerf-or-not-to-nerf.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Representation Learning with very Limited Images: The Potential of Self-, Synthetic- and Formula-Supervision

![Section Papers](https://img.shields.io/badge/Section%20Papers-20-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-12-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-1-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Image Guided Inpainting with Parameter Efficient Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Lim_Image_Guided_Inpainting_with_Parameter_Efficient_Learning_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Augmenting Features via Contrastive Learning-based Generative Model for Long-Tailed Classification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Park_Augmenting_Features_via_Contrastive_Learning-Based_Generative_Model_for_Long-Tailed_Classification_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| G2L: A High-Dimensional Geometric Approach for Automatic Generation of Highly Accurate Pseudo-Labels | [![GitHub](https://img.shields.io/github/stars/Hmic1102/Auto-generated-pseudo-label?style=flat)](https://github.com/Hmic1102/Auto-generated-pseudo-label) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Kender_G2L_A_High-Dimensional_Geometric_Approach_for_Automatic_Generation_of_Highly_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Self-Supervised Hypergraphs for Learning Multiple World Interpretations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Marcu_Self-Supervised_Hypergraphs_for_Learning_Multiple_World_Interpretations_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07615-b31b1b.svg)](https://arxiv.org/abs/2308.07615) | :heavy_minus_sign: |
| Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Kwarciak_Deep_Generative_Networks_for_Heterogeneous_Augmentation_of_Cranial_Defects_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04883-b31b1b.svg)](https://arxiv.org/abs/2308.04883) | :heavy_minus_sign: |
| 360&deg; from a Single Camera: A Few-Shot Approach for LiDAR Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Reichardt_360deg_from_a_Single_Camera_A_Few-Shot_Approach_for_LiDAR_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Adaptive Self-Training for Object Detection | [![GitHub](https://img.shields.io/github/stars/rvandeghen/ASTOD?style=flat)](https://github.com/rvandeghen/ASTOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Vandeghen_Adaptive_Self-Training_for_Object_Detection_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05911-b31b1b.svg)](https://arxiv.org/abs/2212.05911) | :heavy_minus_sign: |
| FedLID: Self-Supervised Federated Learning for Leveraging Limited Image Data | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Psaltis_FedLID_Self-Supervised_Federated_Learning_for_Leveraging_Limited_Image_Data_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| A Horse with no Labels: Self-Supervised Horse Pose Estimation from Unlabelled Images and Synthetic Prior | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Sosa_A_Horse_with_no_Labels_Self-Supervised_Horse_Pose_Estimation_from_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03411-b31b1b.svg)](https://arxiv.org/abs/2308.03411) | :heavy_minus_sign: |
| Boosting Semi-Supervised Learning by Bridging High and Low-Confidence Predictions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Nguyen_Boosting_Semi-Supervised_Learning_by_Bridging_high_and_low-Confidence_Predictions_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07509-b31b1b.svg)](https://arxiv.org/abs/2308.07509) | :heavy_minus_sign: |
| SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/Yussef93/SelectNAdaptICCVW?style=flat)](https://github.com/Yussef93/SelectNAdaptICCVW) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Dawoud_SelectNAdapt_Support_Set_Selection_for_Few-Shot_Domain_Adaptation_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04946-b31b1b.svg)](https://arxiv.org/abs/2308.04946) | :heavy_minus_sign: |
| MIAD: A Maintenance Inspection Dataset for Unsupervised Anomaly Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://miad-2022.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Bao_MIAD_A_Maintenance_Inspection_Dataset_for_Unsupervised_Anomaly_Detection_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13968-b31b1b.svg)](https://arxiv.org/abs/2211.13968) | :heavy_minus_sign: |
| Enhancing Classification Accuracy on Limited Data via Unconditional GAN | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Hong_Enhancing_Classification_Accuracy_on_Limited_Data_via_Unconditional_GAN_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Self-Training and Multi-Task Learning for Limited Data: Evaluation Study on Object Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://lhoangan.github.io/multas/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Le_Self-Training_and_Multi-Task_Learning_for_Limited_Data_Evaluation_Study_on_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06288-b31b1b.svg)](https://arxiv.org/abs/2309.06288) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=73we1N0azNk) |
| JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Bicsi_JEDI_Joint_Expert_Distillation_in_a_Semi-Supervised_Multi-Dataset_Student-Teacher_Scenario_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04934-b31b1b.svg)](https://arxiv.org/abs/2308.04934) | :heavy_minus_sign: |
| Semantic RGB-D Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Li_Semantic_RGB-D_Image_Synthesis_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11356-b31b1b.svg)](https://arxiv.org/abs/2308.11356) | :heavy_minus_sign: |
| Learning Universal Semantic Correspondences with No Supervision and Automatic Data Curation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Shtedritski_Learning_Universal_Semantic_Correspondences_with_No_Supervision_and_Automatic_Data_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Guiding Video Prediction with Explicit Procedural Knowledge | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Takenaka_Guiding_Video_Prediction_with_Explicit_Procedural_Knowledge_ICCVW_2023_paper.pdf) | :heavy_minus_sign: |
| Frequency-Aware Self-Supervised Long-Tailed Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Lin_Frequency-Aware_Self-Supervised_Long-Tailed_Learning_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.04723-b31b1b.svg)](https://arxiv.org/abs/2309.04723) | :heavy_minus_sign: |
| Tensor Factorization for Leveraging Cross-Modal Knowledge in Data-Constrained Infrared Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023W/LIMIT/papers/Sharma_Tensor_Factorization_for_Leveraging_Cross-Modal_Knowledge_in_Data-Constrained_Infrared_Object_ICCVW_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16592-b31b1b.svg)](https://arxiv.org/abs/2309.16592) | :heavy_minus_sign: |
