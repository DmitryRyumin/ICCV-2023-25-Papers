# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/biometrics.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/faces-and-gestures.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Datasets and Evaluation

![Section Papers](https://img.shields.io/badge/Section%20Papers-53-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-46-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-41-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-14-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| A Step Towards Understanding why Classification Helps Regression | [![GitHub](https://img.shields.io/github/stars/SilviaLauraPintea/reg-cls?style=flat)](https://github.com/SilviaLauraPintea/reg-cls) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pintea_A_step_towards_understanding_why_classification_helps_regression_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10603-b31b1b.svg)](https://arxiv.org/abs/2308.10603) | :heavy_minus_sign: |
| DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-Centric Rendering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dna-rendering.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/DNA-Rendering/DNA-Rendering?style=flat)](https://github.com/DNA-Rendering/DNA-Rendering) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DNA-Rendering_A_Diverse_Neural_Actor_Repository_for_High-Fidelity_Human-Centric_Rendering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10173-b31b1b.svg)](https://arxiv.org/abs/2307.10173) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xlhfvxvu7nc) |
| Robo3D: Towards Robust and Reliable 3D Perception against Corruptions | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ldkong.com/Robo3D) <br /> [![GitHub](https://img.shields.io/github/stars/ldkong1205/Robo3D?style=flat)](https://github.com/ldkong1205/Robo3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kong_Robo3D_Towards_Robust_and_Reliable_3D_Perception_against_Corruptions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17597-b31b1b.svg)](https://arxiv.org/abs/2303.17597) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kM8n-jMg0qw) |
| Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vsd-benchmark.github.io/vsd/) <br /> [![GitHub](https://img.shields.io/github/stars/vsd-benchmark/vsd?style=flat)](https://github.com/vsd-benchmark/vsd) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Barkan_Efficient_Discovery_and_Effective_Evaluation_of_Visual_Perceptual_Similarity_A_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14753-b31b1b.svg)](https://arxiv.org/abs/2308.14753) | :heavy_minus_sign: |
| DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://clarence-lee-sheng.github.io/DetermiNet/) <br /> [![GitHub](https://img.shields.io/github/stars/clarence-lee-sheng/DetermiNet?style=flat)](https://github.com/clarence-lee-sheng/DetermiNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_DetermiNet_A_Large-Scale_Diagnostic_Dataset_for_Complex_Visually-Grounded_Referencing_using_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03483-b31b1b.svg)](https://arxiv.org/abs/2309.03483) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rsTrUVL8yzM) |
| Beyond Object Recognition: A New Benchmark Towards Object Concept Learning | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mvig-rhos.com/ocl) <br /> [![GitHub](https://img.shields.io/github/stars/silicx/ObjectConceptLearning?style=flat)](https://github.com/silicx/ObjectConceptLearning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Beyond_Object_Recognition_A_New_Benchmark_towards_Object_Concept_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02710-b31b1b.svg)](https://arxiv.org/abs/2212.02710) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=NTyJmTzhfkE) |
| HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://eslambakr.github.io/hrsbench.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/eslambakr/HRS_benchmark?style=flat)](https://github.com/eslambakr/HRS_benchmark) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bakr_HRS-Bench_Holistic_Reliable_and_Scalable_Benchmark_for_Text-to-Image_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05390-b31b1b.svg)](https://arxiv.org/abs/2304.05390) | :heavy_minus_sign: |
| SegRCDB: Semantic Segmentation via Formula-Driven Supervised Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dahlian00.github.io/SegRCDBPage/) <br /> [![GitHub](https://img.shields.io/github/stars/dahlian00/SegRCDB?style=flat)](https://github.com/dahlian00/SegRCDB) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shinoda_SegRCDB_Semantic_Segmentation_via_Formula-Driven_Supervised_Learning_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5qj9_wQ_fQg) |
| LoTE-Animal: A Long Time-Span Dataset for Endangered Animal Behavior Understanding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lote-animal.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LoTE-Animal_A_Long_Time-span_Dataset_for_Endangered_Animal_Behavior_Understanding_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Building3D: A Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Building3D_A_Urban-Scale_Dataset_and_Benchmarks_for_Learning_Roof_Structures_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11914-b31b1b.svg)](https://arxiv.org/abs/2307.11914) | :heavy_minus_sign: |
| Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos | [![GitHub](https://img.shields.io/github/stars/dondongwon/LPMDataset?style=flat)](https://github.com/dondongwon/LPMDataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Lecture_Presentations_Multimodal_Dataset_Towards_Understanding_Multimodality_in_Educational_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.08080-b31b1b.svg)](https://arxiv.org/abs/2208.08080) | :heavy_minus_sign: |
| Probabilistic Precision and Recall Towards Reliable Evaluation of Generative Models | [![GitHub](https://img.shields.io/github/stars/kdst-team/Probablistic_precision_recall?style=flat)](https://github.com/kdst-team/Probablistic_precision_recall) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Probabilistic_Precision_and_Recall_Towards_Reliable_Evaluation_of_Generative_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01590-b31b1b.svg)](https://arxiv.org/abs/2309.01590) | :heavy_minus_sign: |
| EgoObjects: A Large-Scale Egocentric Dataset for Fine-Grained Object Understanding | [![GitHub](https://img.shields.io/github/stars/facebookresearch/EgoObjects?style=flat)](https://github.com/facebookresearch/EgoObjects) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_EgoObjects_A_Large-Scale_Egocentric_Dataset_for_Fine-Grained_Object_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08816-b31b1b.svg)](https://arxiv.org/abs/2309.08816) | :heavy_minus_sign: |
| CAME: Contrastive Automated Model Evaluation | [![GitHub](https://img.shields.io/github/stars/pengr/Contrastive_AutoEval?style=flat)](https://github.com/pengr/Contrastive_AutoEval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_CAME_Contrastive_Automated_Model_Evaluation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11111-b31b1b.svg)](https://arxiv.org/abs/2308.11111) | :heavy_minus_sign: |
| Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.projectaria.com/datasets/adt/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06362-b31b1b.svg)](https://arxiv.org/abs/2306.06362) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=rrnJQ5NQEiQ) |
| Exploring Video Quality Assessment on User Generated Contents from Aesthetic and Technical Perspectives | [![GitHub](https://img.shields.io/github/stars/VQAssessment/DOVER?style=flat)](https://github.com/VQAssessment/DOVER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Exploring_Video_Quality_Assessment_on_User_Generated_Contents_from_Aesthetic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.04894-b31b1b.svg)](https://arxiv.org/abs/2211.04894) | :heavy_minus_sign: |
| Going Beyond Nouns with Vision & Language Models using Synthetic Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://synthetic-vic.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/uvavision/SyViC?style=flat)](https://github.com/uvavision/SyViC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cascante-Bonilla_Going_Beyond_Nouns_With_Vision__Language_Models_Using_Synthetic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17590-b31b1b.svg)](https://arxiv.org/abs/2303.17590) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=dITNWLs35cQ) |
| H3WB: Human3.6M 3D WholeBody Dataset and Benchmark | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://vision.imar.ro/human3.6m/description.php) <br /> [![GitHub](https://img.shields.io/github/stars/wholebody3d/wholebody3d?style=flat)](https://github.com/wholebody3d/wholebody3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_H3WB_Human3.6M_3D_WholeBody_Dataset_and_Benchmark_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15692-b31b1b.svg)](https://arxiv.org/abs/2211.15692) | :heavy_minus_sign: |
| Zenseact Open Dataset: A Large-Scale and Diverse Multimodal Dataset for Autonomous Driving | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://zod.zenseact.com/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Alibeigi_Zenseact_Open_Dataset_A_Large-Scale_and_Diverse_Multimodal_Dataset_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02008-b31b1b.svg)](https://arxiv.org/abs/2305.02008) | :heavy_minus_sign: |
| CAD-Estate: Large-Scale CAD Model Annotation in RGB Videos | [![GitHub](https://img.shields.io/github/stars/google-research/cad-estate?style=flat)](https://github.com/google-research/cad-estate) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Maninis_CAD-Estate_Large-scale_CAD_Model_Annotation_in_RGB_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09011-b31b1b.svg)](https://arxiv.org/abs/2306.09011) | :heavy_minus_sign: |
| Neglected Free Lunch - Learning Image Classifiers using Annotation Byproducts | [![GitHub](https://img.shields.io/github/stars/naver-ai/NeglectedFreeLunch?style=flat)](https://github.com/naver-ai/NeglectedFreeLunch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Neglected_Free_Lunch_-_Learning_Image_Classifiers_Using_Annotation_Byproducts_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17595-b31b1b.svg)](https://arxiv.org/abs/2303.17595) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=9HEj3Km2TWo) |
| Chaotic World: A Large and Challenging Benchmark for Human Behavior Understanding in Chaotic Events | [![GitHub](https://img.shields.io/github/stars/sutdcv/Chaotic-World?style=flat)](https://github.com/sutdcv/Chaotic-World) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ong_Chaotic_World_A_Large_and_Challenging_Benchmark_for_Human_Behavior_ICCV_2023_paper.pdf) <br /> [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/373692522_Chaotic_World_A_Large_and_Challenging_Benchmark_for_Human_Behavior_Understanding_in_Chaotic_Events) | :heavy_minus_sign: |
| MOSE: A New Dataset for Video Object Segmentation in Complex Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/MOSE/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/MOSE-api?style=flat)](https://github.com/henghuiding/MOSE-api) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_MOSE_A_New_Dataset_for_Video_Object_Segmentation_in_Complex_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.01872-b31b1b.svg)](https://arxiv.org/abs/2302.01872) | :heavy_minus_sign: |
| Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet | [![GitHub](https://img.shields.io/github/stars/YanNeu/spurious_imagenet?style=flat)](https://github.com/YanNeu/spurious_imagenet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Neuhaus_Spurious_Features_Everywhere_-_Large-Scale_Detection_of_Harmful_Spurious_Features_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04871-b31b1b.svg)](https://arxiv.org/abs/2212.04871) | :heavy_minus_sign: |
| Chop & Learn: Recognizing and Generating Object-State Compositions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chopnlearn.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Saini_Chop__Learn_Recognizing_and_Generating_Object-State_Compositions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14339-b31b1b.svg)](https://arxiv.org/abs/2309.14339) | :heavy_minus_sign: |
| Building Bridge Across the Time: Disruption and Restoration of Murals in the Wild | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Building_Bridge_Across_the_Time_Disruption_and_Restoration_of_Murals_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| HoloAssist: An Egocentric Human Interaction Dataset for Interactive AI Assistants in the Real World | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://holoassist.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_HoloAssist_an_Egocentric_Human_Interaction_Dataset_for_Interactive_AI_Assistants_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.17024-b31b1b.svg)](https://arxiv.org/abs/2309.17024) | :heavy_minus_sign: |
| SynBody: Synthetic Dataset with Layered Human Models for 3D Human Perception and Modeling | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://synbody.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_SynBody_Synthetic_Dataset_with_Layered_Human_Models_for_3D_Human_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17368-b31b1b.svg)](https://arxiv.org/abs/2303.17368) | :heavy_minus_sign: |
| OxfordTVG-HIC: Can Machine Make Humorous Captions from Images? | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://torrvision.com/tvghic/) <br /> [![GitHub](https://img.shields.io/github/stars/runjiali-rl/Oxford_HIC?style=flat)](https://github.com/runjiali-rl/Oxford_HIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_OxfordTVG-HIC_Can_Machine_Make_Humorous_Captions_from_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11636-b31b1b.svg)](https://arxiv.org/abs/2307.11636) | :heavy_minus_sign: |
| LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lojzezust.github.io/lars-dataset/) <br /> [![GitHub](https://img.shields.io/github/stars/lojzezust/lars_evaluator?style=flat)](https://github.com/lojzezust/lars_evaluator) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zust_LaRS_A_Diverse_Panoptic_Maritime_Obstacle_Detection_Dataset_and_Benchmark_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09618-b31b1b.svg)](https://arxiv.org/abs/2308.09618) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=70TACDeZ6kI) |
| Joint Metrics Matter: A Better Standard for Trajectory Forecasting | [![GitHub](https://img.shields.io/github/stars/ericaweng/joint-metrics-matter?style=flat)](https://github.com/ericaweng/joint-metrics-matter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Joint_Metrics_Matter_A_Better_Standard_for_Trajectory_Forecasting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06292-b31b1b.svg)](https://arxiv.org/abs/2305.06292) | :heavy_minus_sign: |
| LPFF: A Portrait Dataset for Face Generators Across Large Poses | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_LPFF_A_Portrait_Dataset_for_Face_Generators_Across_Large_Poses_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14407-b31b1b.svg)](https://arxiv.org/abs/2303.14407) | :heavy_minus_sign: |
| Replay: Multi-Modal Multi-View Acted Videos for Casual Holography | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://replay-dataset.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/replay_dataset?style=flat)](https://github.com/facebookresearch/replay_dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shapovalov_Replay_Multi-modal_Multi-view_Acted_Videos_for_Casual_Holography_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12067-b31b1b.svg)](https://arxiv.org/abs/2307.12067) | :heavy_minus_sign: |
| Human-Centric Scene Understanding for 3D Large-Scale Scenarios | [![GitHub](https://img.shields.io/github/stars/4DVLab/HuCenLife?style=flat)](https://github.com/4DVLab/HuCenLife) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Human-centric_Scene_Understanding_for_3D_Large-scale_Scenarios_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14392-b31b1b.svg)](https://arxiv.org/abs/2307.14392) | :heavy_minus_sign: |
| Pre-Training Vision Transformers with Very Limited Synthesized Images | [![GitHub](https://img.shields.io/github/stars/ryoo-nakamura/OFDB?style=flat)](https://github.com/ryoo-nakamura/OFDB) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakamura_Pre-training_Vision_Transformers_with_Very_Limited_Synthesized_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14710-b31b1b.svg)](https://arxiv.org/abs/2307.14710) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=BzgNBwZt1W4) |
| FACET: Fairness in Computer Vision Evaluation Benchmark | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://facet.metademolab.com/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gustafson_FACET_Fairness_in_Computer_Vision_Evaluation_Benchmark_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00035-b31b1b.svg)](https://arxiv.org/abs/2309.00035) | :heavy_minus_sign: |
| EmoSet: A Large-Scale Visual Emotion Dataset with Rich Attributes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcc.tech/EmoSet) <br /> [![GitHub](https://img.shields.io/github/stars/JingyuanYY/EmoSet?style=flat)](https://github.com/JingyuanYY/EmoSet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_EmoSet_A_Large-scale_Visual_Emotion_Dataset_with_Rich_Attributes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07961-b31b1b.svg)](https://arxiv.org/abs/2307.07961) | :heavy_minus_sign: |
| RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://adwardlee.github.io/view_renderih/) <br /> [![GitHub](https://img.shields.io/github/stars/adwardlee/RenderIH?style=flat)](https://github.com/adwardlee/RenderIH) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RenderIH_A_Large-Scale_Synthetic_Dataset_for_3D_Interacting_Hand_Pose_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.09301-b31b1b.svg)](https://arxiv.org/abs/2309.09301) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eUVE61O-K0s) |
| TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tifa-benchmark.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Yushi-Hu/tifa?style=flat)](https://github.com/Yushi-Hu/tifa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_TIFA_Accurate_and_Interpretable_Text-to-Image_Faithfulness_Evaluation_with_Question_Answering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11897-b31b1b.svg)](https://arxiv.org/abs/2303.11897) | :heavy_minus_sign: |
| Exploring the Sim2Real Gap using Digital Twins | [![GitHub](https://img.shields.io/github/stars/SruthiSudhakar/Exploring-the-Sim2Real-Gap-using-Digital-Twins-Dataset?style=flat)](https://github.com/SruthiSudhakar/Exploring-the-Sim2Real-Gap-using-Digital-Twins-Dataset) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakar_Exploring_the_Sim2Real_Gap_Using_Digital_Twins_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/clothesnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ClothesNet_An_Information-Rich_3D_Garment_Model_Repository_with_Simulated_Clothes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09987-b31b1b.svg)](https://arxiv.org/abs/2308.09987) | :heavy_minus_sign: |
| Video State-Changing Object Segmentation | [![GitHub](https://img.shields.io/github/stars/venom12138/VSCOS?style=flat)](https://github.com/venom12138/VSCOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Video_State-Changing_Object_Segmentation_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xkmKjuVTzrk) |
| PlanarTrack: A Large-Scale Challenging Benchmark for Planar Object Tracking | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hengfan2010.github.io/projects/PlanarTrack/) <br /> [![GitHub](https://img.shields.io/github/stars/HengLan/PlanarTrack?style=flat)](https://github.com/HengLan/PlanarTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_PlanarTrack_A_Large-scale_Challenging_Benchmark_for_Planar_Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07625-b31b1b.svg)](https://arxiv.org/abs/2303.07625) | :heavy_minus_sign: |
| AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception | [![GitHub](https://img.shields.io/github/stars/ydk122024/AIDE?style=flat)](https://github.com/ydk122024/AIDE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_AIDE_A_Vision-Driven_Multi-View_Multi-Modal_Multi-Tasking_Dataset_for_Assistive_Driving_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.13933-b31b1b.svg)](https://arxiv.org/abs/2307.13933) | :heavy_minus_sign: |
| Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ophai.hms.harvard.edu/datasets/harvard-gdp1000) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Harvard_Glaucoma_Detection_and_Progression_A_Multimodal_Multitask_Dataset_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13411-b31b1b.svg)](https://arxiv.org/abs/2308.13411) | :heavy_minus_sign: |
| ARNOLD: A Benchmark for Language-Grounded Task Learning with Continuous States in Realistic 3D Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://arnold-benchmark.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/arnold-benchmark/arnold?style=flat)](https://github.com/arnold-benchmark/arnold) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_ARNOLD_A_Benchmark_for_Language-Grounded_Task_Learning_with_Continuous_States_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.04321-b31b1b.svg)](https://arxiv.org/abs/2304.04321) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=w-Cp1PRDWzI) |
| FishNet: A Large-Scale Dataset and Benchmark for Fish Recognition, Detection, and Functional Trait Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fishnet-2023.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/faixan-khan/FishNet?style=flat)](https://github.com/faixan-khan/FishNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_FishNet_A_Large-scale_Dataset_and_Benchmark_for_Fish_Recognition_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Content-based Pixel Retrieval in Revisited Oxford and Paris | [![GitHub](https://img.shields.io/github/stars/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval?style=flat)](https://github.com/anguoyuan/Pixel_retrieval-Segmented_instance_retrieval) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/An_Towards_Content-based_Pixel_Retrieval_in_Revisited_Oxford_and_Paris_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.05438-b31b1b.svg)](https://arxiv.org/abs/2309.05438) | :heavy_minus_sign: |
| A Large-Scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition | [![GitHub](https://img.shields.io/github/stars/AndongDeng/BEAR?style=flat)](https://github.com/AndongDeng/BEAR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_A_Large-scale_Study_of_Spatiotemporal_Representation_Learning_with_a_New_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13505-b31b1b.svg)](https://arxiv.org/abs/2303.13505) | :heavy_minus_sign: |
| SQAD: Automatic Smartphone Camera Quality Assessment and Benchmarking | [![GitHub](https://img.shields.io/github/stars/aiff22/SQAD?style=flat)](https://github.com/aiff22/SQAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_SQAD_Automatic_Smartphone_Camera_Quality_Assessment_and_Benchmarking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Revisiting Scene Text Recognition: A Data Perspective | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://union14m.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Mountchicken/Union14M?style=flat)](https://github.com/Mountchicken/Union14M) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Revisiting_Scene_Text_Recognition_A_Data_Perspective_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08723-b31b1b.svg)](https://arxiv.org/abs/2307.08723) | :heavy_minus_sign: |
| Will Large-Scale Generative Models Corrupt Future Datasets? | [![GitHub](https://img.shields.io/github/stars/moskomule/dataset-contamination?style=flat)](https://github.com/moskomule/dataset-contamination) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hataya_Will_Large-scale_Generative_Models_Corrupt_Future_Datasets_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08095-b31b1b.svg)](https://arxiv.org/abs/2211.08095) | :heavy_minus_sign: |
| 360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://360vot.hkustvgd.com/) <br /> [![GitHub](https://img.shields.io/github/stars/HuajianUP/360VOT?style=flat)](https://github.com/HuajianUP/360VOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_360VOT_A_New_Benchmark_Dataset_for_Omnidirectional_Visual_Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14630-b31b1b.svg)](https://arxiv.org/abs/2307.14630) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=PKAVzyGBJMw) |
