# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/embodied-vision-active-agents-simulation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/transfer-low-shot-continual-long-tail-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Recognition: Retrieval

![Section Papers](https://img.shields.io/badge/Section%20Papers-31-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-16-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-18-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-2-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unsupervised Feature Representation Learning for Domain-Generalized Cross-Domain Image Retrieval | [![GitHub](https://img.shields.io/github/stars/conghui1002/DG-UCDIR?style=flat)](https://github.com/conghui1002/DG-UCDIR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Unsupervised_Feature_Representation_Learning_for_Domain-generalized_Cross-domain_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DEDRIFT: Robust Similarity Search under Content Drift | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Baranchuk_DEDRIFT_Robust_Similarity_Search_under_Content_Drift_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02752-b31b1b.svg)](https://arxiv.org/abs/2308.02752) | :heavy_minus_sign: |
| Global Features are All You Need for Image Retrieval and Reranking | [![GitHub](https://img.shields.io/github/stars/ShihaoShao-GH/SuperGlobal?style=flat)](https://github.com/ShihaoShao-GH/SuperGlobal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Global_Features_are_All_You_Need_for_Image_Retrieval_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06954-b31b1b.svg)](https://arxiv.org/abs/2308.06954) | :heavy_minus_sign: |
| HSE: Hybrid Species Embedding for Deep Metric Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_HSE_Hybrid_Species_Embedding_for_Deep_Metric_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Discrepant and Multi-Instance Proxies for Unsupervised Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Discrepant_and_Multi-Instance_Proxies_for_Unsupervised_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/yangbincv/GUR?style=flat)](https://github.com/yangbincv/GUR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition | [![GitHub](https://img.shields.io/github/stars/gmberton/EigenPlaces?style=flat)](https://github.com/gmberton/EigenPlaces) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10832-b31b1b.svg)](https://arxiv.org/abs/2308.10832) | :heavy_minus_sign: |
| Simple Baselines for Interactive Video Retrieval with Questions and Answers | [![GitHub](https://img.shields.io/github/stars/kevinliang888/IVR-QA-baselines?style=flat)](https://github.com/kevinliang888/IVR-QA-baselines) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Simple_Baselines_for_Interactive_Video_Retrieval_with_Questions_and_Answers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10402-b31b1b.svg)](https://arxiv.org/abs/2308.10402) | :heavy_minus_sign: |
| Fan-Beam Binarization Difference Projection (FB-BDP): A Novel Local Object Descriptor for Fine-Grained Leaf Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fan-Beam_Binarization_Difference_Projection_FB-BDP_A_Novel_Local_Object_Descriptor_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Conditional_Cross_Attention_Network_for_Multi-Space_Embedding_without_Entanglement_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.13254-b31b1b.svg)](https://arxiv.org/abs/2307.13254) | :heavy_minus_sign: |
| Learning Concordant Attention via Target-Aware Alignment for Visible-Infrared Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Person Re-Identification without Identification via Event Anonymization | [![GitHub](https://img.shields.io/github/stars/IIT-PAVIS/ReId_without_Id?style=flat)](https://github.com/IIT-PAVIS/ReId_without_Id) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahmad_Person_Re-Identification_without_Identification_via_Event_anonymization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04402-b31b1b.svg)](https://arxiv.org/abs/2308.04402) | :heavy_minus_sign: |
| Divide&Classify: Fine-Grained Classification for City-Wide Visual Geo-Localization | [![GitHub](https://img.shields.io/github/stars/ga1i13o/Divide-and-Classify?style=flat)](https://github.com/ga1i13o/Divide-and-Classify) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Trivigno_DivideClassify_Fine-Grained_Classification_for_City-Wide_Visual_Geo-Localization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning | [![GitHub](https://img.shields.io/github/stars/mohwald/gandtr?style=flat)](https://github.com/mohwald/gandtr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mohwald_Dark_Side_Augmentation_Generating_Diverse_Night_Examples_for_Metric_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16351-b31b1b.svg)](https://arxiv.org/abs/2309.16351) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zlT-GJOcgYw) |
| PIDRo: Parallel Isomeric Attention with Dynamic Routing for Text-Video Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_PIDRo_Parallel_Isomeric_Attention_with_Dynamic_Routing_for_Text-Video_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Unified Pre-Training with Pseudo Texts for Text-to-Image Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/ZhiyinShao-H/UniPT?style=flat)](https://github.com/ZhiyinShao-H/UniPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Unified_Pre-Training_with_Pseudo_Texts_for_Text-To-Image_Person_Re-Identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01420-b31b1b.svg)](https://arxiv.org/abs/2309.01420) | :heavy_minus_sign: |
| Modality Unifying Network for Visible-Infrared Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Modality_Unifying_Network_for_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06262-b31b1b.svg)](https://arxiv.org/abs/2309.06262) | :heavy_minus_sign: |
| DeepChange: A Long-Term Person Re-Identification Benchmark with Clothes Change | [![GitHub](https://img.shields.io/github/stars/PengBoXiangShang/deepchange?style=flat)](https://github.com/PengBoXiangShang/deepchange) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_DeepChange_A_Long-Term_Person_Re-Identification_Benchmark_with_Clothes_Change_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval | [![GitHub](https://img.shields.io/github/stars/ChiYeungLaw/LexLIP-ICCV23?style=flat)](https://github.com/ChiYeungLaw/LexLIP-ICCV23) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.02908-b31b1b.svg)](https://arxiv.org/abs/2302.02908) | :heavy_minus_sign: |
| Dual Pseudo-Labels Interactive Self-Training for Semi-Supervised Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/XiangboYin/DPIS_USVLReID?style=flat)](https://github.com/XiangboYin/DPIS_USVLReID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Dual_Pseudo-Labels_Interactive_Self-Training_for_Semi-Supervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| BT<sup>2</sup>: Backward-Compatible Training with Basis Transformation | [![GitHub](https://img.shields.io/github/stars/YifeiZhou02/BT-2?style=flat)](https://github.com/YifeiZhou02/BT-2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_BT2_Backward-compatible_Training_with_Basis_Transformation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.03989-b31b1b.svg)](https://arxiv.org/abs/2211.03989) | :heavy_minus_sign: |
| Prototypical Mixing and Retrieval-based Refinement for Label Noise-Resistant Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Prototypical_Mixing_and_Retrieval-Based_Refinement_for_Label_Noise-Resistant_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learning Spatial-Context-Aware Global Visual Feature Representation for Instance Image Retrieval | [![GitHub](https://img.shields.io/github/stars/Zy-Zhang/SpCa?style=flat)](https://github.com/Zy-Zhang/SpCa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Spatial-context-aware_Global_Visual_Feature_Representation_for_Instance_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval | [![GitHub](https://img.shields.io/github/stars/bassyess/CFCD?style=flat)](https://github.com/bassyess/CFCD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Coarse-to-Fine_Learning_Compact_Discriminative_Representation_for_Single-Stage_Image_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04008-b31b1b.svg)](https://arxiv.org/abs/2308.04008) | :heavy_minus_sign: |
| Visible-Infrared Person Re-Identification via Semantic Alignment and Affinity Inference | [![GitHub](https://img.shields.io/github/stars/xiaoye-hhh/SAAI?style=flat)](https://github.com/xiaoye-hhh/SAAI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Visible-Infrared_Person_Re-Identification_via_Semantic_Alignment_and_Affinity_Inference_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Part-Aware Transformer for Generalizable Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/liyuke65535/Part-Aware-Transformer?style=flat)](https://github.com/liyuke65535/Part-Aware-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03322-b31b1b.svg)](https://arxiv.org/abs/2308.03322) | :heavy_minus_sign: |
| Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cmp.felk.cvut.cz/univ_emb/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ypsilantis_Towards_Universal_Image_Embeddings_A_Large-Scale_Dataset_and_Challenge_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01858-b31b1b.svg)](https://arxiv.org/abs/2309.01858) | :heavy_minus_sign: |
| Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Dual_Learning_with_Dynamic_Knowledge_Distillation_for_Partially_Relevant_Video_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Fine-Grained Unsupervised Domain Adaptation for Gait Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Fine-grained_Unsupervised_Domain_Adaptation_for_Gait_Recognition_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TsWfYqz8qbk) |
| FashionNTM: Multi-Turn Fashion Image Retrieval via Cascaded Memory | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/eng.ucsd.edu/fashionntm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pal_FashionNTM_Multi-turn_Fashion_Image_Retrieval_via_Cascaded_Memory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10170-b31b1b.svg)](https://arxiv.org/abs/2308.10170) | :heavy_minus_sign: |
| CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition | [![GitHub](https://img.shields.io/github/stars/rayguan97/crossloc3d?style=flat)](https://github.com/rayguan97/crossloc3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_CrossLoc3D_Aerial-Ground_Cross-Source_3D_Place_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17778-b31b1b.svg)](https://arxiv.org/abs/2303.17778) | :heavy_minus_sign: |
