# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/photogrammetry-and-remote-sensing.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/machine-learning-other-than-deep-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Efficient and Scalable Vision

![Section Papers](https://img.shields.io/badge/Section%20Papers-63-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-49-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-43-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-2-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| AdaNIC: Towards Practical Neural Image Compression via Dynamic Transform Routing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tao_AdaNIC_Towards_Practical_Neural_Image_Compression_via_Dynamic_Transform_Routing_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Rethinking Vision Transformers for MobileNet Size and Speed | [![GitHub](https://img.shields.io/github/stars/snap-research/EfficientFormer?style=flat)](https://github.com/snap-research/EfficientFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08059-b31b1b.svg)](https://arxiv.org/abs/2212.08059) | :heavy_minus_sign: |
| DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow?style=flat)](https://github.com/IRMVLab/DELFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_DELFlow_Dense_Efficient_Learning_of_Scene_Flow_for_Large-Scale_Point_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.04383) | :heavy_minus_sign: |
| Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://wisionlab.com/project/eventful-transformers/) <br /> [![GitHub](https://img.shields.io/github/stars/WISION-Lab/eventful-transformer?style=flat)](https://github.com/WISION-Lab/eventful-transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dutson_Eventful_Transformers_Leveraging_Temporal_Redundancy_in_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13494-b31b1b.svg)](https://arxiv.org/abs/2308.13494) | :heavy_minus_sign: |
| Inherent Redundancy in Spiking Neural Networks | [![GitHub](https://img.shields.io/github/stars/BICLab/ASA-SNN?style=flat)](https://github.com/BICLab/ASA-SNN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Inherent_Redundancy_in_Spiking_Neural_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08227-b31b1b.svg)](https://arxiv.org/abs/2308.08227) | :heavy_minus_sign: |
| Achievement-based Training Progress Balancing for Multi-Task Learning | [![GitHub](https://img.shields.io/github/stars/samsung/Achievement-based-MTL?style=flat)](https://github.com/samsung/Achievement-based-MTL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yun_Achievement-Based_Training_Progress_Balancing_for_Multi-Task_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Prune Spatio-Temporal Tokens by Semantic-Aware Temporal Accumulation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04549-b31b1b.svg)](https://arxiv.org/abs/2308.04549) | :heavy_minus_sign: |
| Differentiable Transportation Pruning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Differentiable_Transportation_Pruning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08483-b31b1b.svg)](https://arxiv.org/abs/2307.08483) | :heavy_minus_sign: |
| XiNet: Efficient Neural Networks for tinyML | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ancilotto_XiNet_Efficient_Neural_Networks_for_tinyML_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/enyac-group/evol-q?style=flat)](https://github.com/enyac-group/evol-q) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Frumkin_Jumping_through_Local_Minima_Quantization_in_the_Loss_Landscape_of_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10814-b31b1b.svg)](https://arxiv.org/abs/2308.10814) | :heavy_minus_sign: |
| A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Colbert_A2Q_Accumulator-Aware_Quantization_with_Guaranteed_Overflow_Avoidance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.13504v1) | :heavy_minus_sign: |
| Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Workie-Talkie_Accelerating_Federated_Learning_by_Overlapping_Computing_and_Communications_via_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DenseShift: Towards Accurate and Efficient Low-Bit Power-of-Two Quantization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/xinlinli170/noah-research/tree/master/S3-Training) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DenseShift_Towards_Accurate_and_Efficient_Low-Bit_Power-of-Two_Quantization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.09708-b31b1b.svg)](https://arxiv.org/abs/2208.09708) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=L_C6dBkVttg) |
| PRANC: Pseudo RAndom Networks for Compacting Deep Models | [![GitHub](https://img.shields.io/github/stars/UCDvision/PRANC?style=flat)](https://github.com/UCDvision/PRANC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nooralinejad_PRANC_Pseudo_RAndom_Networks_for_Compacting_Deep_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.08464-b31b1b.svg)](https://arxiv.org/abs/2206.08464) | :heavy_minus_sign: |
| Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement | [![GitHub](https://img.shields.io/github/stars/apple/ml-dr?style=flat)](https://github.com/apple/ml-dr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Faghri_Reinforce_Data_Multiply_Impact_Improved_Model_Accuracy_and_Robustness_with_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08983-b31b1b.svg)](https://arxiv.org/abs/2303.08983) | :heavy_minus_sign: |
| A Fast Unified System for 3D Object Detection and Tracking | [![GitHub](https://img.shields.io/github/stars/theitzin/FUS3D?style=flat)](https://github.com/theitzin/FUS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Heitzinger_A_Fast_Unified_System_for_3D_Object_Detection_and_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Estimator Meets Equilibrium Perspective: A Rectified Straight through Estimator for Binary Neural Networks Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Estimator_Meets_Equilibrium_Perspective_A_Rectified_Straight_Through_Estimator_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06689-b31b1b.svg)](https://arxiv.org/abs/2308.06689) | :heavy_minus_sign: |
| I-ViT: Integer-Only Quantization for Efficient Vision Transformer Inference | [![GitHub](https://img.shields.io/github/stars/zkkli/I-ViT?style=flat)](https://github.com/zkkli/I-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_I-ViT_Integer-only_Quantization_for_Efficient_Vision_Transformer_Inference_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2207.01405-b31b1b.svg)](https://arxiv.org/abs/2207.01405) | :heavy_minus_sign: |
| EMQ: Evolving Training-Free Proxies for Automated Mixed Precision Quantization | [![GitHub](https://img.shields.io/github/stars/lilujunai/EMQ-series?style=flat)](https://github.com/lilujunai/EMQ-series) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_EMQ_Evolving_Training-free_Proxies_for_Automated_Mixed_Precision_Quantization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10554-b31b1b.svg)](https://arxiv.org/abs/2307.10554) | :heavy_minus_sign: |
| Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Local_or_Global_Selective_Knowledge_Assimilation_for_Federated_Learning_with_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08809-b31b1b.svg)](https://arxiv.org/abs/2307.08809) | :heavy_minus_sign: |
| DataDAM: Efficient Dataset Distillation with Attention Matching | [![GitHub](https://img.shields.io/github/stars/DataDistillation/DataDAM?style=flat)](https://github.com/DataDistillation/DataDAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sajedi_DataDAM_Efficient_Dataset_Distillation_with_Attention_Matching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.00093-b31b1b.svg)](https://arxiv.org/abs/2310.00093) | :heavy_minus_sign: |
| SAFE: Machine Unlearning with Shard Graphs | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dukler_SAFE_Machine_Unlearning_With_Shard_Graphs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13169-b31b1b.svg)](https://arxiv.org/abs/2304.13169) | :heavy_minus_sign: |
| ResQ: Residual Quantization for Video Perception | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Abati_ResQ_Residual_Quantization_for_Video_Perception_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09511-b31b1b.svg)](https://arxiv.org/abs/2308.09511) | :heavy_minus_sign: |
| Efficient Computation Sharing for Multi-Task Visual Scene Understanding | [![GitHub](https://img.shields.io/github/stars/sarashoouri/EfficientMTL?style=flat)](https://github.com/sarashoouri/EfficientMTL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shoouri_Efficient_Computation_Sharing_for_Multi-Task_Visual_Scene_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09663-b31b1b.svg)](https://arxiv.org/abs/2303.09663) | :heavy_minus_sign: |
| Essential Matrix Estimation using Convex Relaxations in Orthogonal Space | [![GitHub](https://img.shields.io/github/stars/armandok/QME?style=flat)](https://github.com/armandok/QME) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Karimian_Essential_Matrix_Estimation_using_Convex_Relaxations_in_Orthogonal_Space_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TripLe: Revisiting Pretrained Model Reuse and Progressive Learning for Efficient Vision Transformer Scaling and Searching | :heavy_minus_sign: |[![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_TripLe_Revisiting_Pretrained_Model_Reuse_and_Progressive_Learning_for_Efficient_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DiffRate: Differentiable Compression Rate for Efficient Vision Transformers | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DiffRate?style=flat)](https://github.com/OpenGVLab/DiffRate) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DiffRate__Differentiable_Compression_Rate_for_Efficient_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.17997-b31b1b.svg)](https://arxiv.org/abs/2305.17997) | :heavy_minus_sign: |
| Bridging Cross-Task Protocol Inconsistency for Distillation in Dense Object Detection | [![GitHub](https://img.shields.io/github/stars/TinyTigerPan/BCKD?style=flat)](https://github.com/TinyTigerPan/BCKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Bridging_Cross-task_Protocol_Inconsistency_for_Distillation_in_Dense_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14286-b31b1b.svg)](https://arxiv.org/abs/2308.14286) | :heavy_minus_sign: |
| From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels | [![GitHub](https://img.shields.io/github/stars/yzd-v/cls_KD?style=flat)](https://github.com/yzd-v/cls_KD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_From_Knowledge_Distillation_to_Self-Knowledge_Distillation_A_Unified_Approach_with_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13005-b31b1b.svg)](https://arxiv.org/abs/2303.13005) | :heavy_minus_sign: |
| Efficient 3D Semantic Segmentation with Superpoint Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://drprojects.github.io/superpoint-transformer) <br /> [![GitHub](https://img.shields.io/github/stars/drprojects/superpoint_transformer?style=flat)](https://github.com/drprojects/superpoint_transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Robert_Efficient_3D_Semantic_Segmentation_with_Superpoint_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08045-b31b1b.svg)](https://arxiv.org/abs/2306.08045) | :heavy_minus_sign: |
| Dataset Quantization | [![GitHub](https://img.shields.io/github/stars/magic-research/Dataset_Quantization?style=flat)](https://github.com/magic-research/Dataset_Quantization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Dataset_Quantization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10524-b31b1b.svg)](https://arxiv.org/abs/2308.10524) | :heavy_minus_sign: |
| Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy | [![GitHub](https://img.shields.io/github/stars/JieShibo/PETL-ViT?style=flat)](https://github.com/JieShibo/PETL-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jie_Revisiting_the_Parameter_Efficiency_of_Adapters_from_the_Perspective_of_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16867-b31b1b.svg)](https://arxiv.org/abs/2307.16867) | :heavy_minus_sign: |
| RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/zkkli/RepQ-ViT?style=flat)](https://github.com/zkkli/RepQ-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_RepQ-ViT_Scale_Reparameterization_for_Post-Training_Quantization_of_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.08254-b31b1b.svg)](https://arxiv.org/abs/2212.08254) | :heavy_minus_sign: |
| Semantically Structured Image Compression via Irregular Group-based Decoupling | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow?style=flat)](https://github.com/IRMVLab/DELFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Semantically_Structured_Image_Compression_via_Irregular_Group-Based_Decoupling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.02586-b31b1b.svg)](https://arxiv.org/abs/2305.02586) | :heavy_minus_sign: |
| SeiT: Storage-Efficient Vision Training with Tokens using 1% of Pixel Storage | [![GitHub](https://img.shields.io/github/stars/naver-ai/seit?style=flat)](https://github.com/naver-ai/seit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_SeiT_Storage-Efficient_Vision_Training_with_Tokens_Using_1_of_Pixel_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11114-b31b1b.svg)](https://arxiv.org/abs/2303.11114) | :heavy_minus_sign: |
| SMMix: Self-Motivated Image Mixing for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/ChenMnZ/SMMix?style=flat)](https://github.com/ChenMnZ/SMMix) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_SMMix_Self-Motivated_Image_Mixing_for_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.12977-b31b1b.svg)](https://arxiv.org/abs/2212.12977) | :heavy_minus_sign: |
| Multi-Label Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/penghui-yang/L2D?style=flat)](https://github.com/penghui-yang/L2D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Multi-Label_Knowledge_Distillation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06453-b31b1b.svg)](https://arxiv.org/abs/2308.06453) | :heavy_minus_sign: |
| UGC: Unified GAN Compression for Efficient Image-to-Image Translation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_UGC_Unified_GAN_Compression_for_Efficient_Image-to-Image_Translation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.09310-b31b1b.svg)](https://arxiv.org/abs/2309.09310) | :heavy_minus_sign: |
| MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos with Spherical Buffers and Padded Convolutions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Parger_MotionDeltaCNN_Sparse_CNN_Inference_of_Frame_Differences_in_Moving_Camera_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09887-b31b1b.svg)](https://arxiv.org/abs/2210.09887) | :heavy_minus_sign: |
| EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction | [![GitHub](https://img.shields.io/github/stars/mit-han-lab/efficientvit?style=flat)](https://github.com/mit-han-lab/efficientvit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_EfficientViT_Lightweight_Multi-Scale_Attention_for_High-Resolution_Dense_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.14756-b31b1b.svg)](https://arxiv.org/abs/2205.14756) | :heavy_minus_sign: |
| DREAM: Efficient Dataset Distillation by Representative Matching | [![GitHub](https://img.shields.io/github/stars/lyq312318224/DREAM?style=flat)](https://github.com/lyq312318224/DREAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DREAM_Efficient_Dataset_Distillation_by_Representative_Matching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14416-b31b1b.svg)](https://arxiv.org/abs/2302.14416) | :heavy_minus_sign: |
| INSTA-BNN: Binary Neural Network with INSTAnce-Aware Threshold | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_INSTA-BNN_Binary_Neural_Network_with_INSTAnce-aware_Threshold_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2204.07439-b31b1b.svg)](https://arxiv.org/abs/2204.07439) | :heavy_minus_sign: |
| Deep Incubation: Training Large Models by Divide-and-Conquering | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Deep-Incubation?style=flat)](https://github.com/LeapLabTHU/Deep-Incubation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Deep_Incubation_Training_Large_Models_by_Divide-and-Conquering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04129-b31b1b.svg)](https://arxiv.org/abs/2212.04129) | :heavy_minus_sign: |
| AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/google-research/google-research/tree/master/moe_mtl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_AdaMV-MoE_Adaptive_Multi-Task_Vision_Mixture-of-Experts_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Overcoming Forgetting Catastrophe in Quantization-Aware Training | [![GitHub](https://img.shields.io/github/stars/tinganchen/LifeQuant?style=flat)](https://github.com/tinganchen/LifeQuant) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Overcoming_Forgetting_Catastrophe_in_Quantization-Aware_Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Window-based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are more Efficient than Single Models | [![GitHub](https://img.shields.io/github/stars/Guoxoug/window-early-exit?style=flat)](https://github.com/Guoxoug/window-early-exit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_Window-Based_Early-Exit_Cascades_for_Uncertainty_Estimation_When_Deep_Ensembles_are_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08010-b31b1b.svg)](https://arxiv.org/abs/2303.08010) | :heavy_minus_sign: |
| ORC: Network Group-based Knowledge Distillation using Online Role Change | [![GitHub](https://img.shields.io/github/stars/choijunyong/ORCKD?style=flat)](https://github.com/choijunyong/ORCKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Choi_ORC_Network_Group-based_Knowledge_Distillation_using_Online_Role_Change_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.01186-b31b1b.svg)](https://arxiv.org/abs/2206.01186) | :heavy_minus_sign: |
| RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_RMP-Loss_Regularizing_Membrane_Potential_Distribution_for_Spiking_Neural_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06787-b31b1b.svg)](https://arxiv.org/abs/2308.06787) | :heavy_minus_sign: |
| Structural Alignment for Network Pruning through Partial Regularization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Structural_Alignment_for_Network_Pruning_through_Partial_Regularization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Automated Knowledge Distillation via Monte Carlo Tree Search | [![GitHub](https://img.shields.io/github/stars/lilujunai/Auto-KD?style=flat)](https://github.com/lilujunai/Auto-KD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Automated_Knowledge_Distillation_via_Monte_Carlo_Tree_Search_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SwiftFormer: Efficient Additive Attention for Transformer-based Real-Time Mobile Vision Applications | [![GitHub](https://img.shields.io/github/stars/Amshaker/SwiftFormer?style=flat)](https://github.com/Amshaker/SwiftFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shaker_SwiftFormer_Efficient_Additive_Attention_for_Transformer-based_Real-time_Mobile_Vision_Applications_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15446-b31b1b.svg)](https://arxiv.org/abs/2303.15446) | :heavy_minus_sign: |
| Causal-DFQ: Causality Guided Data-Free Network Quantization | [![GitHub](https://img.shields.io/github/stars/42Shawn/Causal-DFQ?style=flat)](https://github.com/42Shawn/Causal-DFQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shang_Causal-DFQ_Causality_Guided_Data-Free_Network_Quantization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.13682-b31b1b.svg)](https://arxiv.org/abs/2309.13682) | :heavy_minus_sign: |
| Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks | [![GitHub](https://img.shields.io/github/stars/Akimoto-Cris/RD_PRUNE?style=flat)](https://github.com/Akimoto-Cris/RD_PRUNE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Efficient_Joint_Optimization_of_Layer-Adaptive_Weight_Pruning_in_Deep_Neural_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10438-b31b1b.svg)](https://arxiv.org/abs/2308.10438) | :heavy_minus_sign: |
| Automatic Network Pruning via Hilbert-Schmidt Independence Criterion Lasso under Information Bottleneck Principle | [![GitHub](https://img.shields.io/github/stars/sunggo/APIB?style=flat)](https://github.com/sunggo/APIB) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Automatic_Network_Pruning_via_Hilbert-Schmidt_Independence_Criterion_Lasso_under_Information_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Distribution Shift Matters for Knowledge Distillation with Webly Collected Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Distribution_Shift_Matters_for_Knowledge_Distillation_with_Webly_Collected_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11469-b31b1b.svg)](https://arxiv.org/abs/2307.11469) | :heavy_minus_sign: |
| FastRecon: Few-Shot Industrial Anomaly Detection via Fast Feature Reconstruction | [![GitHub](https://img.shields.io/github/stars/FzJun26th/FastRecon?style=flat)](https://github.com/FzJun26th/FastRecon) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_FastRecon_Few-shot_Industrial_Anomaly_Detection_via_Fast_Feature_Reconstruction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| E<sup>2</sup>VPT: An Effective and Efficient Approach for Visual Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/ChengHan111/E2VPT?style=flat)](https://github.com/ChengHan111/E2VPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_E2VPT_An_Effective_and_Efficient_Approach_for_Visual_Prompt_Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.13770-b31b1b.svg)](https://arxiv.org/abs/2307.13770) | :heavy_minus_sign: |
| Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/kkakkkka/ETRIS?style=flat)](https://github.com/kkakkkka/ETRIS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Bridging_Vision_and_Language_Encoders_Parameter-Efficient_Tuning_for_Referring_Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11545-b31b1b.svg)](https://arxiv.org/abs/2307.11545) | :heavy_minus_sign: |
| SHACIRA: Scalable HAsh-Grid Compression for Implicit Neural Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shacira.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Sharath-girish/Shacira?style=flat)](https://github.com/Sharath-girish/Shacira) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Girish_SHACIRA_Scalable_HAsh-grid_Compression_for_Implicit_Neural_Representations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15848-b31b1b.svg)](https://arxiv.org/abs/2309.15848) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zRr9ZqlmSzY) |
| Efficient Deep Space Filling Curve | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Efficient_Deep_Space_Filling_Curve_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Q-Diffusion: Quantizing Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://xiuyuli.com/qdiffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion?style=flat)](https://github.com/Xiuyu-Li/q-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Q-Diffusion_Quantizing_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.04304-b31b1b.svg)](https://arxiv.org/abs/2302.04304) | :heavy_minus_sign: |
| Lossy and Lossless (L2) Post-Training Model Size Compression | [![GitHub](https://img.shields.io/github/stars/ModelTC/L2_Compression?style=flat)](https://github.com/ModelTC/L2_Compression) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Lossy_and_Lossless_L2_Post-training_Model_Size_Compression_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04269-b31b1b.svg)](https://arxiv.org/abs/2308.04269) | :heavy_minus_sign: |
| Robustifying Token Attention for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/guoyongcs/TAPADL?style=flat)](https://github.com/guoyongcs/TAPADL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11126-b31b1b.svg)](https://arxiv.org/abs/2303.11126) | :heavy_minus_sign: |
