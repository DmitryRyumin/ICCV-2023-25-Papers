# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/computer-vision-theory.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/object-pose-estimation-and-tracking.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Video Analysis and Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-51-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-38-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-32-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-6-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Long-Range Multimodal Pretraining for Movie Understanding | [![GitHub](https://img.shields.io/github/stars/dawitmureja/LMP?style=flat)](https://github.com/dawitmureja/LMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Argaw_Long-range_Multimodal_Pretraining_for_Movie_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09775-b31b1b.svg)](https://arxiv.org/pdf/2308.09775.pdf) | :heavy_minus_sign: |
| Cross-View Semantic Alignment for Livestreaming Product Recognition | [![GitHub](https://img.shields.io/github/stars/adxcreative/RICE?style=flat)](https://github.com/adxcreative/RICE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Cross-view_Semantic_Alignment_for_Livestreaming_Product_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04912-b31b1b.svg)](https://arxiv.org/abs/2308.04912) | :heavy_minus_sign: |
| HTML: Hybrid Temporal-Scale Multimodal Learning Framework for Referring Video Object Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mingfei.info/HTML/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_HTML_Hybrid_Temporal-scale_Multimodal_Learning_Framework_for_Referring_Video_Object_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DyGait: Exploiting Dynamic Representations for High-Performance Gait Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14953-b31b1b.svg)](https://arxiv.org/abs/2303.14953) | :heavy_minus_sign: |
| Identity-Consistent Aggregation for Video Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Identity-Consistent_Aggregation_for_Video_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07737-b31b1b.svg)](https://arxiv.org/abs/2308.07737) | :heavy_minus_sign: |
| Augmenting and Aligning Snippets for Few-Shot Video Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/xuyu0010/SSA2lign?style=flat)](https://github.com/xuyu0010/SSA2lign) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Augmenting_and_Aligning_Snippets_for_Few-Shot_Video_Domain_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10451-b31b1b.svg)](https://arxiv.org/abs/2303.10451) | :heavy_minus_sign: |
| Action Sensitivity Learning for Temporal Action Localization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Action_Sensitivity_Learning_for_Temporal_Action_Localization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.15701-b31b1b.svg)](https://arxiv.org/abs/2305.15701) | :heavy_minus_sign: |
| SwinLSTM: Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM | [![GitHub](https://img.shields.io/github/stars/SongTang-x/SwinLSTM?style=flat)](https://github.com/SongTang-x/SwinLSTM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_SwinLSTM_Improving_Spatiotemporal_Prediction_Accuracy_using_Swin_Transformer_and_LSTM_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09891-b31b1b.svg)](https://arxiv.org/abs/2308.09891) | :heavy_minus_sign: |
| LVOS: A Benchmark for Long-Term Video Object Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lingyihongfd.github.io/lvos.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/LingyiHongfd/LVOS?style=flat)](https://github.com/LingyiHongfd/LVOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_LVOS_A_Benchmark_for_Long-term_Video_Object_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10181-b31b1b.svg)](https://arxiv.org/abs/2211.10181) | :heavy_minus_sign: |
| MGMAE: Motion Guided Masking for Video Masked Autoencoding | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/MGMAE?style=flat)](https://github.com/MCG-NJU/MGMAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_MGMAE_Motion_Guided_Masking_for_Video_Masked_Autoencoding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10794-b31b1b.svg)](https://arxiv.org/abs/2308.10794) | :heavy_minus_sign: |
| Markov Game Video Augmentation for Action Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aziere_Markov_Game_Video_Augmentation_for_Action_Segmentation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| COOL-CHIC: Coordinate-based Low Complexity Hierarchical Image Codec | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://orange-opensource.github.io/Cool-Chic/) <br /> [![GitHub](https://img.shields.io/github/stars/Orange-OpenSource/Cool-Chic?style=flat)](https://github.com/Orange-OpenSource/Cool-Chic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ladune_COOL-CHIC_Coordinate-based_Low_Complexity_Hierarchical_Image_Codec_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ReGen: A Good Generative Zero-Shot Video Classifier Should be Rewarded | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_ReGen_A_good_Generative_Zero-Shot_Video_Classifier_Should_be_Rewarded_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Task Agnostic Restoration of Natural Video Dynamics | [![GitHub](https://img.shields.io/github/stars/MKashifAli/TARONVD?style=flat)](https://github.com/MKashifAli/TARONVD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ali_Task_Agnostic_Restoration_of_Natural_Video_Dynamics_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.03753-b31b1b.svg)](https://arxiv.org/abs/2206.03753) | :heavy_minus_sign: |
| Normalizing Flows for Human Pose Anomaly Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://orhir.github.io/STG_NF/) <br /> [![GitHub](https://img.shields.io/github/stars/orhir/STG-NF?style=flat)](https://github.com/orhir/STG-NF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hirschorn_Normalizing_Flows_for_Human_Pose_Anomaly_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10946-b31b1b.svg)](https://arxiv.org/abs/2211.10946) | :heavy_minus_sign: |
| Movement Enhancement toward Multi-Scale Video Feature Representation for Temporal Action Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Movement_Enhancement_toward_Multi-Scale_Video_Feature_Representation_for_Temporal_Action_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Event-Guided Procedure Planning from Instructional Videos with Text Supervision | [![GitHub](https://img.shields.io/github/stars/AlanWang0o0/ISEE-E3P?style=flat)](https://github.com/AlanWang0o0/ISEE-E3P) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Event-Guided_Procedure_Planning_from_Instructional_Videos_with_Text_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08885-b31b1b.svg)](https://arxiv.org/abs/2308.08885) | :heavy_minus_sign: |
| SCANet: Scene Complexity Aware Network for Weakly-Supervised Video Moment Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yoon_SCANet_Scene_Complexity_Aware_Network_for_Weakly-Supervised_Video_Moment_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.05241-b31b1b.svg)](https://arxiv.org/abs/2310.05241) | :heavy_minus_sign: |
| Spatio-Temporal Prompting Network for Robust Video Feature Extraction | [![GitHub](https://img.shields.io/github/stars/guanxiongsun/STPN?style=flat)](https://github.com/guanxiongsun/STPN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Spatio-temporal_Prompting_Network_for_Robust_Video_Feature_Extraction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TeD-SPAD: Temporal Distinctiveness for Self-Supervised Privacy-Preservation for Video Anomaly Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://joefioresi718.github.io/TeD-SPAD_webpage/) <br /> [![GitHub](https://img.shields.io/github/stars/UCF-CRCV/TeD-SPAD?style=flat)](https://github.com/UCF-CRCV/TeD-SPAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11072-b31b1b.svg)](https://arxiv.org/abs/2308.11072) | :heavy_minus_sign: |
| Non-Semantics Suppressed Mask Learning for Unsupervised Video Semantic Compression | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_Non-Semantics_Suppressed_Mask_Learning_for_Unsupervised_Video_Semantic_Compression_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| UnLoc: A Unified Framework for Video Localization Tasks | [![GitHub](https://img.shields.io/github/stars/google-research/scenic?style=flat)](https://github.com/google-research/scenic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_UnLoc_A_Unified_Framework_for_Video_Localization_Tasks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11062-b31b1b.svg)](https://arxiv.org/abs/2308.11062) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=B7-mnHj5jno) |
| SkeleTR: Towards Skeleton-based Action Recognition in the Wild | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Duan_SkeleTR_Towards_Skeleton-based_Action_Recognition_in_the_Wild_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| AutoAD II: The Sequel - Who, When, and What in Movie Audio Description | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.robots.ox.ac.uk/~vgg/research/autoad/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_AutoAD_II_The_Sequel_-_Who_When_and_What_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.06838-b31b1b.svg)](https://arxiv.org/abs/2310.06838) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gMQSoib6lSI) |
| What can a Cook in Italy Teach a Mechanic in India? Action Recognition Generalisation over Scenarios and Locations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chiaraplizz.github.io/what-can-a-cook/) <br /> [![GitHub](https://img.shields.io/github/stars/Chiaraplizz/ARGO1M-What-can-a-cook?style=flat)](https://github.com/Chiaraplizz/ARGO1M-What-can-a-cook) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Plizzari_What_Can_a_Cook_in_Italy_Teach_a_Mechanic_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08713-b31b1b.svg)](https://arxiv.org/abs/2306.08713) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C507QYUItTs) |
| Localizing Moments in Long Video via Multimodal Guidance | [![GitHub](https://img.shields.io/github/stars/waybarrios/guidance-based-video-grounding?style=flat)](https://github.com/waybarrios/guidance-based-video-grounding) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Barrios_Localizing_Moments_in_Long_Video_Via_Multimodal_Guidance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.13372-b31b1b.svg)](https://arxiv.org/abs/2302.13372) | :heavy_minus_sign: |
| LAC - Latent Action Composition for Skeleton-based Action Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://walker1126.github.io/LAC/) <br /> [![GitHub](https://img.shields.io/github/stars/walker1126/Latent_Action_Composition?style=flat)](https://github.com/walker1126/Latent_Action_Composition) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_LAC_-_Latent_Action_Composition_for_Skeleton-based_Action_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14500-b31b1b.svg)](https://arxiv.org/abs/2308.14500) | :heavy_minus_sign: |
| RIGID: Recurrent GAN Inversion and Editing of Real Face Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cnnlstm.github.io/RIGID/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_RIGID_Recurrent_GAN_Inversion_and_Editing_of_Real_Face_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06097-b31b1b.svg)](https://arxiv.org/abs/2308.06097) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=x_bUe6HxDeo) |
| Uncertainty-Aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://actionlab-cv.github.io/EgoHandTrajPred/) <br /> [![GitHub](https://img.shields.io/github/stars/oppo-us-research/USST?style=flat)](https://github.com/oppo-us-research/USST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bao_Uncertainty-aware_State_Space_Transformer_for_Egocentric_3D_Hand_Trajectory_Forecasting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08243-b31b1b.svg)](https://arxiv.org/abs/2307.08243) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MYY6GmqZSJA) |
| What can Simple Arithmetic Operations do for Temporal Modeling? | [![GitHub](https://img.shields.io/github/stars/whwu95/ATM?style=flat)](https://github.com/whwu95/ATM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_What_Can_Simple_Arithmetic_Operations_Do_for_Temporal_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08908-b31b1b.svg)](https://arxiv.org/abs/2307.08908) | :heavy_minus_sign: |
| UATVR: Uncertainty-Adaptive Text-Video Retrieval | [![GitHub](https://img.shields.io/github/stars/bofang98/UATVR?style=flat)](https://github.com/bofang98/UATVR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_UATVR_Uncertainty-Adaptive_Text-Video_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06309-b31b1b.svg)](https://arxiv.org/abs/2301.06309) | :heavy_minus_sign: |
| D3G: Exploring Gaussian Prior for Temporal Sentence Grounding with Glance Annotation | [![GitHub](https://img.shields.io/github/stars/solicucu/D3G?style=flat)](https://github.com/solicucu/D3G) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_D3G_Exploring_Gaussian_Prior_for_Temporal_Sentence_Grounding_with_Glance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04197-b31b1b.svg)](https://arxiv.org/abs/2308.04197) | :heavy_minus_sign: |
| Unsupervised Open-Vocabulary Object Localization in Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Unsupervised_Open-Vocabulary_Object_Localization_in_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.09858-b31b1b.svg)](https://arxiv.org/abs/2309.09858) | :heavy_minus_sign: |
| HiVLP: Hierarchical Interactive Video-Language Pre-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_HiVLP_Hierarchical_Interactive_Video-Language_Pre-Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Scanning Only Once: An End-to-End Framework for Fast Temporal Grounding in Long Videos | [![GitHub](https://img.shields.io/github/stars/afcedf/SOONet?style=flat)](https://github.com/afcedf/SOONet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_Scanning_Only_Once_An_End-to-end_Framework_for_Fast_Temporal_Grounding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08345-b31b1b.svg)](https://arxiv.org/abs/2303.08345) | :heavy_minus_sign: |
| Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://talalwasim.github.io/Video-FocalNets/) <br /> [![GitHub](https://img.shields.io/github/stars/TalalWasim/Video-FocalNets?style=flat)](https://github.com/TalalWasim/Video-FocalNets) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wasim_Video-FocalNets_Spatio-Temporal_Focal_Modulation_for_Video_Action_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Lip2Vec: Efficient and Robust Visual Speech Recognition via Latent-to-Latent Visual to Audio Representation Mapping | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Djilali_Lip2Vec_Efficient_and_Robust_Visual_Speech_Recognition_via_Latent-to-Latent_Visual_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06112-b31b1b.svg)](https://arxiv.org/abs/2308.06112) | :heavy_minus_sign: |
| Video OWL-ViT: Temporally-Consistent Open-World Localization in Video | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Heigold_Video_OWL-ViT_Temporally-consistent_Open-world_Localization_in_Video_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Tubelet-Contrastive Self-Supervision for Video-Efficient Generalization | [![GitHub](https://img.shields.io/github/stars/fmthoker/tubelet-contrast?style=flat)](https://github.com/fmthoker/tubelet-contrast) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Thoker_Tubelet-Contrastive_Self-Supervision_for_Video-Efficient_Generalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11003-b31b1b.svg)](https://arxiv.org/abs/2303.11003) | :heavy_minus_sign: |
| Memory-and-Anticipation Transformer for Online Action Understanding | [![GitHub](https://img.shields.io/github/stars/Echo0125/Memory-and-Anticipation-Transformer?style=flat)](https://github.com/Echo0125/Memory-and-Anticipation-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Memory-and-Anticipation_Transformer_for_Online_Action_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07893-b31b1b.svg)](https://arxiv.org/abs/2308.07893) | :heavy_minus_sign: |
| Video Action Segmentation via Contextually Refined Temporal Keypoints | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Video_Action_Segmentation_via_Contextually_Refined_Temporal_Keypoints_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Knowing where to Focus: Event-Aware Transformer for Video Grounding | [![GitHub](https://img.shields.io/github/stars/jinhyunj/EaTR?style=flat)](https://github.com/jinhyunj/EaTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jang_Knowing_Where_to_Focus_Event-aware_Transformer_for_Video_Grounding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06947-b31b1b.svg)](https://arxiv.org/abs/2308.06947) | :heavy_minus_sign: |
| MPI-Flow: Learning Realistic Optical Flow with Multiplane Images | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/mpi-flow) <br /> [![GitHub](https://img.shields.io/github/stars/Sharpiless/MPI-Flow?style=flat)](https://github.com/Sharpiless/MPI-Flow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_MPI-Flow_Learning_Realistic_Optical_Flow_with_Multiplane_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06714-b31b1b.svg)](https://arxiv.org/abs/2309.06714) | :heavy_minus_sign: |
| Discovering Spatio-Temporal Rationales for Video Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Discovering_Spatio-Temporal_Rationales_for_Video_Question_Answering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12058-b31b1b.svg)](https://arxiv.org/abs/2307.12058) | :heavy_minus_sign: |
| Scalable Video Object Segmentation with Simplified Framework | [![GitHub](https://img.shields.io/github/stars/jimmy-dq/SimVOS?style=flat)](https://github.com/jimmy-dq/SimVOS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Scalable_Video_Object_Segmentation_with_Simplified_Framework_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09903-b31b1b.svg)](https://arxiv.org/abs/2308.09903) | :heavy_minus_sign: |
| Root Pose Decomposition Towards Generic Non-Rigid 3D Reconstruction with Monocular Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rpd-share.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Root_Pose_Decomposition_Towards_Generic_Non-rigid_3D_Reconstruction_with_Monocular_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10089-b31b1b.svg)](https://arxiv.org/abs/2308.10089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CkGnYxNZv70) |
| Helping Hands: An Object-Aware Ego-Centric Video Recognition Model | [![GitHub](https://img.shields.io/github/stars/Chuhanxx/helping_hand_for_egocentric_videos?style=flat)](https://github.com/Chuhanxx/helping_hand_for_egocentric_videos) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Helping_Hands_An_Object-Aware_Ego-Centric_Video_Recognition_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07918-b31b1b.svg)](https://arxiv.org/abs/2308.07918) | :heavy_minus_sign: |
| Modeling the Relative Visual Tempo for Self-Supervised Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/Zhuysheng/RVTCLR?style=flat)](https://github.com/Zhuysheng/RVTCLR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Modeling_the_Relative_Visual_Tempo_for_Self-supervised_Skeleton-based_Action_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation | [![GitHub](https://img.shields.io/github/stars/lxtGH/Tube-Link?style=flat)](https://github.com/lxtGH/Tube-Link) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Tube-Link_A_Flexible_Cross_Tube_Framework_for_Universal_Video_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12782-b31b1b.svg)](https://arxiv.org/abs/2303.12782) | :heavy_minus_sign: |
| Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning | [![GitHub](https://img.shields.io/github/stars/alibaba-mmai-research/DiST?style=flat)](https://github.com/alibaba-mmai-research/DiST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qing_Disentangling_Spatial_and_Temporal_Learning_for_Efficient_Image-to-Video_Transfer_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07911-b31b1b.svg)](https://arxiv.org/abs/2309.07911) | :heavy_minus_sign: |
| Tem-Adapter: Adapting Image-Text Pretraining for Video Question Answer | [![GitHub](https://img.shields.io/github/stars/XLiu443/Tem-adapter?style=flat)](https://github.com/XLiu443/Tem-adapter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Tem-Adapter_Adapting_Image-Text_Pretraining_for_Video_Question_Answer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08414-b31b1b.svg)](https://arxiv.org/abs/2308.08414) | :heavy_minus_sign: |
