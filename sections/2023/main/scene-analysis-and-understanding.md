# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/medical-and-biological-vision-cell-microscopy.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/multimodal-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Scene Analysis and Understanding

![Section Papers](https://img.shields.io/badge/Section%20Papers-40-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-33-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-30-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-5-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Generalized Few-Shot Point Cloud Segmentation via Geometric Words | [![GitHub](https://img.shields.io/github/stars/Pixie8888/GFS-3DSeg_GWs?style=flat)](https://github.com/Pixie8888/GFS-3DSeg_GWs) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Generalized_Few-Shot_Point_Cloud_Segmentation_via_Geometric_Words_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11222-b31b1b.svg)](https://arxiv.org/abs/2309.11222) | :heavy_minus_sign: |
| Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer | [![GitHub](https://img.shields.io/github/stars/shiyujiao/Boosting3DoFAccuracy?style=flat)](https://github.com/shiyujiao/Boosting3DoFAccuracy) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Boosting_3-DoF_Ground-to-Satellite_Camera_Localization_Accuracy_via_Geometry-Guided_Cross-View_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08015-b31b1b.svg)](https://arxiv.org/abs/2307.08015) | :heavy_minus_sign: |
| EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization | [![GitHub](https://img.shields.io/github/stars/minnjung/EP2P-Loc?style=flat)](https://github.com/minnjung/EP2P-Loc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_EP2P-Loc_End-to-End_3D_Point_to_2D_Pixel_Localization_for_Large-Scale_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07471-b31b1b.svg)](https://arxiv.org/abs/2309.07471) | :heavy_minus_sign: |
| Multi-Task View Synthesis with Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zsh2000.github.io/mtvs.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/zsh2000/MuvieNeRF?style=flat)](https://github.com/zsh2000/MuvieNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Multi-task_View_Synthesis_with_Neural_Radiance_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.17450-b31b1b.svg)](https://arxiv.org/abs/2309.17450) | :heavy_minus_sign: |
| Multi-Task Learning with Knowledge Distillation for Dense Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Multi-Task_Learning_with_Knowledge_Distillation_for_Dense_Prediction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World | [![GitHub](https://img.shields.io/github/stars/Yuqifan1117/CaCao?style=flat)](https://github.com/Yuqifan1117/CaCao) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Visually-Prompted_Language_Model_for_Fine-Grained_Scene_Graph_Generation_in_an_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13233-b31b1b.svg)](https://arxiv.org/abs/2303.13233) | :heavy_minus_sign: |
| CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/XiaRho/CMDA?style=flat)](https://github.com/XiaRho/CMDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_CMDA_Cross-Modality_Domain_Adaptation_for_Nighttime_Semantic_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15942-b31b1b.svg)](https://arxiv.org/abs/2307.15942) | :heavy_minus_sign: |
| VQA-GNN: Reasoning with Multimodal Knowledge via Graph Neural Networks for Visual Question Answering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_VQA-GNN_Reasoning_with_Multimodal_Knowledge_via_Graph_Neural_Networks_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2205.11501-b31b1b.svg)](https://arxiv.org/abs/2205.11501) | :heavy_minus_sign: |
| Disentangle then Parse: Night-Time Semantic Segmentation with Illumination Disentanglement | [![GitHub](https://img.shields.io/github/stars/w1oves/DTP?style=flat)](https://github.com/w1oves/DTP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Disentangle_then_Parse_Night-time_Semantic_Segmentation_with_Illumination_Disentanglement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09362-b31b1b.svg)](https://arxiv.org/abs/2307.09362) | :heavy_minus_sign: |
| Visual Traffic Knowledge Graph Generation from Scene Images | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://www.nlpr.ia.ac.cn/pal/RS10K.html) <br /> | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Visual_Traffic_Knowledge_Graph_Generation_from_Scene_Images_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Agglomerative Transformer for Human-Object Interaction Detection | [![GitHub](https://img.shields.io/github/stars/six6607/AGER?style=flat)](https://github.com/six6607/AGER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tu_Agglomerative_Transformer_for_Human-Object_Interaction_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08370-b31b1b.svg)](https://arxiv.org/abs/2308.08370) | :heavy_minus_sign: |
| 3D Neural Embedding Likelihood: Probabilistic Inverse Graphics for Robust 6D Pose Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://probcomp.github.io/nel/) <br /> [![GitHub](https://img.shields.io/github/stars/google-deepmind/threednel?style=flat)](https://github.com/google-deepmind/threednel) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_3D_Neural_Embedding_Likelihood_Probabilistic_Inverse_Graphics_for_Robust_6D_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.03744-b31b1b.svg)](https://arxiv.org/abs/2302.03744) | :heavy_minus_sign: |
| HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation | [![GitHub](https://img.shields.io/github/stars/franciszzj/HiLo?style=flat)](https://github.com/franciszzj/HiLo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_HiLo_Exploiting_High_Low_Frequency_Relations_for_Unbiased_Panoptic_Scene_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15994-b31b1b.svg)](https://arxiv.org/abs/2303.15994) | :heavy_minus_sign: |
| RLIPv2: Fast Scaling of Relational Language-Image Pre-Training | [![GitHub](https://img.shields.io/github/stars/JacobYuan7/RLIPv2?style=flat)](https://github.com/JacobYuan7/RLIPv2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_RLIPv2_Fast_Scaling_of_Relational_Language-Image_Pre-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09351-b31b1b.svg)](https://arxiv.org/abs/2308.09351) | :heavy_minus_sign: |
| UniSeg: A Unified Multi-Modal LiDAR Segmentation Network and the OpenPCSeg Codebase | [![GitHub](https://img.shields.io/github/stars/PJLab-ADG/PCSeg?style=flat)](https://github.com/PJLab-ADG/PCSeg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_UniSeg_A_Unified_Multi-Modal_LiDAR_Segmentation_Network_and_the_OpenPCSeg_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.05573-b31b1b.svg)](https://arxiv.org/abs/2309.05573) | :heavy_minus_sign: |
| See more and Know More: Zero-Shot Point Cloud Segmentation via Multi-Modal Visual Data | [![GitHub](https://img.shields.io/github/stars/4DVLab/See_More_Know_More?style=flat)](https://github.com/4DVLab/See_More_Know_More) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_See_More_and_Know_More_Zero-shot_Point_Cloud_Segmentation_via_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10782-b31b1b.svg)](https://arxiv.org/abs/2307.10782) | :heavy_minus_sign: |
| Compositional Feature Augmentation for Unbiased Scene Graph Generation | [![GitHub](https://img.shields.io/github/stars/HKUST-LongGroup/CFA?style=flat)](https://github.com/HKUST-LongGroup/CFA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Compositional_Feature_Augmentation_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06712-b31b1b.svg)](https://arxiv.org/abs/2308.06712) | :heavy_minus_sign: |
| Multi-Weather Image Restoration via Domain Translation | [![GitHub](https://img.shields.io/github/stars/pwp1208/Domain_Translation_Multi-weather_Restoration?style=flat)](https://github.com/pwp1208/Domain_Translation_Multi-weather_Restoration) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patil_Multi-weather_Image_Restoration_via_Domain_Translation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| CLIPTER: Looking at the Bigger Picture in Scene Text Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aberdam_CLIPTER_Looking_at_the_Bigger_Picture_in_Scene_Text_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07464-b31b1b.svg)](https://arxiv.org/abs/2301.07464) | :heavy_minus_sign: |
| Towards Models that Can See and Read | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ganz_Towards_Models_that_Can_See_and_Read_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.07389-b31b1b.svg)](https://arxiv.org/abs/2301.07389) | :heavy_minus_sign: |
| SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://weiyithu.github.io/SurroundOcc/) <br /> [![GitHub](https://img.shields.io/github/stars/weiyithu/SurroundOcc?style=flat)](https://github.com/weiyithu/SurroundOcc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_SurroundOcc_Multi-camera_3D_Occupancy_Prediction_for_Autonomous_Driving_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09551-b31b1b.svg)](https://arxiv.org/abs/2303.09551) | :heavy_minus_sign: |
| DDP: Diffusion Model for Dense Visual Prediction | [![GitHub](https://img.shields.io/github/stars/JiYuanFeng/DDP?style=flat)](https://github.com/JiYuanFeng/DDP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ji_DDP_Diffusion_Model_for_Dense_Visual_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17559-b31b1b.svg)](https://arxiv.org/abs/2303.17559) | :heavy_minus_sign: |
| Understanding 3D Object Interaction from a Single Image | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jasonqsy.github.io/3DOI/) <br /> [![GitHub](https://img.shields.io/github/stars/JasonQSY/3DOI?style=flat)](https://github.com/JasonQSY/3DOI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Understanding_3D_Object_Interaction_from_a_Single_Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.09664-b31b1b.svg)](https://arxiv.org/abs/2305.09664) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YDIL93XxHyk) |
| ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://wuqianyi.top/objectsdf++) <br /> [![GitHub](https://img.shields.io/github/stars/QianyiWu/objectsdf_plus?style=flat)](https://github.com/QianyiWu/objectsdf_plus) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_ObjectSDF_Improved_Object-Compositional_Neural_Implicit_Surfaces_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07868-b31b1b.svg)](https://arxiv.org/abs/2308.07868) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aR7TAW-tLkE) |
| Improving Equivariance in State-of-the-Art Supervised Depth and Normal Predictors | [![GitHub](https://img.shields.io/github/stars/mikuhatsune/equivariance?style=flat)](https://github.com/mikuhatsune/equivariance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_Improving_Equivariance_in_State-of-the-Art_Supervised_Depth_and_Normal_Predictors_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16646-b31b1b.svg)](https://arxiv.org/abs/2309.16646) | :heavy_minus_sign: |
| CrossMatch: Source-Free Domain Adaptive Semantic Segmentation via Cross-Modal Consistency Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yin_CrossMatch_Source-Free_Domain_Adaptive_Semantic_Segmentation_via_Cross-Modal_Consistency_Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Semantic Attention Flow Fields for Monocular Dynamic Scene Decomposition | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://visual.cs.brown.edu/projects/semantic-attention-flow-fields-webpage/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Semantic_Attention_Flow_Fields_for_Monocular_Dynamic_Scene_Decomposition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.01526-b31b1b.svg)](https://arxiv.org/abs/2303.01526) | :heavy_minus_sign: |
| Holistic Geometric Feature Learning for Structured Reconstruction | [![GitHub](https://img.shields.io/github/stars/Geo-Tell/F-Learn?style=flat)](https://github.com/Geo-Tell/F-Learn) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Holistic_Geometric_Feature_Learning_for_Structured_Reconstruction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.09622-b31b1b.svg)](https://arxiv.org/abs/2309.09622) | :heavy_minus_sign: |
| Scalable Multi-Temporal Remote Sensing Change Data Generation via Simulating Stochastic Change Process | [![GitHub](https://img.shields.io/github/stars/Z-Zheng/Changen?style=flat)](https://github.com/Z-Zheng/Changen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Scalable_Multi-Temporal_Remote_Sensing_Change_Data_Generation_via_Simulating_Stochastic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.17031-b31b1b.svg)](https://arxiv.org/abs/2309.17031) | :heavy_minus_sign: |
| TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_TaskExpert_Dynamically_Assembling_Multi-Task_Representations_with_Memorial_Mixture-of-Experts_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15324-b31b1b.svg)](https://arxiv.org/abs/2307.15324) | :heavy_minus_sign: |
| Thinking Image Color Aesthetics Assessment: Models, Datasets and Benchmarks | [![GitHub](https://img.shields.io/github/stars/woshidandan/Image-Color-Aesthetics-Assessment?style=flat)](https://github.com/woshidandan/Image-Color-Aesthetics-Assessment) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Thinking_Image_Color_Aesthetics_Assessment_Models_Datasets_and_Benchmarks_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| STEERER: Resolving Scale Variations for Counting and Localization via Selective Inheritance Learning | [![GitHub](https://img.shields.io/github/stars/taohan10200/STEERER?style=flat)](https://github.com/taohan10200/STEERER) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_STEERER_Resolving_Scale_Variations_for_Counting_and_Localization_via_Selective_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10468-b31b1b.svg)](https://arxiv.org/abs/2308.10468) | :heavy_minus_sign: |
| Object-Aware Gaze Target Detection | [![GitHub](https://img.shields.io/github/stars/francescotonini/object-aware-gaze-target-detection?style=flat)](https://github.com/francescotonini/object-aware-gaze-target-detection) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tonini_Object-aware_Gaze_Target_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09662-b31b1b.svg)](https://arxiv.org/abs/2307.09662) | :heavy_minus_sign: |
| Weakly Supervised Referring Image Segmentation with Intra-Chunk and Inter-Chunk Consistency | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Weakly_Supervised_Referring_Image_Segmentation_with_Intra-Chunk_and_Inter-Chunk_Consistency_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Vision Relation Transformer for Unbiased Scene Graph Generation | [![GitHub](https://img.shields.io/github/stars/visinf/veto?style=flat)](https://github.com/visinf/veto) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sudhakaran_Vision_Relation_Transformer_for_Unbiased_Scene_Graph_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09472-b31b1b.svg)](https://arxiv.org/abs/2308.09472) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=N4YqmfDY-t0) |
| DDIT: Semantic Scene Completion via Deformable Deep Implicit Templates | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DDIT_Semantic_Scene_Completion_via_Deformable_Deep_Implicit_Templates_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DQS3D: Densely-Matched Quantization-Aware Semi-Supervised 3D Detection | [![GitHub](https://img.shields.io/github/stars/AIR-DISCOVER/DQS3D?style=flat)](https://github.com/AIR-DISCOVER/DQS3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_DQS3D_Densely-matched_Quantization-aware_Semi-supervised_3D_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13031-b31b1b.svg)](https://arxiv.org/abs/2304.13031) | :heavy_minus_sign: |
| Shape Anchor Guided Holistic Indoor Scene Understanding | [![GitHub](https://img.shields.io/github/stars/Geo-Tell/AncRec?style=flat)](https://github.com/Geo-Tell/AncRec) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Shape_Anchor_Guided_Holistic_Indoor_Scene_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11133-b31b1b.svg)](https://arxiv.org/abs/2309.11133) | :heavy_minus_sign: |
| SGAligner: 3D Scene Alignment with Scene Graphs | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sayandebsarkar.com/sgaligner/) <br /> [![GitHub](https://img.shields.io/github/stars/sayands/sgaligner?style=flat)](https://github.com/sayands/sgaligner) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sarkar_SGAligner_3D_Scene_Alignment_with_Scene_Graphs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.14880-b31b1b.svg)](https://arxiv.org/abs/2304.14880) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Oq89hgocg4Q) |
| Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/betrayed_caption/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/jianzongwu/betrayed-by-captions?style=flat)](https://github.com/jianzongwu/betrayed-by-captions) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Betrayed_by_Captions_Joint_Caption_Grounding_and_Generation_for_Open_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00805-b31b1b.svg)](https://arxiv.org/abs/2301.00805) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=b8WuuvyGp3M) |
