# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/datasets-and-evaluation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/medical-and-biological-vision-cell-microscopy.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Faces and Gestures

![Section Papers](https://img.shields.io/badge/Section%20Papers-45-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-29-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-22-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-7-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| DeePoint: Visual Pointing Recognition and Direction Estimation | [![GitHub](https://img.shields.io/github/stars/kyotovision-public/deepoint?style=flat)](https://github.com/kyotovision-public/deepoint) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakamura_DeePoint_Visual_Pointing_Recognition_and_Direction_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06977-b31b1b.svg)](https://arxiv.org/abs/2304.06977) | :heavy_minus_sign: |
| Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Contactless_Pulse_Estimation_Leveraging_Pseudo_Labels_and_Self-Supervision_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_Most_Important_Person-Guided_Dual-Branch_Cross-Patch_Attention_for_Group_Affect_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07055-b31b1b.svg)](https://arxiv.org/abs/2212.07055) | :heavy_minus_sign: |
| ContactGen: Generative Contact Modeling for Grasp Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stevenlsw.github.io/contactgen/) <br /> [![GitHub](https://img.shields.io/github/stars/stevenlsw/contactgen?style=flat)](https://github.com/stevenlsw/contactgen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2310.03740) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pBgaQdMdB3Q) |
| Imitator: Personalized Speech-Driven 3D Facial Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://balamuruganthambiraja.github.io/Imitator/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Thambiraja_Imitator_Personalized_Speech-driven_3D_Facial_Animation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2301.00023) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JhXTdjiUCUw) |
| DVGaze: Dual-View Gaze Estimation | [![GitHub](https://img.shields.io/github/stars/yihuacheng/DVGaze?style=flat)](https://github.com/yihuacheng/DVGaze) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_DVGaze_Dual-View_Gaze_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10310-b31b1b.svg)](https://arxiv.org/abs/2308.10310) | :heavy_minus_sign: |
| TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective | [![GitHub](https://img.shields.io/github/stars/DanJun6737/TransFace?style=flat)](https://github.com/DanJun6737/TransFace) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dan_TransFace_Calibrating_Transformer_Training_for_Face_Recognition_from_a_Data-Centric_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10133-b31b1b.svg)](https://arxiv.org/abs/2308.10133) | :heavy_minus_sign: |
| Towards Unsupervised Domain Generalization for Face Anti-Spoofing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Towards_Unsupervised_Domain_Generalization_for_Face_Anti-Spoofing_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Reinforced Disentanglement for Face Swapping without Skip Connection | [![GitHub](https://img.shields.io/github/stars/alaist/RD-FS?style=flat)](https://github.com/alaist/RD-FS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Reinforced_Disentanglement_for_Face_Swapping_without_Skip_Connection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07928-b31b1b.svg)](https://arxiv.org/abs/2307.07928) | :heavy_minus_sign: |
| CoSign: Exploring Co-Occurrence Signals in Skeleton-based Continuous Sign Language Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiao_CoSign_Exploring_Co-occurrence_Signals_in_Skeleton-based_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqiaopeng.github.io/emotalk/) <br /> [![GitHub](https://img.shields.io/github/stars/psyai-net/EmoTalk_release?style=flat)](https://github.com/psyai-net/EmoTalk_release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_EmoTalk_Speech-Driven_Emotional_Disentanglement_for_3D_Face_Animation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11089-b31b1b.svg)](https://arxiv.org/abs/2303.11089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0uV2B1m-XjI) |
| LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_LA-Net_Landmark-Aware_Learning_for_Reliable_Facial_Expression_Recognition_under_Label_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09023-b31b1b.svg)](https://arxiv.org/abs/2307.09023) | :heavy_minus_sign: |
| ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling | [![GitHub](https://img.shields.io/github/stars/LiuLinyun/ASM-unofficial?style=flat)](https://github.com/LiuLinyun/ASM-unofficial) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_ASM_Adaptive_Skinning_Model_for_High-Quality_3D_Face_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09423-b31b1b.svg)](https://arxiv.org/abs/2304.09423) | :heavy_minus_sign: |
| Troubleshooting Ethnic Quality Bias with Curriculum Domain Adaptation for Face Image Quality Assessment | [![GitHub](https://img.shields.io/github/stars/oufuzhao/EQBM?style=flat)](https://github.com/oufuzhao/EQBM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ou_Troubleshooting_Ethnic_Quality_Bias_with_Curriculum_Domain_Adaptation_for_Face_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| UniFace: Unified Cross-Entropy Loss for Deep Face Recognition | [![GitHub](https://img.shields.io/github/stars/CVI-SZU/UniFace?style=flat)](https://github.com/CVI-SZU/UniFace) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_UniFace_Unified_Cross-Entropy_Loss_for_Deep_Face_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Human Part-Wise 3D Motion Context Learning for Sign Language Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Human_Part-wise_3D_Motion_Context_Learning_for_Sign_Language_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09305-b31b1b.svg)](https://arxiv.org/abs/2308.09305) | :heavy_minus_sign: |
| Weakly-Supervised Text-Driven Contrastive Learning for Facial Behavior Understanding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Weakly-Supervised_Text-Driven_Contrastive_Learning_for_Facial_Behavior_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00058-b31b1b.svg)](https://arxiv.org/abs/2304.00058) | :heavy_minus_sign: |
| HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zxz267.github.io/HaMuCo/) <br /> [![GitHub](https://img.shields.io/github/stars/zxz267/HaMuCo?style=flat)](https://github.com/zxz267/HaMuCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_HaMuCo_Hand_Pose_Estimation_via_Multiview_Collaborative_Self-Supervised_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.00988-b31b1b.svg)](https://arxiv.org/abs/2302.00988) | :heavy_minus_sign: |
| ReactioNet: Learning High-Order Facial Behavior from Universal Stimulus-Reaction by Dyadic Relation Reasoning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_ReactioNet_Learning_High-Order_Facial_Behavior_from_Universal_Stimulus-Reaction_by_Dyadic_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_CLIP-Cluster_CLIP-Guided_Attribute_Hallucination_for_Face_Clustering_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learning Human Dynamics in Autonomous Driving Scenarios | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_Human_Dynamics_in_Autonomous_Driving_Scenarios_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation | [![GitHub](https://img.shields.io/github/stars/zyhbili/LivelySpeaker?style=flat)](https://github.com/zyhbili/LivelySpeaker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhi_LivelySpeaker_Towards_Semantic-Aware_Co-Speech_Gesture_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.09294-b31b1b.svg)](https://arxiv.org/abs/2309.09294) | :heavy_minus_sign: |
| Controllable Guide-Space for Generalizable Face Forgery Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_Controllable_Guide-Space_for_Generalizable_Face_Forgery_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14039-b31b1b.svg)](https://arxiv.org/abs/2307.14039) | :heavy_minus_sign: |
| Unpaired Multi-Domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map | [![GitHub](https://img.shields.io/github/stars/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap?style=flat)](https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Unpaired_Multi-domain_Attribute_Translation_of_3D_Facial_Shapes_with_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13245-b31b1b.svg)](https://arxiv.org/abs/2308.13245) | :heavy_minus_sign: |
| Emotional Listener Portrait: Neural Listener Head Generation with Emotion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Emotional_Listener_Portrait_Neural_Listener_Head_Generation_with_Emotion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.00068-b31b1b.svg)](https://arxiv.org/abs/2310.00068) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KCzA5dnXf-I) |
| Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://merl.com/demos/steered-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nair_Steered_Diffusion_A_Generalized_Framework_for_Plug-and-Play_Conditional_Image_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.00224-b31b1b.svg)](https://arxiv.org/abs/2310.00224) | :heavy_minus_sign: |
| Invariant Feature Regularization for Fair Face Recognition | [![GitHub](https://img.shields.io/github/stars/PanasonicConnect/InvReg?style=flat)](https://github.com/PanasonicConnect/InvReg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Invariant_Feature_Regularization_for_Fair_Face_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.14652-b31b1b.svg)](https://arxiv.org/abs/2310.14652) | :heavy_minus_sign: |
| Gloss-Free Sign Language Translation: Improving from Visual-Language Pretraining | [![GitHub](https://img.shields.io/github/stars/zhoubenjia/GFSLT-VLP?style=flat)](https://github.com/zhoubenjia/GFSLT-VLP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Gloss-Free_Sign_Language_Translation_Improving_from_Visual-Language_Pretraining_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14768-b31b1b.svg)](https://arxiv.org/abs/2307.14768) | :heavy_minus_sign: |
| Contrastive Pseudo Learning for Open-World DeepFake Attribution | [![GitHub](https://img.shields.io/github/stars/TencentYoutuResearch/OpenWorld-DeepFakeAttribution?style=flat)](https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Contrastive_Pseudo_Learning_for_Open-World_DeepFake_Attribution_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11132-b31b1b.svg)](https://arxiv.org/abs/2309.11132) | :heavy_minus_sign: |
| Continual Learning for Personalized Co-Speech Gesture Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://chahuja.com/cdiffgan/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahuja_Continual_Learning_for_Personalized_Co-speech_Gesture_Generation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| HandR<sup>2</sup>N<sup>2</sup>: Iterative 3D Hand Pose Estimation using a Residual Recurrent Neural Network | [![GitHub](https://img.shields.io/github/stars/cwc1260/HandR2N2?style=flat)](https://github.com/cwc1260/HandR2N2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SPACE: Speech-Driven Portrait Animation with Controllable Expression | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/dir/space/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gururani_SPACE_Speech-driven_Portrait_Animation_with_Controllable_Expression_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09809-b31b1b.svg)](https://arxiv.org/abs/2211.09809) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=DdCvJ8JI2-M) |
| How to Boost Face Recognition with StyleGAN? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://seva100.github.io/stylegan-for-facerec) <br /> [![GitHub](https://img.shields.io/github/stars/seva100/stylegan-for-facerec?style=flat)](https://github.com/seva100/stylegan-for-facerec) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sevastopolskiy_How_to_Boost_Face_Recognition_with_StyleGAN_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.10090-b31b1b.svg)](https://arxiv.org/abs/2210.10090) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Bsi0RMTdEaI) |
| ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.idiap.ch/en/dataset/childplay-gaze) <br /> [![Zenodo](https://img.shields.io/badge/Zenodo-dataset-FFD1BF.svg)](https://zenodo.org/record/8252535) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tafasca_ChildPlay_A_New_Benchmark_for_Understanding_Childrens_Gaze_Behaviour_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.01630-b31b1b.svg)](https://arxiv.org/abs/2307.01630) | :heavy_minus_sign: |
| Robust One-Shot Face Video Re-Enactment using Hybrid Latent Spaces of StyleGAN2 | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://trevineoorloff.github.io/FaceVideoReenactment_HybridLatents.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Oorloff_Robust_One-Shot_Face_Video_Re-enactment_using_Hybrid_Latent_Spaces_of_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07848-b31b1b.svg)](https://arxiv.org/abs/2302.07848) | :heavy_minus_sign: |
| Data-Free Class-Incremental Hand Gesture Recognition | [![GitHub](https://img.shields.io/github/stars/humansensinglab/dfcil-hgr?style=flat)](https://github.com/humansensinglab/dfcil-hgr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://humansensing.cs.cmu.edu/sites/default/files/Data-Free%20Class-Incremental%20Hand%20Gesture%20Recognition_0.pdf) | :heavy_minus_sign: |
| Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition | [![GitHub](https://img.shields.io/github/stars/Carpumpkin/InBoMem?style=flat)](https://github.com/Carpumpkin/InBoMem) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Robust_Representations_with_Information_Bottleneck_and_Memory_Network_for_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Knowledge-Spreader: Learning Semi-Supervised Facial Action Dynamics by Consistifying Knowledge Granularity | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Knowledge-Spreader_Learning_Semi-Supervised_Facial_Action_Dynamics_by_Consistifying_Knowledge_Granularity_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Face Clustering via Graph Convolutional Networks with Confidence Edges | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Face_Clustering_via_Graph_Convolutional_Networks_with_Confidence_Edges_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| StyleGANEX: StyleGAN-based Manipulation Beyond Cropped Aligned Faces | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/styleganex/) <br /> [![GitHub](https://img.shields.io/github/stars/williamyang1991/StyleGANEX?style=flat)](https://github.com/williamyang1991/StyleGANEX) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_StyleGANEX_StyleGAN-Based_Manipulation_Beyond_Cropped_Aligned_Faces_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06146-b31b1b.svg)](https://arxiv.org/abs/2303.06146) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8oK0TXQmxg8) |
| SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes | [![GitHub](https://img.shields.io/github/stars/anonymous-author-sub/seeable?style=flat)](https://github.com/anonymous-author-sub/seeable) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Larue_SeeABLE_Soft_Discrepancies_and_Bounded_Contrastive_Learning_for_Exposing_Deepfakes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11296-b31b1b.svg)](https://arxiv.org/abs/2211.11296) | :heavy_minus_sign: |
| Adaptive Nonlinear Latent Transformation for Conditional Face Editing | [![GitHub](https://img.shields.io/github/stars/Hzzone/AdaTrans?style=flat)](https://github.com/Hzzone/AdaTrans) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Adaptive_Nonlinear_Latent_Transformation_for_Conditional_Face_Editing_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07790-b31b1b.svg)](https://arxiv.org/abs/2307.07790) | :heavy_minus_sign: |
| Semi-Supervised Speech-Driven 3D Facial Animation via Cross-Modal Encoding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Semi-supervised_Speech-driven_3D_Facial_Animation_via_Cross-modal_Encoding_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ICD-Face: Intra-Class Compactness Distillation for Face Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_ICD-Face_Intra-class_Compactness_Distillation_for_Face_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| C<sup>2</sup>ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_C2ST_Cross-Modal_Contextualized_Sequence_Transduction_for_Continuous_Sign_Language_Recognition_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
