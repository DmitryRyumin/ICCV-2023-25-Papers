# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/recognition-detection.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/vision-and-audio.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Image and Video Synthesis

![Section Papers](https://img.shields.io/badge/Section%20Papers-135-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-118-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-104-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-30-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Text-Driven Generative Domain Adaptation with Spectral Consistency Regularization | [![GitHub](https://img.shields.io/github/stars/Victarry/Adaptation-SCR?style=flat)](https://github.com/Victarry/Adaptation-SCR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Text-Driven_Generative_Domain_Adaptation_with_Spectral_Consistency_Regularization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Silver_MosaiQ_Quantum_Generative_Adversarial_Networks_for_Image_Generation_on_NISQ_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11096-b31b1b.svg)](https://arxiv.org/abs/2308.11096) | :heavy_minus_sign: |
| Controllable Visual-Tactile Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://visual-tactile-synthesis.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuihanGao/visual-tactile-synthesis?style=flat)](https://github.com/RuihanGao/visual-tactile-synthesis) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Controllable_Visual-Tactile_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.03051-b31b1b.svg)](https://arxiv.org/abs/2305.03051) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TdwPfwsGX3I) |
| Editing Implicit Assumptions in Text-to-Image Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://time-diffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/bahjat-kawar/time-diffusion?style=flat)](https://github.com/bahjat-kawar/time-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Orgad_Editing_Implicit_Assumptions_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08084-b31b1b.svg)](https://arxiv.org/abs/2303.08084) | :heavy_minus_sign: |
| DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars | [![GitHub](https://img.shields.io/github/stars/SamsungLabs/DINAR?style=flat)](https://github.com/SamsungLabs/DINAR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Svitov_DINAR_Diffusion_Inpainting_of_Neural_Textures_for_One-Shot_Human_Avatars_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09375-b31b1b.svg)](https://arxiv.org/abs/2303.09375) | :heavy_minus_sign: |
| Smoothness Similarity Regularization for Few-Shot GAN Adaptation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sushko_Smoothness_Similarity_Regularization_for_Few-Shot_GAN_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09717-b31b1b.svg)](https://arxiv.org/abs/2308.09717) | :heavy_minus_sign: |
| HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_HSR-Diff_Hyperspectral_Image_Super-Resolution_via_Conditional_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.12085-b31b1b.svg)](https://arxiv.org/abs/2306.12085) | :heavy_minus_sign: |
| Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yorkucvil.github.io/Photoconsistent-NVS/) <br /> [![GitHub](https://img.shields.io/github/stars/YorkUCVIL/Photoconsistent-NVS?style=flat)](https://github.com/YorkUCVIL/Photoconsistent-NVS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Long-Term_Photometric_Consistent_Novel_View_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10700-b31b1b.svg)](https://arxiv.org/abs/2304.10700) | :heavy_minus_sign: |
| AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_AutoDiffusion_Training-Free_Optimization_of_Time_Steps_and_Architectures_for_Automated_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.10438-b31b1b.svg)](https://arxiv.org/abs/2309.10438) | :heavy_minus_sign: |
| Collecting the Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures | [![GitHub](https://img.shields.io/github/stars/NannanLi999/pt_square?style=flat)](https://github.com/NannanLi999/pt_square) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Collecting_The_Puzzle_Pieces_Disentangled_Self-Driven_Human_Pose_Transfer_by_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01887-b31b1b.svg)](https://arxiv.org/abs/2210.01887) | :heavy_minus_sign: |
| Multi-Directional Subspace Editing in Style-Space | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chennaveh.github.io/MDSE/) <br /> [![GitHub](https://img.shields.io/github/stars/chennaveh/MDSE?style=flat)](https://github.com/chennaveh/MDSE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Naveh_Multi-Directional_Subspace_Editing_in_Style-Space_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11825-b31b1b.svg)](https://arxiv.org/abs/2211.11825) | :heavy_minus_sign: |
| HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stelabou.github.io/hyperreenact.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/StelaBou/HyperReenact?style=flat)](https://github.com/StelaBou/HyperReenact) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bounareli_HyperReenact_One-Shot_Reenactment_via_Jointly_Learning_to_Refine_and_Retarget_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10797-b31b1b.svg)](https://arxiv.org/abs/2307.10797) | :heavy_minus_sign: |
| Generating Realistic Images from in-the-Wild Sounds | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Generating_Realistic_Images_from_In-the-wild_Sounds_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02405-b31b1b.svg)](https://arxiv.org/abs/2309.02405) | :heavy_minus_sign: |
| CC3D: Layout-Conditioned Generation of Compositional 3D Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sherwinbahmani.github.io/cc3d/) <br /> [![GitHub](https://img.shields.io/github/stars/sherwinbahmani/cc3d?style=flat)](https://github.com/sherwinbahmani/cc3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bahmani_CC3D_Layout-Conditioned_Generation_of_Compositional_3D_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12074-b31b1b.svg)](https://arxiv.org/abs/2303.12074) | :heavy_minus_sign: |
| UMFuse: Unified Multi View Fusion for Human Editing Applications | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mdsrlab.github.io/2023/08/13/UMFuse-ICCV.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jain_UMFuse_Unified_Multi_View_Fusion_for_Human_Editing_Applications_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.10157-b31b1b.svg)](https://arxiv.org/abs/2211.10157) | :heavy_minus_sign: |
| Evaluating Data Attribution for Text-to-Image Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://peterwang512.github.io/GenDataAttribution/) <br /> [![GitHub](https://img.shields.io/github/stars/PeterWang512/GenDataAttribution?style=flat)](https://github.com/PeterWang512/GenDataAttribution) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Evaluating_Data_Attribution_for_Text-to-Image_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.09345-b31b1b.svg)](https://arxiv.org/abs/2306.09345) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iO6fiSyyv40) |
| Neural Characteristic Function Learning for Conditional Image Generation | [![GitHub](https://img.shields.io/github/stars/Zhangjialu126/ccf_gan?style=flat)](https://github.com/Zhangjialu126/ccf_gan) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Neural_Characteristic_Function_Learning_for_Conditional_Image_Generation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| WaveIPT: Joint Attention and Flow Alignment in the Wavelet Domain for Pose Transfer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_WaveIPT_Joint_Attention_and_Flow_Alignment_in_the_Wavelet_domain_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/LayoutGeneration/tree/main/LayoutDiffusion) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/LayoutGeneration?style=flat)](https://github.com/microsoft/LayoutGeneration) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_LayoutDiffusion_Improving_Graphic_Layout_Generation_by_Discrete_Diffusion_Probabilistic_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11589-b31b1b.svg)](https://arxiv.org/abs/2303.11589) | :heavy_minus_sign: |
| Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation | [![GitHub](https://img.shields.io/github/stars/AiArt-HDU/HIDA?style=flat)](https://github.com/AiArt-HDU/HIDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Human-Inspired_Facial_Sketch_Synthesis_with_Dynamic_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00216-b31b1b.svg)](https://arxiv.org/abs/2309.00216) | :heavy_minus_sign: |
| Conceptual and Hierarchical Latent Space Decomposition for Face Editing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ozkan_Conceptual_and_Hierarchical_Latent_Space_Decomposition_for_Face_Editing_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jeon_Improving_Diversity_in_Zero-Shot_GAN_Adaptation_with_Semantic_Variations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10554-b31b1b.svg)](https://arxiv.org/abs/2308.10554) | :heavy_minus_sign: |
| BallGAN: 3D-Aware Image Synthesis with a Spherical Background | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://minjung-s.github.io/ballgan) <br /> [![GitHub](https://img.shields.io/github/stars/minjung-s/BallGAN?style=flat)](https://github.com/minjung-s/BallGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shin_BallGAN_3D-aware_Image_Synthesis_with_a_Spherical_Background_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.09091-b31b1b.svg)](https://arxiv.org/abs/2301.09091) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RUIWWMiomuY) |
| End-to-End Diffusion Latent Optimization Improves Classifier Guidance | [![GitHub](https://img.shields.io/github/stars/salesforce/DOODL?style=flat)](https://github.com/salesforce/DOODL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wallace_End-to-End_Diffusion_Latent_Optimization_Improves_Classifier_Guidance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13703-b31b1b.svg)](https://arxiv.org/abs/2303.13703) | :heavy_minus_sign: |
| Deep Geometrized Cartoon Line Inbetweening | [![GitHub](https://img.shields.io/github/stars/lisiyao21/AnimeInbet?style=flat)](https://github.com/lisiyao21/AnimeInbet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Siyao_Deep_Geometrized_Cartoon_Line_Inbetweening_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16643-b31b1b.svg)](https://arxiv.org/abs/2309.16643) | :heavy_minus_sign: |
| UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://unitedhuman.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/UnitedHuman/UnitedHuman?style=flat)](https://github.com/UnitedHuman/UnitedHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_UnitedHuman_Harnessing_Multi-Source_Data_for_High-Resolution_Human_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14335-b31b1b.svg)](https://arxiv.org/abs/2309.14335) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pdsfUYFDLSw) |
| Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Towards_Authentic_Face_Restoration_with_Iterative_Diffusion_Models_and_Beyond_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08996-b31b1b.svg)](https://arxiv.org/abs/2307.08996) | :heavy_minus_sign: |
| SVDiff: Compact Parameter Space for Diffusion Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/mkshing/svdiff-pytorch?style=flat)](https://github.com/mkshing/svdiff-pytorch) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_SVDiff_Compact_Parameter_Space_for_Diffusion_Fine-Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11305-b31b1b.svg)](https://arxiv.org/abs/2303.11305) | :heavy_minus_sign: |
| MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices | [![GitHub](https://img.shields.io/github/stars/Picsart-AI-Research/MI-GAN?style=flat)](https://github.com/Picsart-AI-Research/MI-GAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sargsyan_MI-GAN_A_Simple_Baseline_for_Image_Inpainting_on_Mobile_Devices_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Structure and Content-Guided Video Synthesis with Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.runwayml.com/gen1) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Esser_Structure_and_Content-Guided_Video_Synthesis_with_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.03011-b31b1b.svg)](https://arxiv.org/abs/2302.03011) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y2_JmgzTeeo) |
| Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yuxinn-j.github.io/projects/Scenimefy.html) <br /> [![GitHub](https://img.shields.io/github/stars/Yuxinn-J/Scenimefy?style=flat)](https://github.com/Yuxinn-J/Scenimefy) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/YuxinJ/Scenimefy) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Scenimefy_Learning_to_Craft_Anime_Scene_via_Semi-Supervised_Image-to-Image_Translation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12968-b31b1b.svg)](https://arxiv.org/abs/2308.12968) | :heavy_minus_sign: |
| Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_Efficient-VQGAN_Towards_High-Resolution_Image_Generation_with_Efficient_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.05400-b31b1b.svg)](https://arxiv.org/abs/2310.05400) | :heavy_minus_sign: |
| A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance | [![GitHub](https://img.shields.io/github/stars/humansensinglab/cycle-diffusion?style=flat)](https://github.com/humansensinglab/cycle-diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_A_Latent_Space_of_Stochastic_Diffusion_Models_for_Zero-Shot_Image_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Generative Multiplane Neural Radiance for 3D-Aware Image Generation | [![GitHub](https://img.shields.io/github/stars/VIROBO-15/GMNR?style=flat)](https://github.com/VIROBO-15/GMNR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kumar_Generative_Multiplane_Neural_Radiance_for_3D-Aware_Image_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01172-b31b1b.svg)](https://arxiv.org/abs/2304.01172) | :heavy_minus_sign: |
| Parallax-Tolerant Unsupervised Deep Image Stitching | [![GitHub](https://img.shields.io/github/stars/nie-lang/UDIS2?style=flat)](https://github.com/nie-lang/UDIS2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nie_Parallax-Tolerant_Unsupervised_Deep_Image_Stitching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.08207-b31b1b.svg)](https://arxiv.org/abs/2302.08207) | :heavy_minus_sign: |
| GAIT: Generating Aesthetic Indoor Tours with Deep Reinforcement Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://desaixie.github.io/gait-rl/) <br /> [![GitHub](https://img.shields.io/github/stars/desaixie/gait?style=flat)](https://github.com/desaixie/gait) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_GAIT_Generating_Aesthetic_Indoor_Tours_with_Deep_Reinforcement_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EverLight: Indoor-Outdoor Editable HDR Lighting Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/everlight/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dastjerdi_EverLight_Indoor-Outdoor_Editable_HDR_Lighting_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13207-b31b1b.svg)](https://arxiv.org/abs/2304.13207) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Mk2ZhXxzLRY) |
| Prompt Tuning Inversion for Text-Driven Image Editing using Diffusion Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Prompt_Tuning_Inversion_for_Text-driven_Image_Editing_Using_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.04441-b31b1b.svg)](https://arxiv.org/abs/2305.04441) | :heavy_minus_sign: |
| Efficient Diffusion Training via Min-SNR Weighting Strategy | [![GitHub](https://img.shields.io/github/stars/TiankaiHang/Min-SNR-Diffusion-Training?style=flat)](https://github.com/TiankaiHang/Min-SNR-Diffusion-Training) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hang_Efficient_Diffusion_Training_via_Min-SNR_Weighting_Strategy_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09556-b31b1b.svg)](https://arxiv.org/abs/2303.09556) | :heavy_minus_sign: |
| BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion | [![GitHub](https://img.shields.io/github/stars/showlab/BoxDiff?style=flat)](https://github.com/showlab/BoxDiff) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_BoxDiff_Text-to-Image_Synthesis_with_Training-Free_Box-Constrained_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10816-b31b1b.svg)](https://arxiv.org/abs/2307.10816) | :heavy_minus_sign: |
| Improving Sample Quality of Diffusion Models using Self-Attention Guidance | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ku-cvlab.github.io/Self-Attention-Guidance/) <br /> [![GitHub](https://img.shields.io/github/stars/KU-CVLAB/Self-Attention-Guidance?style=flat)](https://github.com/KU-CVLAB/Self-Attention-Guidance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Improving_Sample_Quality_of_Diffusion_Models_Using_Self-Attention_Guidance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.00939-b31b1b.svg)](https://arxiv.org/abs/2210.00939) | :heavy_minus_sign: |
| Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation | [![GitHub](https://img.shields.io/github/stars/EnVision-Research/Selective-Diffusion-Distillation?style=flat)](https://github.com/EnVision-Research/Selective-Diffusion-Distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Not_All_Steps_are_Created_Equal_Selective_Diffusion_Distillation_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08448-b31b1b.svg)](https://arxiv.org/abs/2307.08448) | :heavy_minus_sign: |
| Deep Image Harmonization with Learnable Augmentation | [![GitHub](https://img.shields.io/github/stars/bcmi/SycoNet-Adaptive-Image-Harmonization?style=flat)](https://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Niu_Deep_Image_Harmonization_with_Learnable_Augmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.00376-b31b1b.svg)](https://arxiv.org/abs/2308.00376) | :heavy_minus_sign: |
| Out-of-Domain GAN Inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation | [![GitHub](https://img.shields.io/github/stars/AbnerVictor/OOD-GAN-inversion?style=flat)](https://github.com/AbnerVictor/OOD-GAN-inversion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Out-of-Domain_GAN_Inversion_via_Invertibility_Decomposition_for_Photo-Realistic_Human_Face_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.09262-b31b1b.svg)](https://arxiv.org/abs/2212.09262) | :heavy_minus_sign: |
| Bidirectionally Deformable Motion Modulation for Video-based Human Pose Transfer | [![GitHub](https://img.shields.io/github/stars/rocketappslab/bdmm?style=flat)](https://github.com/rocketappslab/bdmm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Bidirectionally_Deformable_Motion_Modulation_For_Video-based_Human_Pose_Transfer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07754-b31b1b.svg)](https://arxiv.org/abs/2307.07754) | :heavy_minus_sign: |
| Size does Matter: Size-Aware Virtual Try-On via Clothing-Oriented Transformation Try-On Network | [![GitHub](https://img.shields.io/github/stars/cotton6/COTTON-size-does-matter?style=flat)](https://github.com/cotton6/COTTON-size-does-matter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Size_Does_Matter_Size-aware_Virtual_Try-on_via_Clothing-oriented_Transformation_Try-on_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cyberiada.github.io/VidStyleODE/) <br /> [![GitHub](https://img.shields.io/github/stars/MoayedHajiAli/VidStyleODE-official?style=flat)](https://github.com/MoayedHajiAli/VidStyleODE-official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ali_VidStyleODE_Disentangled_Video_Editing_via_StyleGAN_and_NeuralODEs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06020-b31b1b.svg)](https://arxiv.org/abs/2304.06020) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Cfh-mgr1isc) |
| Learning Global-Aware Kernel for Image Harmonization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shen_Learning_Global-aware_Kernel_for_Image_Harmonization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.11676-b31b1b.svg)](https://arxiv.org/abs/2305.11676) | :heavy_minus_sign: |
| Expressive Text-to-Image Generation with Rich Text | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rich-text-to-image.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/SongweiGe/rich-text-to-image?style=flat)](https://github.com/SongweiGe/rich-text-to-image) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/songweig/rich-text-to-image) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Expressive_Text-to-Image_Generation_with_Rich_Text_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06720-b31b1b.svg)](https://arxiv.org/abs/2304.06720) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ihDbAUh0LXk) |
| A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ommo.luchongshan.com/) <br /> [![GitHub](https://img.shields.io/github/stars/luchongshan/OMMO?style=flat)](https://github.com/luchongshan/OMMO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_A_Large-Scale_Outdoor_Multi-Modal_Dataset_and_Benchmark_for_Novel_View_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.06782-b31b1b.svg)](https://arxiv.org/abs/2301.06782) | [![Loom](https://a11ybadges.com/badge?logo=loom)](https://www.loom.com/share/7b9ed35bfb3649eda051398d3a51cda7) |
| Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fictionarry.github.io/ER-NeRF/) <br /> [![GitHub](https://img.shields.io/github/stars/Fictionarry/ER-NeRF?style=flat)](https://github.com/Fictionarry/ER-NeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Efficient_Region-Aware_Neural_Radiance_Fields_for_High-Fidelity_Talking_Portrait_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09323-b31b1b.svg)](https://arxiv.org/abs/2307.09323) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Gc2d3Z8MMuI) |
| Perceptual Artifacts Localization for Image Synthesis Tasks | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://owenzlz.github.io/PAL4VST/) <br /> [![GitHub](https://img.shields.io/github/stars/owenzlz/PAL4VST?style=flat)](https://github.com/owenzlz/PAL4VST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Perceptual_Artifacts_Localization_for_Image_Synthesis_Tasks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.05590-b31b1b.svg)](https://arxiv.org/abs/2310.05590) | :heavy_minus_sign: |
| Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pmh9960.github.io/research/GCDP/) <br /> [![GitHub](https://img.shields.io/github/stars/pmh9960/GCDP?style=flat)](https://github.com/pmh9960/GCDP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Learning_to_Generate_Semantic_Layouts_for_Higher_Text-Image_Correspondence_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08157-b31b1b.svg)](https://arxiv.org/abs/2308.08157) | :heavy_minus_sign: |
| StylerDALLE: Language-Guided Style Transfer using a Vector-Quantized Tokenizer of a Large-Scale Generative Model | [![GitHub](https://img.shields.io/github/stars/zipengxuc/StylerDALLE?style=flat)](https://github.com/zipengxuc/StylerDALLE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_StylerDALLE_Language-Guided_Style_Transfer_Using_a_Vector-Quantized_Tokenizer_of_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09268-b31b1b.svg)](https://arxiv.org/abs/2303.09268) | :heavy_minus_sign: |
| Shortcut-V2V: Compression Framework for Video-to-Video Translation based on Temporal Redundancy Reduction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shortcut-v2v.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/indigopyj/Shortcut-V2V?style=flat)](https://github.com/indigopyj/Shortcut-V2V) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chung_Shortcut-V2V_Compression_Framework_for_Video-to-Video_Translation_Based_on_Temporal_Redundancy_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08011-b31b1b.svg)](https://arxiv.org/abs/2308.08011) | :heavy_minus_sign: |
| Tune-a-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tuneavideo.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/showlab/Tune-A-Video?style=flat)](https://github.com/showlab/Tune-A-Video) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Tune-A-Video_One-Shot_Tuning_of_Image_Diffusion_Models_for_Text-to-Video_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.11565-b31b1b.svg)](https://arxiv.org/abs/2212.11565) | :heavy_minus_sign: |
| BlendFace: Re-Designing Identity Encoders for Face-Swapping | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mapooon.github.io/BlendFacePage/) <br /> [![GitHub](https://img.shields.io/github/stars/mapooon/BlendFace?style=flat)](https://github.com/mapooon/BlendFace) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shiohara_BlendFace_Re-designing_Identity_Encoders_for_Face-Swapping_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10854-b31b1b.svg)](https://arxiv.org/abs/2307.10854) | :heavy_minus_sign: |
| Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zxyin.github.io/TH-PAD/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Talking_Head_Generation_with_Probabilistic_Audio-to-Visual_Diffusion_Priors_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04248-b31b1b.svg)](https://arxiv.org/abs/2212.04248) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CrLXg7Cq8w8) |
| LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhujiapeng.github.io/linkgan/) <br /> [![GitHub](https://img.shields.io/github/stars/zhujiapeng/linkgan?style=flat)](https://github.com/zhujiapeng/linkgan) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_LinkGAN_Linking_GAN_Latents_to_Pixels_for_Controllable_Image_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.04604-b31b1b.svg)](https://arxiv.org/abs/2301.04604) | :heavy_minus_sign: |
| Open-Vocabulary Object Segmentation with Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lipurple.github.io/Grounded_Diffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/Lipurple/Grounded-Diffusion?style=flat)](https://github.com/Lipurple/Grounded-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Open-vocabulary_Object_Segmentation_with_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.05221-b31b1b.svg)](https://arxiv.org/abs/2301.05221) | :heavy_minus_sign: |
| StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models | [![GitHub](https://img.shields.io/github/stars/rafaelheid-it/StyleDiffusion?style=flat)](https://github.com/rafaelheid-it/StyleDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_StyleDiffusion_Controllable_Disentangled_Style_Transfer_via_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07863-b31b1b.svg)](https://arxiv.org/abs/2308.07863) | :heavy_minus_sign: |
| ToonTalker: Cross-Domain Face Reenactment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://opentalker.github.io/ToonTalker/) <br /> [![GitHub](https://img.shields.io/github/stars/OpenTalker/ToonTalker?style=flat)](https://github.com/OpenTalker/ToonTalker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_ToonTalker_Cross-Domain_Face_Reenactment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12866-b31b1b.svg)](https://arxiv.org/abs/2308.12866) | :heavy_minus_sign: |
| Dense Text-to-Image Generation with Attention Modulation | [![GitHub](https://img.shields.io/github/stars/naver-ai/DenseDiffusion?style=flat)](https://github.com/naver-ai/DenseDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12964-b31b1b.svg)](https://arxiv.org/abs/2308.12964) | :heavy_minus_sign: |
| Householder Projector for Unsupervised Latent Semantics Discovery | [![GitHub](https://img.shields.io/github/stars/KingJamesSong/HouseholderGAN?style=flat)](https://github.com/KingJamesSong/HouseholderGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Householder_Projector_for_Unsupervised_Latent_Semantics_Discovery_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08012-b31b1b.svg)](https://arxiv.org/abs/2307.08012) | :heavy_minus_sign: |
| Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation | [![GitHub](https://img.shields.io/github/stars/bcmi/Image-Harmonization-Dataset-ccHarmony?style=flat)](https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Niu_Deep_Image_Harmonization_with_Globally_Guided_Feature_Transformation_and_Relation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.00356-b31b1b.svg)](https://arxiv.org/abs/2308.00356) | :heavy_minus_sign: |
| One-Shot Generative Domain Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://genforce.github.io/genda/) <br /> [![GitHub](https://img.shields.io/github/stars/genforce/genda?style=flat)](https://github.com/genforce/genda) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_One-Shot_Generative_Domain_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2111.09876-b31b1b.svg)](https://arxiv.org/abs/2111.09876) | :heavy_minus_sign: |
| Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lightbulb12294.github.io/hashing-nvd/) <br /> [![GitHub](https://img.shields.io/github/stars/vllab/hashing-nvd?style=flat)](https://github.com/vllab/hashing-nvd) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chan_Hashing_Neural_Video_Decomposition_with_Multiplicative_Residuals_in_Space-Time_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14022-b31b1b.svg)](https://arxiv.org/abs/2309.14022) | :heavy_minus_sign: |
| Versatile Diffusion: Text, Images and Variations All in One Diffusion Model | [![GitHub](https://img.shields.io/github/stars/SHI-Labs/Versatile-Diffusion?style=flat)](https://github.com/SHI-Labs/Versatile-Diffusion) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/shi-labs/Versatile-Diffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Versatile_Diffusion_Text_Images_and_Variations_All_in_One_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.08332-b31b1b.svg)](https://arxiv.org/abs/2211.08332) | :heavy_minus_sign: |
| Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn?style=flat)](https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Harnessing_the_Spatial-Temporal_Attention_of_Diffusion_Models_for_High-Fidelity_Text-to-Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03869-b31b1b.svg)](https://arxiv.org/abs/2304.03869) | :heavy_minus_sign: |
| FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model | [![GitHub](https://img.shields.io/github/stars/vvictoryuki/FreeDoM?style=flat)](https://github.com/vvictoryuki/FreeDoM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09833-b31b1b.svg)](https://arxiv.org/abs/2303.09833) | :heavy_minus_sign: |
| MasaCtrl: Tuning-Free Mutual Self-Attention Control for Consistent Image Synthesis and Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ljzycmd.github.io/projects/MasaCtrl/) <br /> [![GitHub](https://img.shields.io/github/stars/TencentARC/MasaCtrl?style=flat)](https://github.com/TencentARC/MasaCtrl)  <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/TencentARC/MasaCtrl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_MasaCtrl_Tuning-Free_Mutual_Self-Attention_Control_for_Consistent_Image_Synthesis_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08465-b31b1b.svg)](https://arxiv.org/abs/2304.08465) | :heavy_minus_sign: |
| Personalized Image Generation for Color Vision Deficiency Population | [![GitHub](https://img.shields.io/github/stars/Jiangshuyi0V0/CVD-GAN?style=flat)](https://github.com/Jiangshuyi0V0/CVD-GAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Personalized_Image_Generation_for_Color_Vision_Deficiency_Population_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ReNeRF: Relightable Neural Radiance Fields with Nearfield Lighting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ReNeRF_Relightable_Neural_Radiance_Fields_with_Nearfield_Lighting_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iPBesfjNVXM) |
| MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://magicfusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/MagicFusion/MagicFusion.github.io?style=flat)](https://github.com/MagicFusion/MagicFusion.github.io) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MagicFusion_Boosting_Text-to-Image_Generation_Performance_by_Fusing_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13126-b31b1b.svg)](https://arxiv.org/abs/2303.13126) | :heavy_minus_sign: |
| PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap using Pose-Preserved Text-to-Image Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://gwang-kim.github.io/podia_3d/) <br /> [![GitHub](https://img.shields.io/github/stars/gwang-kim/PODIA-3D?style=flat)](https://github.com/gwang-kim/PODIA-3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_PODIA-3D_Domain_Adaptation_of_3D_Generative_Model_Across_Large_Domain_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01900-b31b1b.svg)](https://arxiv.org/abs/2304.01900) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KNpbtqeDshk) |
| Pluralistic Aging Diffusion Autoencoder | [![GitHub](https://img.shields.io/github/stars/raywang335/PADA?style=flat)](https://github.com/raywang335/PADA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Pluralistic_Aging_Diffusion_Autoencoder_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11086-b31b1b.svg)](https://arxiv.org/abs/2303.11086) | :heavy_minus_sign: |
| DPM-OT: A New Diffusion Probabilistic Model based on Optimal Transport | [![GitHub](https://img.shields.io/github/stars/cognaclee/DPM-OT?style=flat)](https://github.com/cognaclee/DPM-OT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_DPM-OT_A_New_Diffusion_Probabilistic_Model_Based_on_Optimal_Transport_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11308-b31b1b.svg)](https://arxiv.org/abs/2307.11308) | :heavy_minus_sign: |
| Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yuangan.github.io/eat/) <br /> [![GitHub](https://img.shields.io/github/stars/yuangan/EAT_code?style=flat)](https://github.com/yuangan/EAT_code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gan_Efficient_Emotional_Adaptation_for_Audio-Driven_Talking-Head_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.04946-b31b1b.svg)](https://arxiv.org/abs/2309.04946) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lp2nSLZp-88) |
| DiFaReli: Diffusion Face Relighting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusion-face-relighting.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/diffusion-face-relighting/difareli_code?style=flat)](https://github.com/diffusion-face-relighting/difareli_code) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ponglertnapakorn_DiFaReli_Diffusion_Face_Relighting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09479-b31b1b.svg)](https://arxiv.org/abs/2304.09479) | :heavy_minus_sign: |
| TALL: Thumbnail Layout for Deepfake Video Detection | [![GitHub](https://img.shields.io/github/stars/rainy-xu/TALL4Deepfake?style=flat)](https://github.com/rainy-xu/TALL4Deepfake) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07494-b31b1b.svg)](https://arxiv.org/abs/2307.07494) | :heavy_minus_sign: |
| LAW-Diffusion: Complex Scene Generation by Diffusion with Layouts | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_LAW-Diffusion_Complex_Scene_Generation_by_Diffusion_with_Layouts_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06713-b31b1b.svg)](https://arxiv.org/abs/2308.06713) | :heavy_minus_sign: |
| DreamPose: Fashion Video Synthesis with Stable Diffusion | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://grail.cs.washington.edu/projects/dreampose/) <br /> [![GitHub](https://img.shields.io/github/stars/johannakarras/DreamPose?style=flat)](https://github.com/johannakarras/DreamPose) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Karras_DreamPose_Fashion_Video_Synthesis_with_Stable_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06025-b31b1b.svg)](https://arxiv.org/abs/2304.06025) | :heavy_minus_sign: |
| Ablating Concepts in Text-to-Image Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.cmu.edu/~concept-ablation/) <br /> [![GitHub](https://img.shields.io/github/stars/nupurkmr9/concept-ablation?style=flat)](https://github.com/nupurkmr9/concept-ablation) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/nupurkmr9/concept-ablation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kumari_Ablating_Concepts_in_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13516-b31b1b.svg)](https://arxiv.org/abs/2303.13516) | :heavy_minus_sign: |
| DReg-NeRF: Deep Registration for Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aibluefisher.github.io/DReg-NeRF/) <br /> [![GitHub](https://img.shields.io/github/stars/AIBluefisher/DReg-NeRF?style=flat)](https://github.com/AIBluefisher/DReg-NeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_DReg-NeRF_Deep_Registration_for_Neural_Radiance_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09386-b31b1b.svg)](https://arxiv.org/abs/2308.09386) | :heavy_minus_sign: |
| The Euclidean Space is Evil: Hyperbolic Attribute Editing for Few-Shot Image Generation | [![GitHub](https://img.shields.io/github/stars/lingxiao-li/HAE?style=flat)](https://github.com/lingxiao-li/HAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_The_Euclidean_Space_is_Evil_Hyperbolic_Attribute_Editing_for_Few-shot_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12347-b31b1b.svg)](https://arxiv.org/abs/2211.12347) | :heavy_minus_sign: |
| Discriminative Class Tokens for Text-to-Image Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vesteinn.github.io/disco/) <br /> [![GitHub](https://img.shields.io/github/stars/idansc/discriminative_class_tokens?style=flat)](https://github.com/idansc/discriminative_class_tokens) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Schwartz_Discriminative_Class_Tokens_for_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17155-b31b1b.svg)](https://arxiv.org/abs/2303.17155) | :heavy_minus_sign: |
| General Image-to-Image Translation with One-Shot Image Guidance | [![GitHub](https://img.shields.io/github/stars/CrystalNeuro/visual-concept-translator?style=flat)](https://github.com/CrystalNeuro/visual-concept-translator) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_General_Image-to-Image_Translation_with_One-Shot_Image_Guidance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14352-b31b1b.svg)](https://arxiv.org/abs/2307.14352) | :heavy_minus_sign: |
| Text2Performer: Text-Driven Human Video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yumingj.github.io/projects/Text2Performer.html) <br /> [![GitHub](https://img.shields.io/github/stars/yumingj/Text2Performer?style=flat)](https://github.com/yumingj/Text2Performer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Text2Performer_Text-Driven_Human_Video_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08483-b31b1b.svg)](https://arxiv.org/abs/2304.08483) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YwhaJUk_qo0) |
| AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks | [![GitHub](https://img.shields.io/github/stars/kibeom-hong/aespa-net?style=flat)](https://github.com/kibeom-hong/aespa-net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_AesPA-Net_Aesthetic_Pattern-Aware_Style_Transfer_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09724-b31b1b.svg)](https://arxiv.org/abs/2307.09724) | :heavy_minus_sign: |
| Controllable Person Image Synthesis with Pose-Constrained Latent Diffusion | [![GitHub](https://img.shields.io/github/stars/BrandonHanx/PoCoLD?style=flat)](https://github.com/BrandonHanx/PoCoLD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Controllable_Person_Image_Synthesis_with_Pose-Constrained_Latent_Diffusion_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| PATMAT: Person Aware Tuning of Mask-Aware Transformer for Face Inpainting | [![GitHub](https://img.shields.io/github/stars/humansensinglab/PATMAT?style=flat)](https://github.com/humansensinglab/PATMAT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Motamed_PATMAT_Person_Aware_Tuning_of_Mask-Aware_Transformer_for_Face_Inpainting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06107-b31b1b.svg)](https://arxiv.org/abs/2304.06107) | :heavy_minus_sign: |
| Virtual Try-On with Pose-Garment Keypoints Guided Inpainting | [![GitHub](https://img.shields.io/github/stars/lizhi-ntu/KGI?style=flat)](https://github.com/lizhi-ntu/KGI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Virtual_Try-On_with_Pose-Garment_Keypoints_Guided_Inpainting_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Online Clustered Codebook | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://chuanxiaz.com/cvq/) <br /> [![GitHub](https://img.shields.io/github/stars/lyndonzheng/CVQ-VAE?style=flat)](https://github.com/lyndonzheng/CVQ-VAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Online_Clustered_Codebook_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15139-b31b1b.svg)](https://arxiv.org/abs/2307.15139) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=g098J5Obxvs) |
| InfiniCity: Infinite-Scale City Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hubert0527.github.io/infinicity/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_InfiniCity_Infinite-Scale_City_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.09637-b31b1b.svg)](https://arxiv.org/abs/2301.09637) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=eaoTVZSLPH4) |
| Make-it-3D: High-fidelity 3D Creation from a Single Image with Diffusion Prior | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://make-it-3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/junshutang/Make-It-3D?style=flat)](https://github.com/junshutang/Make-It-3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_Make-It-3D_High-fidelity_3D_Creation_from_A_Single_Image_with_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14184-b31b1b.svg)](https://arxiv.org/abs/2303.14184) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2M8JJFeDBFk) |
| SAMPLING: Scene-Adaptive Hierarchical Multiplane Images Representation for Novel View Synthesis from a Single Image | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pkuvdig.github.io/SAMPLING/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_SAMPLING_Scene-adaptive_Hierarchical_Multiplane_Images_Representation_for_Novel_View_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06323-b31b1b.svg)](https://arxiv.org/abs/2309.06323) | :heavy_minus_sign: |
| StyleLipSync: Style-based Personalized Lip-Sync Video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stylelipsync.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/AMEERAZAM08/StyleLipSync?style=flat)](https://github.com/AMEERAZAM08/StyleLipSync) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ki_StyleLipSync_Style-based_Personalized_Lip-sync_Video_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00521-b31b1b.svg)](https://arxiv.org/abs/2305.00521) | :heavy_minus_sign: |
| StyleInV: A Temporal Style Modulated Inversion Network for Unconditional Video Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/styleinv/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/johannwyh/StyleInV?style=flat)](https://github.com/johannwyh/StyleInV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_StyleInV_A_Temporal_Style_Modulated_Inversion_Network_for_Unconditional_Video_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16909-b31b1b.svg)](https://arxiv.org/abs/2308.16909) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R_v_L-32_Vo) |
| 3D-Aware Generative Model for Improved Side-View Image Synthesis | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jo_3D-Aware_Generative_Model_for_Improved_Side-View_Image_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.10388-b31b1b.svg)](https://arxiv.org/abs/2309.10388) | :heavy_minus_sign: |
| Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer | [![GitHub](https://img.shields.io/github/stars/YSerin/ZeCon?style=flat)](https://github.com/YSerin/ZeCon) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Zero-Shot_Contrastive_Loss_for_Text-Guided_Diffusion_Image_Style_Transfer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08622-b31b1b.svg)](https://arxiv.org/abs/2303.08622) | :heavy_minus_sign: |
| FlipNeRF: Flipped Reflection Rays for Few-Shot Novel View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shawn615.github.io/flipnerf/) <br /> [![GitHub](https://img.shields.io/github/stars/shawn615/FlipNeRF?style=flat)](https://github.com/shawn615/FlipNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Seo_FlipNeRF_Flipped_Reflection_Rays_for_Few-shot_Novel_View_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.17723-b31b1b.svg)](https://arxiv.org/abs/2306.17723) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_XNsRxzaPjw) |
| Inverse Problem Regularization with Hierarchical Variational Autoencoders | [![GitHub](https://img.shields.io/github/stars/jprost76/PnP-HVAE?style=flat)](https://github.com/jprost76/PnP-HVAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Prost_Inverse_Problem_Regularization_with_Hierarchical_Variational_Autoencoders_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11217-b31b1b.svg)](https://arxiv.org/abs/2303.11217) | :heavy_minus_sign: |
| 3D-Aware Blending with Generative NeRFs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://blandocs.github.io/blendnerf) <br /> [![GitHub](https://img.shields.io/github/stars/naver-ai/BlendNeRF?style=flat)](https://github.com/naver-ai/BlendNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_3D-aware_Blending_with_Generative_NeRFs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.06608-b31b1b.svg)](https://arxiv.org/abs/2302.06608) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mwLPY-QIxkc) |
| NeMF: Inverse Volume Rendering with Neural Microflake Field | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://youjiazhang.github.io/NeMF/) <br /> [![GitHub](https://img.shields.io/github/stars/YoujiaZhang/NeMF?style=flat)](https://github.com/YoujiaZhang/NeMF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeMF_Inverse_Volume_Rendering_with_Neural_Microflake_Field_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00782-b31b1b.svg)](https://arxiv.org/abs/2304.00782) | :heavy_minus_sign: |
| Preserve your Own Correlation: A Noise Prior for Video Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.nvidia.com/labs/dir/pyoco/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Preserve_Your_Own_Correlation_A_Noise_Prior_for_Video_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.10474-b31b1b.svg)](https://arxiv.org/abs/2305.10474) | :heavy_minus_sign: |
| iVS-Net: Learning Human View Synthesis from Internet Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_iVS-Net_Learning_Human_View_Synthesis_from_Internet_Videos_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EGC: Image Generation and Classification via a Diffusion Energy-based Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://guoqiushan.github.io/egc.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/GuoQiushan/EGC?style=flat)](https://github.com/GuoQiushan/EGC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_EGC_Image_Generation_and_Classification_via_a_Diffusion_Energy-Based_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02012-b31b1b.svg)](https://arxiv.org/abs/2304.02012) | :heavy_minus_sign: |
| Automatic Animation of Hair Blowing in Still Portrait Photos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nevergiveu.github.io/AutomaticHairBlowing/) <br /> [![GitHub](https://img.shields.io/github/stars/Rysertio/automatic-hair-blowing?style=flat)](https://github.com/Rysertio/automatic-hair-blowing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiao_Automatic_Animation_of_Hair_Blowing_in_Still_Portrait_Photos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14207-b31b1b.svg)](https://arxiv.org/abs/2309.14207) | :heavy_minus_sign: |
| HoloFusion: Towards Photo-Realistic 3D Generative Modeling | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://holodiffusion.github.io/holofusion/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Karnewar_HoloFusion_Towards_Photo-realistic_3D_Generative_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14244-b31b1b.svg)](https://arxiv.org/abs/2308.14244) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wJ7PfTgcVgM) |
| Foreground Object Search by Distilling Composite Image Feature | [![GitHub](https://img.shields.io/github/stars/bcmi/Foreground-Object-Search-Dataset-FOSD?style=flat)](https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Foreground_Object_Search_by_Distilling_Composite_Image_Feature_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04990-b31b1b.svg)](https://arxiv.org/abs/2308.04990) | :heavy_minus_sign: |
| OrthoPlanes: A Novel Representation for Better 3D-Awareness of GANs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://orthoplanes.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/OrthoPlanes/op3d?style=flat)](https://github.com/OrthoPlanes/op3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_OrthoPlanes_A_Novel_Representation_for_Better_3D-Awareness_of_GANs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15830-b31b1b.svg)](https://arxiv.org/abs/2309.15830) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=o8ghAi975vo) |
| 3DHumanGAN: 3D-Aware Human Image Generation with 3D Pose Mapping | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dhumangan.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/3dhumangan/3DHumanGAN?style=flat)](https://github.com/3dhumangan/3DHumanGAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_3DHumanGAN_3D-Aware_Human_Image_Generation_with_3D_Pose_Mapping_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.07378-b31b1b.svg)](https://arxiv.org/abs/2212.07378) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-bUNfhNYj24) |
| MODA: Mapping-Once Audio-Driven Portrait Animation with Dual Attentions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liuyunfei.net/projects/iccv23-moda/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_MODA_Mapping-Once_Audio-driven_Portrait_Animation_with_Dual_Attentions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.10008-b31b1b.svg)](https://arxiv.org/abs/2307.10008) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=VO6m49VC3zw) |
| Minimum Latency Deep Online Video Stabilization | [![GitHub](https://img.shields.io/github/stars/liuzhen03/NNDVS?style=flat)](https://github.com/liuzhen03/NNDVS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Minimum_Latency_Deep_Online_Video_Stabilization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02073-b31b1b.svg)](https://arxiv.org/abs/2212.02073) | :heavy_minus_sign: |
| StableVideo: Text-Driven Consistency-Aware Diffusion Video Editing | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rese1f.github.io/StableVideo/) <br /> [![GitHub](https://img.shields.io/github/stars/rese1f/StableVideo?style=flat)](https://github.com/rese1f/StableVideo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chai_StableVideo_Text-driven_Consistency-aware_Diffusion_Video_Editing_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09592-b31b1b.svg)](https://arxiv.org/abs/2308.09592) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=qKs09aX1AJM) |
| Localizing Object-Level Shape Variations with Text-to-Image Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://orpatashnik.github.io/local-prompt-mixing/) <br /> [![GitHub](https://img.shields.io/github/stars/orpatashnik/local-prompt-mixing?style=flat)](https://github.com/orpatashnik/local-prompt-mixing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Patashnik_Localizing_Object-Level_Shape_Variations_with_Text-to-Image_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11306-b31b1b.svg)](https://arxiv.org/abs/2303.11306) | :heavy_minus_sign: |
| Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://harlanhong.github.io/publications/mcnet.html) <br /> [![GitHub](https://img.shields.io/github/stars/harlanhong/ICCV2023-MCNET?style=flat)](https://github.com/harlanhong/ICCV2023-MCNET) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_Implicit_Identity_Representation_Conditioned_Memory_Compensation_Network_for_Talking_Head_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09906-b31b1b.svg)](https://arxiv.org/abs/2307.09906) | :heavy_minus_sign: |
| ESSAformer: Efficient Transformer for Hyperspectral Image Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Rexzhan/ESSAformer?style=flat)](https://github.com/Rexzhan/ESSAformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ESSAformer_Efficient_Transformer_for_Hyperspectral_Image_Super-resolution_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14010-b31b1b.svg)](https://arxiv.org/abs/2307.14010) | :heavy_minus_sign: |
| GlueGen: Plug and Play Multi-Modal Encoders for X-to-Image Generation | [![GitHub](https://img.shields.io/github/stars/salesforce/GlueGen?style=flat)](https://github.com/salesforce/GlueGen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qin_GlueGen_Plug_and_Play_Multi-modal_Encoders_for_X-to-image_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10056-b31b1b.svg)](https://arxiv.org/abs/2303.10056) | :heavy_minus_sign: |
| UHDNeRF: Ultra-High-Definition Neural Radiance Fields | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_UHDNeRF_Ultra-High-Definition_Neural_Radiance_Fields_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| All-to-Key Attention for Arbitrary Style Transfer | [![GitHub](https://img.shields.io/github/stars/LearningHx/StyA2K?style=flat)](https://github.com/LearningHx/StyA2K) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_All-to-Key_Attention_for_Arbitrary_Style_Transfer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04105-b31b1b.svg)](https://arxiv.org/abs/2212.04105) | :heavy_minus_sign: |
| Diverse Inpainting and Editing with GAN Inversion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yildirim_Diverse_Inpainting_and_Editing_with_GAN_Inversion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15033-b31b1b.svg)](https://arxiv.org/abs/2307.15033) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=C9L_4jPNi7k) |
| MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sichun233746.github.io/MoTIF/) <br /> [![GitHub](https://img.shields.io/github/stars/sichun233746/MoTIF?style=flat)](https://github.com/sichun233746/MoTIF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_MoTIF_Learning_Motion_Trajectories_with_Local_Implicit_Neural_Functions_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07988-b31b1b.svg)](https://arxiv.org/abs/2307.07988) | :heavy_minus_sign: |
| RANA: Relightable Articulated Neural Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nvlabs.github.io/RANA/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Iqbal_RANA_Relightable_Articulated_Neural_Avatars_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03237-b31b1b.svg)](https://arxiv.org/abs/2212.03237) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=s-hIhIMjPqQ) |
| DiffCloth: Diffusion based Garment Synthesis and Manipulation via Structural Cross-Modal Semantic Alignment | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DiffCloth_Diffusion_Based_Garment_Synthesis_and_Manipulation_via_Structural_Cross-modal_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11206-b31b1b.svg)](https://arxiv.org/abs/2308.11206) | :heavy_minus_sign: |
| Masked Diffusion Transformer is a Strong Image Synthesizer | [![GitHub](https://img.shields.io/github/stars/sail-sg/MDT?style=flat)](https://github.com/sail-sg/MDT) <br /> [![Hugging Face](https://img.shields.io/badge/ðŸ¤—-Demo-FFD21F.svg)](https://huggingface.co/spaces/shgao/MDT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Masked_Diffusion_Transformer_is_a_Strong_Image_Synthesizer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14389-b31b1b.svg)](https://arxiv.org/abs/2303.14389) | :heavy_minus_sign: |
| FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model | [![GitHub](https://img.shields.io/github/stars/vvictoryuki/FreeDoM?style=flat)](https://github.com/vvictoryuki/FreeDoM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_FreeDoM_Training-Free_Energy-Guided_Conditional_Diffusion_Model_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09833-b31b1b.svg)](https://arxiv.org/abs/2303.09833) | :heavy_minus_sign: |
| CLNeRF: Continual Learning Meets NeRF | [![GitHub](https://img.shields.io/github/stars/IntelLabs/CLNeRF?style=flat)](https://github.com/IntelLabs/CLNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_CLNeRF_Continual_Learning_Meets_NeRF_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nLRt6OoDGq0) |
| Rethinking Fast Fourier Convolution in Image Inpainting | [![GitHub](https://img.shields.io/github/stars/1911cty/Unbiased-Fast-Fourier-Convolution?style=flat)](https://github.com/1911cty/Unbiased-Fast-Fourier-Convolution) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chu_Rethinking_Fast_Fourier_Convolution_in_Image_Inpainting_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Pix2Video: Video Editing using Image Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://duyguceylan.github.io/pix2video.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/duyguceylan/pix2video?style=flat)](https://github.com/duyguceylan/pix2video) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ceylan_Pix2Video_Video_Editing_using_Image_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12688-b31b1b.svg)](https://arxiv.org/abs/2303.12688) | :heavy_minus_sign: |
| Multi-View Spectral Polarization Propagation for Video Glass Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qiao_Multi-view_Spectral_Polarization_Propagation_for_Video_Glass_Segmentation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| WALDO: Future Video Synthesis using Object Layer Decomposition and Parametric Flow Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://16lemoing.github.io/waldo/) <br /> [![GitHub](https://img.shields.io/github/stars/16lemoing/waldo?style=flat)](https://github.com/16lemoing/waldo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Le_Moing_WALDO_Future_Video_Synthesis_Using_Object_Layer_Decomposition_and_Parametric_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14308-b31b1b.svg)](https://arxiv.org/abs/2211.14308) | :heavy_minus_sign: |
| Ray Conditioning: Trading Photo-Consistency for Photo-Realism in Multi-View Image Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ray-cond.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/echen01/ray-conditioning?style=flat)](https://github.com/echen01/ray-conditioning) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Ray_Conditioning_Trading_Photo-consistency_for_Photo-realism_in_Multi-view_Image_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.13681-b31b1b.svg)](https://arxiv.org/abs/2304.13681) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=S88qmycnOJA) |
| Text-Conditioned Sampling Framework for Text-to-Image Generation with Masked Generative Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Text-Conditioned_Sampling_Framework_for_Text-to-Image_Generation_with_Masked_Generative_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01515-b31b1b.svg)](https://arxiv.org/abs/2304.01515) | :heavy_minus_sign: |
| Efficient Video Prediction via Sparsely Conditioned Flow Matching | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://araachie.github.io/river/) <br /> [![GitHub](https://img.shields.io/github/stars/araachie/river?style=flat)](https://github.com/araachie/river) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Davtyan_Efficient_Video_Prediction_via_Sparsely_Conditioned_Flow_Matching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14575-b31b1b.svg)](https://arxiv.org/abs/2211.14575) | :heavy_minus_sign: |

<!-- | GaFET: Learning Geometry-Aware Facial Expression Translation from in-the-Wild Images | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.03413-b31b1b.svg)](https://arxiv.org/abs/2308.03413) | :heavy_minus_sign: | -->