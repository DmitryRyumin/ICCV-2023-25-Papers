# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ü§ó-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/first-person-egocentric-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/deep-learning-architectures-and-techniques.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Representation Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-40-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-30-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-28-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-1-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis | [![GitHub](https://img.shields.io/github/stars/ivalab/WDiscOOD?style=flat)](https://github.com/ivalab/WDiscOOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_WDiscOOD_Out-of-Distribution_Detection_via_Whitened_Linear_Discriminant_Analysis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.07543-b31b1b.svg)](https://arxiv.org/abs/2303.07543) | :heavy_minus_sign: |
| Pairwise Similarity Learning is SimPLE | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://simple.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wen_Pairwise_Similarity_Learning_is_SimPLE_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.09449-b31b1b.svg)](https://arxiv.org/abs/2310.09449) | :heavy_minus_sign: |
| No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier | [![GitHub](https://img.shields.io/github/stars/ZexiLee/ICCV-2023-FedETF?style=flat)](https://github.com/ZexiLee/ICCV-2023-FedETF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_No_Fear_of_Classifier_Biases_Neural_Collapse_Inspired_Federated_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10058-b31b1b.svg)](https://arxiv.org/abs/2303.10058) | :heavy_minus_sign: |
| Generalizable Neural Fields as Partially Observed Neural Processes | [![GitHub](https://img.shields.io/github/stars/its-gucci/partially-observed-neural-processes?style=flat)](https://github.com/its-gucci/partially-observed-neural-processes) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gu_Generalizable_Neural_Fields_as_Partially_Observed_Neural_Processes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06660-b31b1b.svg)](https://arxiv.org/abs/2309.06660) | :heavy_minus_sign: |
| M2T: Masking Transformers Twice for Faster Decoding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mentzer_M2T_Masking_Transformers_Twice_for_Faster_Decoding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07313-b31b1b.svg)](https://arxiv.org/abs/2304.07313) | :heavy_minus_sign: |
| Keep it SimPool: Who Said Supervised Transformers Suffer from Attention Deficit? | [![GitHub](https://img.shields.io/github/stars/billpsomas/simpool?style=flat)](https://github.com/billpsomas/simpool) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Psomas_Keep_It_SimPool_Who_Said_Supervised_Transformers_Suffer_from_Attention_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06891-b31b1b.svg)](https://arxiv.org/abs/2309.06891) | :heavy_minus_sign: |
| Improving Pixel-based MIM by Reducing Wasted Modeling Capability | [![GitHub](https://img.shields.io/github/stars/open-mmlab/mmpretrain?style=flat)](https://github.com/open-mmlab/mmpretrain) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.00261-b31b1b.svg)](https://arxiv.org/abs/2308.00261) | :heavy_minus_sign: |
| Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kechunl.github.io/AdaCode/) <br /> [![GitHub](https://img.shields.io/github/stars/kechunl/AdaCode?style=flat)](https://github.com/kechunl/AdaCode) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Learning_Image-Adaptive_Codebooks_for_Class-Agnostic_Image_Restoration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.06513-b31b1b.svg)](https://arxiv.org/abs/2306.06513) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7jMYUjq-wwE) |
| Quality Diversity for Visual Pre-Training | [![GitHub](https://img.shields.io/github/stars/ruchikachavhan/quality-diversity-pretraining?style=flat)](https://github.com/ruchikachavhan/quality-diversity-pretraining) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chavhan_Quality_Diversity_for_Visual_Pre-Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Subclass-Balancing Contrastive Learning for Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/JackHck/SBCL?style=flat)](https://github.com/JackHck/SBCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hou_Subclass-balancing_Contrastive_Learning_for_Long-tailed_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15925-b31b1b.svg)](https://arxiv.org/abs/2306.15925) | :heavy_minus_sign: |
| Mastering Spatial Graph Prediction of Road Networks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sotiris_Mastering_Spatial_Graph_Prediction_of_Road_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.00828-b31b1b.svg)](https://arxiv.org/abs/2210.00828) | :heavy_minus_sign: |
| Poincar√© ResNet | [![GitHub](https://img.shields.io/github/stars/maxvanspengler/poincare-resnet?style=flat)](https://github.com/maxvanspengler/poincare-resnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/van_Spengler_Poincare_ResNet_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14027-b31b1b.svg)](https://arxiv.org/abs/2303.14027) | :heavy_minus_sign: |
| Exploring Model Transferability through the Lens of Potential Energy | [![GitHub](https://img.shields.io/github/stars/lixiaotong97/PED?style=flat)](https://github.com/lixiaotong97/PED) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Exploring_Model_Transferability_through_the_Lens_of_Potential_Energy_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15074-b31b1b.svg)](https://arxiv.org/abs/2308.15074) | :heavy_minus_sign: |
| Improving CLIP Fine-Tuning Performance | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Improving_CLIP_Fine-tuning_Performance_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Unsupervised Manifold Linearizing and Clustering | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Unsupervised_Manifold_Linearizing_and_Clustering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01805-b31b1b.svg)](https://arxiv.org/abs/2301.01805) | :heavy_minus_sign: |
| Generalized Sum Pooling for Metric Learning | [![GitHub](https://img.shields.io/github/stars/yetigurbuz/generalized-sum-pooling?style=flat)](https://github.com/yetigurbuz/generalized-sum-pooling) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gurbuz_Generalized_Sum_Pooling_for_Metric_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09228-b31b1b.svg)](https://arxiv.org/abs/2308.09228) | :heavy_minus_sign: |
| Partition Speeds Up Learning Implicit Neural Representations based on Exponential-Increase Hypothesis | [![GitHub](https://img.shields.io/github/stars/1999kevin/INR-Partition?style=flat)](https://github.com/1999kevin/INR-Partition) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Partition_Speeds_Up_Learning_Implicit_Neural_Representations_Based_on_Exponential-Increase_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.14184-b31b1b.svg)](https://arxiv.org/abs/2310.14184) | :heavy_minus_sign: |
| The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://facebookresearch.github.io/maws/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/maws?style=flat)](https://github.com/facebookresearch/maws) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_The_Effectiveness_of_MAE_Pre-Pretraining_for_Billion-Scale_Pretraining_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13496-b31b1b.svg)](https://arxiv.org/abs/2303.13496) | :heavy_minus_sign: |
| Token-Label Alignment for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/Euphoria16/TL-Align?style=flat)](https://github.com/Euphoria16/TL-Align) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiao_Token-Label_Alignment_for_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.06455-b31b1b.svg)](https://arxiv.org/abs/2210.06455) | :heavy_minus_sign: |
| Efficiently Robustify Pre-Trained Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jain_Efficiently_Robustify_Pre-Trained_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.07499-b31b1b.svg)](https://arxiv.org/abs/2309.07499) | :heavy_minus_sign: |
| OFVL-MS: Once for Visual Localization Across Multiple Indoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/mooncake199809/UFVL-Net/tree/main/configs/ofvl_ms) <br /> [![GitHub](https://img.shields.io/github/stars/mooncake199809/UFVL-Net?style=flat)](https://github.com/mooncake199809/UFVL-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_OFVL-MS_Once_for_Visual_Localization_across_Multiple_Indoor_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11928-b31b1b.svg)](https://arxiv.org/abs/2308.11928) | :heavy_minus_sign: |
| Feature Prediction Diffusion Model for Video Anomaly Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Feature_Prediction_Diffusion_Model_for_Video_Anomaly_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Joint Implicit Neural Representation for High-Fidelity and Compact Vector Fonts | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Joint_Implicit_Neural_Representation_for_High-fidelity_and_Compact_Vector_Fonts_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| How Far Pre-Trained Models are from Neural Collapse on the Target Dataset Informs their Transferability | [![GitHub](https://img.shields.io/github/stars/BUserName/NCTI?style=flat)](https://github.com/BUserName/NCTI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_How_Far_Pre-trained_Models_Are_from_Neural_Collapse_on_the_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions | [![GitHub](https://img.shields.io/github/stars/wangck20/OPERA?style=flat)](https://github.com/wangck20/OPERA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_OPERA_Omni-Supervised_Representation_Learning_with_Hierarchical_Supervisions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05557-b31b1b.svg)](https://arxiv.org/abs/2210.05557) | :heavy_minus_sign: |
| Perceptual Grouping in Contrastive Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/kahnchana/clippy?style=flat)](https://github.com/kahnchana/clippy) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ranasinghe_Perceptual_Grouping_in_Contrastive_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.09996-b31b1b.svg)](https://arxiv.org/abs/2210.09996) | :heavy_minus_sign: |
| Fully Attentional Networks with Self-Emerging Token Labeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Fully_Attentional_Networks_with_Self-emerging_Token_Labeling_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Instance and Category Supervision are Alternate Learners for Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_Instance_and_Category_Supervision_are_Alternate_Learners_for_Continual_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-Training | [![GitHub](https://img.shields.io/github/stars/HongYan1123/SkeletonMAE?style=flat)](https://github.com/HongYan1123/SkeletonMAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_SkeletonMAE_Graph-based_Masked_Autoencoder_for_Skeleton_Sequence_Pre-training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08476-b31b1b.svg)](https://arxiv.org/abs/2307.08476) | :heavy_minus_sign: |
| Motion-Guided Masking for Spatiotemporal Representation Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Motion-Guided_Masking_for_Spatiotemporal_Representation_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12962-b31b1b.svg)](https://arxiv.org/abs/2308.12962) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/motion-guided-masking-for-spatiotemporal-representation-learning) | :heavy_minus_sign: |
| Data Augmented Flatness-Aware Gradient Projection for Continual Learning | [![GitHub](https://img.shields.io/github/stars/EnnengYang/DFGP?style=flat)](https://github.com/EnnengYang/DFGP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Data_Augmented_Flatness-aware_Gradient_Projection_for_Continual_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Take-a-Photo: 3D-to-2D Generative Pre-Training of Point Cloud Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://tap.ivg-research.xyz/) <br /> [![GitHub](https://img.shields.io/github/stars/wangzy22/TAP?style=flat)](https://github.com/wangzy22/TAP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Take-A-Photo_3D-to-2D_Generative_Pre-training_of_Point_Cloud_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14971-b31b1b.svg)](https://arxiv.org/abs/2307.14971) | :heavy_minus_sign: |
| BiViT: Extremely Compressed Binary Vision Transformers | [![GitHub](https://img.shields.io/github/stars/ThisisBillhe/BiViT?style=flat)](https://github.com/ThisisBillhe/BiViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.07091-b31b1b.svg)](https://arxiv.org/abs/2211.07091) | :heavy_minus_sign: |
| Spatio-Temporal Crop Aggregation for Video Representation Learning | [![GitHub](https://img.shields.io/github/stars/Separius/SCALE?style=flat)](https://github.com/Separius/SCALE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sameni_Spatio-Temporal_Crop_Aggregation_for_Video_Representation_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.17042-b31b1b.svg)](https://arxiv.org/abs/2211.17042) | :heavy_minus_sign: |
| Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/HanjaeKim98/CoT?style=flat)](https://github.com/HanjaeKim98/CoT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Hierarchical_Visual_Primitive_Experts_for_Compositional_Zero-Shot_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04016-b31b1b.svg)](https://arxiv.org/abs/2308.04016) | :heavy_minus_sign: |
| Semantic Information in Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/sjiang95/semcl?style=flat)](https://github.com/sjiang95/semcl) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Quan_Semantic_Information_in_Contrastive_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Cross-Domain Product Representation Learning for Rich-Content E-Commerce | [![GitHub](https://img.shields.io/github/stars/adxcreative/COPE?style=flat)](https://github.com/adxcreative/COPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bai_Cross-Domain_Product_Representation_Learning_for_Rich-Content_E-Commerce_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05550-b31b1b.svg)](https://arxiv.org/abs/2308.05550) | :heavy_minus_sign: |
| Contrastive Continuity on Augmentation Stability Rehearsal for Continual Self-Supervised Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Contrastive_Continuity_on_Augmentation_Stability_Rehearsal_for_Continual_Self-Supervised_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness | [![GitHub](https://img.shields.io/github/stars/MKYucel/hybrid_augment?style=flat)](https://github.com/MKYucel/hybrid_augment) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yucel_HybridAugment_Unified_Frequency_Spectra_Perturbations_for_Model_Robustness_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11823-b31b1b.svg)](https://arxiv.org/abs/2307.11823) | :heavy_minus_sign: |
| Unleashing Text-to-Image Diffusion Models for Visual Perception | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vpd.ivg-research.xyz/) <br /> [![GitHub](https://img.shields.io/github/stars/wl-zhao/VPD?style=flat)](https://github.com/wl-zhao/VPD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Unleashing_Text-to-Image_Diffusion_Models_for_Visual_Perception_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.02153-b31b1b.svg)](https://arxiv.org/abs/2303.02153) | :heavy_minus_sign: |
