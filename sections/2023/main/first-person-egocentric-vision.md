# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/fairness-privacy-ethics-social-good-transparency-accountability-in-vision.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/representation-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## First Person (Egocentric) Vision

![Section Papers](https://img.shields.io/badge/Section%20Papers-7-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-6-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-3-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-1-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multimodal Distillation for Egocentric Action Recognition | [![GitHub](https://img.shields.io/github/stars/gorjanradevski/multimodal-distillation?style=flat)](https://github.com/gorjanradevski/multimodal-distillation) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Radevski_Multimodal_Distillation_for_Egocentric_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07483-b31b1b.svg)](https://arxiv.org/abs/2307.07483) | :heavy_minus_sign: |
| Self-Supervised Object Detection from Egocentric Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Akiva_Self-Supervised_Object_Detection_from_Egocentric_Videos_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Multi-Label Affordance Mapping from Egocentric Vision | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mur-Labadia_Multi-label_Affordance_Mapping_from_Egocentric_Vision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02120-b31b1b.svg)](https://arxiv.org/abs/2309.02120) | :heavy_minus_sign: |
| Ego-Only: Egocentric Action Detection without Exocentric Transferring | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01380-b31b1b.svg)](https://arxiv.org/abs/2301.01380) | :heavy_minus_sign: |
| COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/stanford.edu/copilot) <br /> [![GitHub](https://img.shields.io/github/stars/leobxpan/COPILOT?style=flat)](https://github.com/leobxpan/COPILOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_COPILOT_Human-Environment_Collision_Prediction_and_Localization_from_Egocentric_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01781-b31b1b.svg)](https://arxiv.org/abs/2210.01781) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lxRTPeac8Oo) |
| EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mvig-rhos.com/ego_pca) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02423-b31b1b.svg)](https://arxiv.org/abs/2309.02423) | :heavy_minus_sign: |
| EgoVLPv2: Egocentric Video-Language Pre-Training with Fusion in the Backbone | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shramanpramanick.github.io/EgoVLPv2/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/EgoVLPv2?style=flat)](https://github.com/facebookresearch/EgoVLPv2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pramanick_EgoVLPv2_Egocentric_Video-Language_Pre-training_with_Fusion_in_the_Backbone_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.05463-b31b1b.svg)](https://arxiv.org/abs/2307.05463) | :heavy_minus_sign: |
