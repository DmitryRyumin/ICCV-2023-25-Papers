# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/representation-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/recognition-detection.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Deep Learning Architectures and Techniques

![Section Papers](https://img.shields.io/badge/Section%20Papers-45-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-38-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-31-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-2-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Efficient Controllable Multi-Task Architectures | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aich_Efficient_Controllable_Multi-Task_Architectures_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11744-b31b1b.svg)](https://arxiv.org/abs/2308.11744) | :heavy_minus_sign: |
| ParCNetV2: Oversized Kernel with Enhanced Attention | [![GitHub](https://img.shields.io/github/stars/XuRuihan/ParCNetV2?style=flat)](https://github.com/XuRuihan/ParCNetV2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ParCNetV2_Oversized_Kernel_with_Enhanced_Attention_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.07157-b31b1b.svg)](https://arxiv.org/abs/2211.07157) | :heavy_minus_sign: |
| Unleashing the Power of Gradient Signal-to-Noise Ratio for Zero-Shot NAS | [![GitHub](https://img.shields.io/github/stars/Sunzh1996/Xi-GSNR?style=flat)](https://github.com/Sunzh1996/Xi-GSNR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sun_Unleashing_the_Power_of_Gradient_Signal-to-Noise_Ratio_for_Zero-Shot_NAS_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| MMST-ViT: Climate Change-Aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer | [![GitHub](https://img.shields.io/github/stars/fudong03/MMST-ViT?style=flat)](https://github.com/fudong03/MMST-ViT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_MMST-ViT_Climate_Change-aware_Crop_Yield_Prediction_via_Multi-Modal_Spatial-Temporal_Vision_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1xc_8KkOxVUVsHUiz9Vgv1nqqOa2O_t-2/view) | :heavy_minus_sign: |
| FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization | [![GitHub](https://img.shields.io/github/stars/apple/ml-fastvit?style=flat)](https://github.com/apple/ml-fastvit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Vasu_FastViT_A_Fast_Hybrid_Vision_Transformer_Using_Structural_Reparameterization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14189-b31b1b.svg)](https://arxiv.org/abs/2303.14189) | :heavy_minus_sign: |
| IIEU: Rethinking Neural Feature Activation from Decision-Making | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_IIEU_Rethinking_Neural_Feature_Activation_from_Decision-Making_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Scratching Visual Transformer's Back with Uniform Attention | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hyeon-Woo_Scratching_Visual_Transformers_Back_with_Uniform_Attention_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.08457-b31b1b.svg)](https://arxiv.org/abs/2210.08457) | :heavy_minus_sign: |
| SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/Moonlit/tree/main/SpaceEvo) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/Moonlit?style=flat)](https://github.com/microsoft/Moonlit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SpaceEvo_Hardware-Friendly_Search_Space_Design_for_Efficient_INT8_Inference_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08308-b31b1b.svg)](https://arxiv.org/abs/2303.08308) | :heavy_minus_sign: |
| ElasticViT: Conflict-Aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/Moonlit/tree/main/ElasticViT) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/Moonlit?style=flat)](https://github.com/microsoft/Moonlit) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_ElasticViT_Conflict-aware_Supernet_Training_for_Deploying_Fast_Vision_Transformer_on_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09730-b31b1b.svg)](https://arxiv.org/abs/2303.09730) | :heavy_minus_sign: |
| Gramian Attention Heads are Strong yet Efficient Vision Learners | [![GitHub](https://img.shields.io/github/stars/Lab-LVM/imagenet-models?style=flat)](https://github.com/Lab-LVM/imagenet-models) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ryu_Gramian_Attention_Heads_are_Strong_yet_Efficient_Vision_Learners_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.16483-b31b1b.svg)](https://arxiv.org/abs/2310.16483) | :heavy_minus_sign: |
| EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/EfficientTrain?style=flat)](https://github.com/LeapLabTHU/EfficientTrain) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_EfficientTrain_Exploring_Generalized_Curriculum_Learning_for_Training_Visual_Backbones_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.09703-b31b1b.svg)](https://arxiv.org/abs/2211.09703) | :heavy_minus_sign: |
| Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction | [![GitHub](https://img.shields.io/github/stars/wjh892521292/Ord2Seq?style=flat)](https://github.com/wjh892521292/Ord2Seq) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Ord2Seq_Regarding_Ordinal_Regression_as_Label_Sequence_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09004-b31b1b.svg)](https://arxiv.org/abs/2307.09004) | :heavy_minus_sign: |
| Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bai_Unified_Data-Free_Compression_Pruning_and_Quantization_without_Fine-Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07209-b31b1b.svg)](https://arxiv.org/abs/2308.07209) | :heavy_minus_sign: |
| LaPE: Layer-Adaptive Position Embedding for Vision Transformers with Independent Layer Normalization | [![GitHub](https://img.shields.io/github/stars/Ingrid725/LaPE?style=flat)](https://github.com/Ingrid725/LaPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_LaPE_Layer-adaptive_Position_Embedding_for_Vision_Transformers_with_Independent_Layer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05262-b31b1b.svg)](https://arxiv.org/abs/2212.05262) | :heavy_minus_sign: |
| Exemplar-Free Continual Transformer with Convolutions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cvir.github.io/projects/contracon) <br /> [![GitHub](https://img.shields.io/github/stars/CVIR/contracon?style=flat)](https://github.com/CVIR/contracon) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Roy_Exemplar-Free_Continual_Transformer_with_Convolutions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11357-b31b1b.svg)](https://arxiv.org/abs/2308.11357) | :heavy_minus_sign: |
| Building Vision Transformers with Hierarchy Aware Feature Aggregation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Building_Vision_Transformers_with_Hierarchy_Aware_Feature_Aggregation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ShiftNAS: Improving One-Shot NAS via Probability Shift | [![GitHub](https://img.shields.io/github/stars/bestfleer/ShiftNAS?style=flat)](https://github.com/bestfleer/ShiftNAS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_ShiftNAS_Improving_One-shot_NAS_via_Probability_Shift_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08300-b31b1b.svg)](https://arxiv.org/abs/2307.08300) | :heavy_minus_sign: |
| DarSwin: Distortion Aware Radial Swin Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/darswin/) <br /> [![GitHub](https://img.shields.io/github/stars/thalesgroup/darswin?style=flat)](https://github.com/thalesgroup/darswin) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Athwale_DarSwin_Distortion_Aware_Radial_Swin_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09691-b31b1b.svg)](https://arxiv.org/abs/2304.09691) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=jghHwwrvSyk) |
| ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_ROME_Robustifying_Memory-Efficient_NAS_via_Topology_Disentanglement_and_Gradient_Accumulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2011.11233-b31b1b.svg)](https://arxiv.org/abs/2011.11233) | :heavy_minus_sign: |
| FDViT: Improve the Hierarchical Architecture of Vision Transformer | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_FDViT_Improve_the_Hierarchical_Architecture_of_Vision_Transformer_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| FLatten Transformer: Vision Transformer using Focused Linear Attention | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/FLatten-Transformer?style=flat)](https://github.com/LeapLabTHU/FLatten-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.00442-b31b1b.svg)](https://arxiv.org/abs/2308.00442) | :heavy_minus_sign: |
| MixPath: A Unified Approach for One-Shot Neural Architecture Search | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chu_MixPath_A_Unified_Approach_for_One-shot_Neural_Architecture_Search_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2001.05887-b31b1b.svg)](https://arxiv.org/abs/2001.05887) | :heavy_minus_sign: |
| SSF: Accelerating Training of Spiking Neural Networks with Stabilized Spiking Flow | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_SSF_Accelerating_Training_of_Spiking_Neural_Networks_with_Stabilized_Spiking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Dynamic Perceiver for Efficient Visual Recognition | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Dynamic_Perceiver?style=flat)](https://github.com/LeapLabTHU/Dynamic_Perceiver) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_Dynamic_Perceiver_for_Efficient_Visual_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11248-b31b1b.svg)](https://arxiv.org/abs/2306.11248) | :heavy_minus_sign: |
| SG-Former: Self-Guided Transformer with Evolving Token Reallocation | [![GitHub](https://img.shields.io/github/stars/OliverRensu/SG-Former?style=flat)](https://github.com/OliverRensu/SG-Former) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_SG-Former_Self-guided_Transformer_with_Evolving_Token_Reallocation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12216-b31b1b.svg)](https://arxiv.org/abs/2308.12216) | :heavy_minus_sign: |
| Scale-Aware Modulation Meet Transformer | [![GitHub](https://img.shields.io/github/stars/AFeng-x/SMT?style=flat)](https://github.com/AFeng-x/SMT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08579-b31b1b.svg)](https://arxiv.org/abs/2307.08579) | :heavy_minus_sign: |
| Learning to Upsample by Learning to Sample | [![GitHub](https://img.shields.io/github/stars/tiny-smart/dysample?style=flat)](https://github.com/tiny-smart/dysample) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Learning_to_Upsample_by_Learning_to_Sample_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15085-b31b1b.svg)](https://arxiv.org/abs/2308.15085) | :heavy_minus_sign: |
| GET: Group Event Transformer for Event-based Vision | [![GitHub](https://img.shields.io/github/stars/Peterande/GET-Group-Event-Transformer?style=flat)](https://github.com/Peterande/GET-Group-Event-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Peng_GET_Group_Event_Transformer_for_Event-Based_Vision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.02642-b31b1b.svg)](https://arxiv.org/abs/2310.02642) | :heavy_minus_sign: |
| Adaptive Frequency Filters as Efficient Global Token Mixers | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg?style=flat)](https://github.com/microsoft/TokenMixers/tree/main/Adaptive%20Frequency%20Filters) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/TokenMixers?style=flat)](https://github.com/microsoft/TokenMixers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Adaptive_Frequency_Filters_As_Efficient_Global_Token_Mixers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14008-b31b1b.svg)](https://arxiv.org/abs/2307.14008) | :heavy_minus_sign: |
| Fcaformer: Forward Cross Attention in Hybrid Vision Transformer | [![GitHub](https://img.shields.io/github/stars/hkzhang-git/FcaFormer?style=flat)](https://github.com/hkzhang-git/FcaFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Fcaformer_Forward_Cross_Attention_in_Hybrid_Vision_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.07198-b31b1b.svg)](https://arxiv.org/abs/2211.07198) | :heavy_minus_sign: |
| Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yaoleiqi.github.io/pub_homepage/2023_ICCV/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/YaoleiQi/DSCNet?style=flat)](https://github.com/YaoleiQi/DSCNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qi_Dynamic_Snake_Convolution_Based_on_Topological_Geometric_Constraints_for_Tubular_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08388-b31b1b.svg)](https://arxiv.org/abs/2307.08388) | :heavy_minus_sign: |
| Sentence Attention Blocks for Answer Grounding | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khoshsirat_Sentence_Attention_Blocks_for_Answer_Grounding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11593-b31b1b.svg)](https://arxiv.org/abs/2309.11593) | :heavy_minus_sign: |
| MST-Compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Vo_MST-compression_Compressing_and_Accelerating_Binary_Neural_Networks_with_Minimum_Spanning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13735-b31b1b.svg)](https://arxiv.org/abs/2308.13735) | :heavy_minus_sign: |
| EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yun_EGformer_Equirectangular_Geometry-biased_Transformer_for_360_Depth_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.07803-b31b1b.svg)](https://arxiv.org/abs/2304.07803) | :heavy_minus_sign: |
| SPANet: Frequency-Balancing Token Mixer using Spectral Pooling Aggregation Modulation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://doranlyong.github.io/projects/spanet/) <br /> [![GitHub](https://img.shields.io/github/stars/DoranLyong/SPANet-official?style=flat)](https://github.com/DoranLyong/SPANet-official) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yun_SPANet_Frequency-balancing_Token_Mixer_using_Spectral_Pooling_Aggregation_Modulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11568-b31b1b.svg)](https://arxiv.org/abs/2308.11568) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wEVuA9-jv00) |
| ModelGiF: Gradient Fields for Model Functional Distance | [![GitHub](https://img.shields.io/github/stars/zju-vipa/modelgif?style=flat)](https://github.com/zju-vipa/modelgif) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_ModelGiF_Gradient_Fields_for_Model_Functional_Distance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11013-b31b1b.svg)](https://arxiv.org/abs/2309.11013) | :heavy_minus_sign: |
| ClusT3: Information Invariant Test-Time Training | [![GitHub](https://img.shields.io/github/stars/dosowiechi/ClusT3?style=flat)](https://github.com/dosowiechi/ClusT3) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hakim_ClusT3_Information_Invariant_Test-Time_Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.12345-b31b1b.svg)](https://arxiv.org/abs/2310.12345) | :heavy_minus_sign: |
| Cumulative Spatial Knowledge Distillation for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/Zzzzz1/CSKD?style=flat)](https://github.com/Zzzzz1/CSKD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_Cumulative_Spatial_Knowledge_Distillation_for_Vision_Transformers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08500-b31b1b.svg)](https://arxiv.org/abs/2307.08500) | :heavy_minus_sign: |
| Luminance-Aware Color Transform for Multiple Exposure Correction | [![GitHub](https://img.shields.io/github/stars/whdgusdl48/LACT?style=flat)](https://github.com/whdgusdl48/LACT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Baek_Luminance-aware_Color_Transform_for_Multiple_Exposure_Correction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks | [![GitHub](https://img.shields.io/github/stars/qymeng94/SLTT?style=flat)](https://github.com/qymeng94/SLTT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Meng_Towards_Memory-_and_Time-Efficient_Backpropagation_for_Training_Spiking_Neural_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.14311-b31b1b.svg)](https://arxiv.org/abs/2302.14311) | :heavy_minus_sign: |
| Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Michalkiewicz_Domain_Generalization_Guided_by_Gradient_Signal_to_Noise_Ratio_of_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2310.07361-b31b1b.svg)](https://arxiv.org/abs/2310.07361) | :heavy_minus_sign: |
| DOT: A Distillation-Oriented Trainer | [![GitHub](https://img.shields.io/github/stars/megvii-research/mdistiller?style=flat)](https://github.com/megvii-research/mdistiller) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DOT_A_Distillation-Oriented_Trainer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08436-b31b1b.svg)](https://arxiv.org/abs/2307.08436) | :heavy_minus_sign: |
| Extensible and Efficient Proxy for Neural Architecture Search | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Extensible_and_Efficient_Proxy_for_Neural_Architecture_Search_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learning to Transform for Generalizable Instance-Wise Invariance | [![GitHub](https://img.shields.io/github/stars/sutkarsh/flow_inv?style=flat)](https://github.com/sutkarsh/flow_inv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16672-b31b1b.svg)](https://arxiv.org/abs/2309.16672) | :heavy_minus_sign: |
| Convolutional Networks with Oriented 1D Kernels | [![GitHub](https://img.shields.io/github/stars/princeton-vl/Oriented1D?style=flat)](https://github.com/princeton-vl/Oriented1D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kirchmeyer_Convolutional_Networks_with_Oriented_1D_Kernels_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15812-b31b1b.svg)](https://arxiv.org/abs/2309.15812) | :heavy_minus_sign: |
