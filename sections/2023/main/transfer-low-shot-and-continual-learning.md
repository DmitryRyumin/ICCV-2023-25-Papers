# ICCV-2023-Papers

<table>
    <tr>
        <td><strong>Application</strong></td>
        <td>
            <a href="https://huggingface.co/spaces/DmitryRyumin/NewEraAI-Papers" style="float:left;">
                <img src="https://img.shields.io/badge/ðŸ¤—-NewEraAI--Papers-FFD21F.svg" alt="App" />
            </a>
        </td>
    </tr>
</table>

<div align="center">
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/human-poseshape-estimation.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/left.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/home.svg" width="40" alt="" />
    </a>
    <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/2023/main/self--semi--and-unsupervised-learning.md">
        <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/right.svg" width="40" alt="" />
    </a>
</div>

## Transfer, Low-Shot, and Continual Learning

![Section Papers](https://img.shields.io/badge/Section%20Papers-12-42BA16) ![Preprint Papers](https://img.shields.io/badge/Preprint%20Papers-7-b31b1b) ![Papers with Open Code](https://img.shields.io/badge/Papers%20with%20Open%20Code-6-1D7FBF) ![Papers with Video](https://img.shields.io/badge/Papers%20with%20Video-0-FF0000)

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Frequency Guidance Matters in Few-Shot Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Frequency_Guidance_Matters_in_Few-Shot_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Sensitivity-Aware Visual Parameter-Efficient Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/ziplab/SPT?style=flat)](https://github.com/ziplab/SPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_Sensitivity-Aware_Visual_Parameter-Efficient_Fine-Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08566-b31b1b.svg)](https://arxiv.org/abs/2303.08566) | :heavy_minus_sign: |
| On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yushu-li.github.io/owttt-site/) <br /> [![GitHub](https://img.shields.io/github/stars/Yushu-Li/OWTTT?style=flat)](https://github.com/Yushu-Li/OWTTT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_On_the_Robustness_of_Open-World_Test-Time_Training_Self-Training_with_Dynamic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09942-b31b1b.svg)](https://arxiv.org/abs/2308.09942) | :heavy_minus_sign: |
| Generating Instance-Level Prompts for Rehearsal-Free Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jung_Generating_Instance-level_Prompts_for_Rehearsal-free_Continual_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Boosting Novel Category Discovery over Domains with Soft Contrastive Learning and all in One Classifier | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zang_Boosting_Novel_Category_Discovery_Over_Domains_with_Soft_Contrastive_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.11262-b31b1b.svg)](https://arxiv.org/abs/2211.11262) | :heavy_minus_sign: |
| A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning | [![GitHub](https://img.shields.io/github/stars/kangzhiq/NNCSL?style=flat)](https://github.com/kangzhiq/NNCSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_A_Soft_Nearest-Neighbor_Framework_for_Continual_Semi-Supervised_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05102-b31b1b.svg)](https://arxiv.org/abs/2212.05102) | :heavy_minus_sign: |
| GraphEcho: Graph-Driven Unsupervised Domain Adaptation for Echocardiogram Video Segmentation | [![GitHub](https://img.shields.io/github/stars/xmed-lab/GraphEcho?style=flat)](https://github.com/xmed-lab/GraphEcho) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_GraphEcho_Graph-Driven_Unsupervised_Domain_Adaptation_for_Echocardiogram_Video_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11145-b31b1b.svg)](https://arxiv.org/abs/2309.11145) | :heavy_minus_sign: |
| ViperGPT: Visual Inference via Python Execution for Reasoning | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://viper.cs.columbia.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/cvlab-columbia/viper?style=flat)](https://github.com/cvlab-columbia/viper) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Suris_ViperGPT_Visual_Inference_via_Python_Execution_for_Reasoning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08128-b31b1b.svg)](https://arxiv.org/abs/2303.08128) | :heavy_minus_sign: |
| Improved Visual Fine-Tuning with Natural Language Supervision | [![GitHub](https://img.shields.io/github/stars/idstcv/TeS?style=flat)](https://github.com/idstcv/TeS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Improved_Visual_Fine-tuning_with_Natural_Language_Supervision_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01489-b31b1b.svg)](https://arxiv.org/abs/2304.01489) | :heavy_minus_sign: |
| Preparing the Future for Continual Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_Preparing_the_Future_for_Continual_Semantic_Segmentation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| MAP: Towards Balanced Generalization of IID and OOD through Model-Agnostic Adapters | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_MAP_Towards_Balanced_Generalization_of_IID_and_OOD_through_Model-Agnostic_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Space-Time Prompting for Video Class-Incremental Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pei_Space-time_Prompting_for_Video_Class-incremental_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
