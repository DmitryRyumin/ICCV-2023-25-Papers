# ICCV-2023-Papers

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![Conference](http://img.shields.io/badge/ICCV-2023-7395C5.svg)](https://iccv2023.thecvf.com)
![Version](https://img.shields.io/badge/version-v0.0.0-rc0)
![GitHub repo size](https://img.shields.io/github/repo-size/DmitryRyumin/ICCV-2023-Papers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/README.md)
![GitHub contributors](https://img.shields.io/github/contributors/dmitryryumin/ICCV-2023-Papers)
![GitHub commit activity (branch)](https://img.shields.io/github/commit-activity/t/dmitryryumin/ICCV-2023-Papers)
![GitHub closed issues](https://img.shields.io/github/issues-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub issues](https://img.shields.io/github/issues/DmitryRyumin/ICCV-2023-Papers)
![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub pull requests](https://img.shields.io/github/issues-pr/dmitryryumin/ICCV-2023-Papers)
![GitHub last commit](https://img.shields.io/github/last-commit/DmitryRyumin/ICCV-2023-Papers)
![GitHub watchers](https://img.shields.io/github/watchers/dmitryryumin/ICCV-2023-Papers)
![GitHub forks](https://img.shields.io/github/forks/dmitryryumin/ICCV-2023-Papers)
![GitHub Repo stars](https://img.shields.io/github/stars/dmitryryumin/ICCV-2023-Papers)
![Visitors](https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FDmitryRyumin%2FICCV-2023-Papers&label=Visitors&countColor=%23263759&style=flat)

> Completed: ![Progress](https://geps.dev/progress/51?successColor=006600)

---

ICCV 2023 Papers: Explore a comprehensive collection of cutting-edge research papers presented at [*ICCV 2023*](https://iccv2023.thecvf.com/), the premier computer vision conference. Keep up to date with the latest advances in computer vision and deep learning. Code implementations included. :star: the repository for the development of visual intelligence!

<p align="center">
    <a href="https://iccv2023.thecvf.com/" target="_blank">
        <img width="600" src="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/images/ICCV2023-banner.jpg" alt="ICCV 2023">
    </a>
<p>

---

[*The online version of the ICCV 2023 Conference Programme*](https://iccv2023.thecvf.com/main.conference.program-107.php), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> :exclamation: Conference table will be up to date all the time.

<table>
    <tr>
        <td><strong>Conference</strong></td>
        <td colspan="1" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td><a href="https://github.com/DmitryRyumin/CVPR-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Speech (SP)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td><a href="https://github.com/DmitryRyumin/ICASSP-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
        <td>INTERSPEECH</td>
        <td><a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers" target="_blank">2023</a></td>
    </tr>
</table>

---

## Contributors

<a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/graphs/contributors">
  <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/ICCV-2023-Papers" />
</a>

<br />
<br />

Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/ICCV-2023-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/ICCV-2023-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://iccv2023.thecvf.com/main.conference.program-107.php)

> :exclamation: Final paper links will be added post-conference.

<details open>
<summary>List of sections<a id="sections"></a></summary>

- [3D from Multi-View and Sensors](#3d-from-multi-view-and-sensors)
- [Adversarial Attack and Defense](#adversarial-attack-and-defense)
- [Vision and Robotics](#vision-and-robotics)
- [Vision and Graphics](#vision-and-graphics)
- [Segmentation, Grouping and Shape Analysis](#segmentation-grouping-and-shape-analysis)
- [Recognition: Categorization](#recognition-categorization)
- [Explainable AI for CV](#explainable-ai-for-cv)
- [Neural Generative Models](#neural-generative-models)
- [Vision and Language](#vision-and-language)
- [Vision, Graphics, and Robotics](#vision-graphics-and-robotics)
- [Privacy, Security, Fairness, and Explainability](#privacy-security-fairness-and-explainability)
- [Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision](#fairness-privacy-ethics-social-good-transparency-accountability-in-vision)
- [First Person (Egocentric) Vision](#first-person-egocentric-vision)
- [Representation Learning](#representation-learning)
- [Deep Learning Architectures](#deep-learning-architectures)
- [Recognition: Detection](#recognition-detection)
- [Image and Video Synthesis](#image-and-video-synthesis)
- [Vision and Audio](#vision-and-audio)
- [Recognition, Segmentation, and Shape Analysis](#recognition-segmentation-and-shape-analysis)
- [Generative AI](#generative-ai)
- [Humans, 3D Modeling, and Driving](#humans-3d-modeling-and-driving)
- [Low-Level Vision and Theory](#low-level-vision-and-theory)
- [Navigation and Autonomous Driving](#navigation-and-autonomous-driving)
- [3D from a Single Image and Shape-from-X](#3d-from-a-single-image-and-shape-from-x)
- [Motion Estimation, Matching and Tracking](#motion-estimation-matching-and-tracking)
- [Action and Event Understanding](#action-and-event-understanding)
- [Computational Imaging](#computational-imaging)
- [Embodied Vision: Active Agents; Simulation](#embodied-vision-active-agents-simulation)
- [Recognition: Retrieval](#recognition-retrieval)
- [Transfer, Low-Shot, Continual, Long-Tail Learning](#transfer-low-shot-continual-long-tail-learning)
- [Low-Level and Physics-based Vision](#low-level-and-physics-based-vision)
- [Computer Vision Theory](#computer-vision-theory)
- [Video Analysis and Understanding](#video-analysis-and-understanding)
- [Object Pose Estimation and Tracking](#object-pose-estimation-and-tracking)
- [3D Shape Modeling and Processing](#3d-shape-modeling-and-processing)
- [Human Pose/Shape Estimation](#human-poseshape-estimation)
- [Transfer, Low-Shot, and Continual Learning](#transfer-low-shot-and-continual-learning)
- [Self-, Semi-, and Unsupervised Learning](#self--semi--and-unsupervised-learning)
- [Self-, Semi-, Meta-, Unsupervised Learning](#self--semi--meta--unsupervised-learning)
- [Photogrammetry and Remote Sensing](#photogrammetry-and-remote-sensing)
- [Efficient and Scalable Vision](#efficient-and-scalable-vision)
- [Machine Learning (other than Deep Learning)](#machine-learning-other-than-deep-learning)
- [Document Analysis and Understanding](#document-analysis-and-understanding)
- [Biometrics](#biometrics)
- [Datasets and Evaluation](#datasets-and-evaluation)
- [Faces and Gestures](#faces-and-gestures)
- [Medical and Biological Vision; Cell Microscopy](#medical-and-biological-vision-cell-microscopy)
- [Scene Analysis and Understanding](#scene-analysis-and-understanding)
- [Multimodal Learning](#multimodal-learning)
- [Human-in-the-Loop Computer Vision](#human-in-the-loop-computer-vision)
- [Image and Video Forensics](#image-and-video-forensics)
- [Geometric Deep Learning](#geometric-deep-learning)
- [Vision Applications and Systems](#vision-applications-and-systems)
- [Machine Learning and Dataset](#machine-learning-and-dataset)

</details>

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from Multi-View and Sensors

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/tof_slam/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Multi-Modal_Neural_Radiance_Field_for_Monocular_Dense_SLAM_with_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14383-b31b1b.svg)](https://arxiv.org/abs/2308.14383) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7aJvVG7OLLQ) |
| ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cy94.github.io/scannetpp/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yeshwanth_ScanNet_A_High-Fidelity_Dataset_of_3D_Indoor_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11417-b31b1b.svg)](https://arxiv.org/abs/2308.11417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=E6P9e2r6M8I) |
| Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Translating_Images_to_Road_Network_A_Non-Autoregressive_Sequence-to-Sequence_Approach_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Doppelgangers: Learning to Disambiguate Images of Similar Structures | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://doppelgangers-3d.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuojinCai/Doppelgangers)](https://github.com/RuojinCai/Doppelgangers) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Doppelgangers_Learning_to_Disambiguate_Images_of_Similar_Structures_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02420-b31b1b.svg)](https://arxiv.org/abs/2309.02420) | :heavy_minus_sign: |
| EgoLoc: Revisiting 3D Object Localization from Egocentric Videos with Visual Queries | [![GitHub](https://img.shields.io/github/stars/Wayne-Mai/EgoLoc)](https://github.com/Wayne-Mai/EgoLoc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mai_EgoLoc_Revisiting_3D_Object_Localization_from_Egocentric_Videos_with_Visual_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.06969-b31b1b.svg)](https://arxiv.org/abs/2212.06969) | :heavy_minus_sign: |
| ClothPose: A Real-world Benchmark for Visual Analysis of Garment Pose via an Indirect Recording Solution | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_ClothPose_A_Real-world_Benchmark_for_Visual_Analysis_of_Garment_Pose_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EMR-MSF: Self-Supervised Recurrent Monocular Scene Flow Exploiting Ego-Motion Rigidity | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_EMR-MSF_Self-Supervised_Recurrent_Monocular_Scene_Flow_Exploiting_Ego-Motion_Rigidity_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ENVIDR: Implicit Differentiable Renderer with Neural Environment Lighting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nexuslrf.github.io/ENVIDR/) <br /> [![GitHub](https://img.shields.io/github/stars/nexuslrf/ENVIDR)](https://github.com/nexuslrf/ENVIDR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_ENVIDR_Implicit_Differentiable_Renderer_with_Neural_Environment_Lighting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13022-b31b1b.svg)](https://arxiv.org/abs/2303.13022) | [![Google Drive](https://img.shields.io/badge/Google%20Drive-4285F4?style=for-the-badge&logo=googledrive&logoColor=white)](https://drive.google.com/file/d/18kU-IWVxboCG8SCGgrBA5JHC0JIgPCS8/view?t=17s) |
| Learning a more Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection | [![GitHub](https://img.shields.io/github/stars/junshengzhou/LevelSetUDF)](https://github.com/junshengzhou/LevelSetUDF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Learning_a_More_Continuous_Zero_Level_Set_in_Unsigned_Distance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11441-b31b1b.svg)](https://arxiv.org/abs/2308.11441) | :heavy_minus_sign: |
| Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts | [![GitHub](https://img.shields.io/github/stars/VITA-Group/GNT-MOVE)](https://github.com/VITA-Group/GNT-MOVE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cong_Enhancing_NeRF_akin_to_Enhancing_LLMs_Generalizable_NeRF_Transformer_with_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11793-b31b1b.svg)](https://arxiv.org/abs/2308.11793) | :heavy_minus_sign: |
| MatrixCity: A Large-Scale City Dataset for City-Scale Neural Rendering and Beyond | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://city-super.github.io/matrixcity/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_MatrixCity_A_Large-scale_City_Dataset_for_City-scale_Neural_Rendering_and_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://city-super.github.io/matrixcity/img/matrixcity_camera_ready.pdf) | :heavy_minus_sign: |
| R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vis.xyz/pub/r3d3/) <br /> [![GitHub](https://img.shields.io/github/stars/SysCV/r3d3)](https://github.com/SysCV/r3d3) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Schmied_R3D3_Dense_3D_Reconstruction_of_Dynamic_Scenes_from_Multiple_Cameras_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14713-b31b1b.svg)](https://arxiv.org/abs/2308.14713) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lkU0lDq9HHw) |
| ClimateNeRF: Extreme Weather Synthesis in Neural Radiance Field | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://climatenerf.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_ClimateNeRF_Extreme_Weather_Synthesis_in_Neural_Radiance_Field_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.13226-b31b1b.svg)](https://arxiv.org/abs/2211.13226) | :heavy_minus_sign: |
| Rendering Humans from Object-Occluded Monocular Videos | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cs.stanford.edu/~xtiange/projects/occnerf/) <br /> [![GitHub](https://img.shields.io/github/stars/tiangexiang/OccNeRF)](https://github.com/tiangexiang/OccNeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_Rendering_Humans_from_Object-Occluded_Monocular_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04622-b31b1b.svg)](https://arxiv.org/abs/2308.04622) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=-LHyNdWGqTM) |
| AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://city-super.github.io/assetfield/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiangli_AssetField_Assets_Mining_and_Reconfiguration_in_Ground_Feature_Plane_Representation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13953-b31b1b.svg)](https://arxiv.org/abs/2303.13953) | :heavy_minus_sign: |
| PETRv2: A Unified Framework for 3D Perception from Multi-Camera Images | [![GitHub](https://img.shields.io/github/stars/megvii-research/PETR)](https://github.com/megvii-research/PETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_PETRv2_A_Unified_Framework_for_3D_Perception_from_Multi-Camera_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2206.01256-b31b1b.svg)](https://arxiv.org/abs/2206.01256) | :heavy_minus_sign: |
| MIMO-NeRF: Fast Neural Rendering with Multi-Input Multi-Output Neural Radiance Fields | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kaneko_MIMO-NeRF_Fast_Neural_Rendering_with_Multi-input_Multi-output_Neural_Radiance_Fields_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Adaptive Positional Encoding for Bundle-Adjusting Neural Radiance Fields | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Adaptive_Positional_Encoding_for_Bundle-Adjusting_Neural_Radiance_Fields_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-View Reconstruction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcai.mpi-inf.mpg.de/projects/NeuS2/) <br /> [![GitHub](https://img.shields.io/github/stars/19reborn/NeuS2)](https://github.com/19reborn/NeuS2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_NeuS2_Fast_Learning_of_Neural_Implicit_Surfaces_for_Multi-view_Reconstruction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05231-b31b1b.svg)](https://arxiv.org/abs/2212.05231) | :heavy_minus_sign: |
| Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition | [![GitHub](https://img.shields.io/github/stars/wqtwjt1996/SUM-L)](https://github.com/wqtwjt1996/SUM-L) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Learning_from_Semantic_Alignment_between_Unpaired_Multiviews_for_Egocentric_Video_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11489-b31b1b.svg)](https://arxiv.org/abs/2308.11489) | :heavy_minus_sign: |
| Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jing_Uncertainty_Guided_Adaptive_Warping_for_Robust_and_Efficient_Stereo_Matching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14071-b31b1b.svg)](https://arxiv.org/abs/2307.14071) | :heavy_minus_sign: |
| Compatibility of Fundamental Matrices for Complete Viewing Graphs | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bratelund_Compatibility_of_Fundamental_Matrices_for_Complete_Viewing_Graphs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10658-b31b1b.svg)](https://arxiv.org/abs/2303.10658) | :heavy_minus_sign: |
| ProtoTransfer: Cross-Modal Prototype Transfer for Point Cloud Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tang_ProtoTransfer_Cross-Modal_Prototype_Transfer_for_Point_Cloud_Segmentation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SA-BEV: Generating Semantic-Aware Bird's-Eye-View Feature for Multi-View 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/mengtan00/SA-BEV)](https://github.com/mengtan00/SA-BEV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_SA-BEV_Generating_Semantic-Aware_Birds-Eye-View_Feature_for_Multi-view_3D_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11477-b31b1b.svg)](https://arxiv.org/abs/2307.11477) | :heavy_minus_sign: |
| GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_GraphAlign_Enhancing_Accurate_Feature_Alignment_by_Graph_matching_for_Multi-Modal_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Tangent Sampson Error: Fast Approximate Two-View Reprojection Error for Central Camera Models | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Terekhov_Tangent_Sampson_Error_Fast_Approximate_Two-view_Reprojection_Error_for_Central_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Using a Waffle Iron for Automotive Point Cloud Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/valeoai/WaffleIron)](https://github.com/valeoai/WaffleIron) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Puy_Using_a_Waffle_Iron_for_Automotive_Point_Cloud_Semantic_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.10100-b31b1b.svg)](https://arxiv.org/abs/2301.10100) | :heavy_minus_sign: |
| Fast Globally Optimal Surface Normal Estimation from an Affine Correspondence | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hajder_Fast_Globally_Optimal_Surface_Normal_Estimation_from_an_Affine_Correspondence_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Preface: A Data-driven Volumetric Prior for Few-shot Ultra High-resolution Face Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://syntec-research.github.io/Preface/) <br /> [![GitHub](https://img.shields.io/github/stars/syntec-research/Preface)](https://github.com/syntec-research/Preface) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Buhler_Preface_A_Data-driven_Volumetric_Prior_for_Few-shot_Ultra_High-resolution_Face_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=oSprm3QTeLc) |
| Canonical Factors for Hybrid Neural Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://brentyi.github.io/tilted/) <br /> [![GitHub](https://img.shields.io/github/stars/brentyi/tilted)](https://github.com/brentyi/tilted) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yi_Canonical_Factors_for_Hybrid_Neural_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15461-b31b1b.svg)](https://arxiv.org/abs/2308.15461) | :heavy_minus_sign: |
| Center-based Decoupled Point-Cloud Registration for 6D Object Pose Estimation | [![GitHub](https://img.shields.io/github/stars/Jiang-HB/CenterReg)](https://github.com/Jiang-HB/CenterReg) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Center-Based_Decoupled_Point-cloud_Registration_for_6D_Object_Pose_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Deep Geometry-Aware Camera Self-Calibration from Video | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hagemann_Deep_Geometry-Aware_Camera_Self-Calibration_from_Video_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints | [![GitHub](https://img.shields.io/github/stars/nburgdorfer/V-FUSE)](https://github.com/nburgdorfer/V-FUSE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Burgdorfer_V-FUSE_Volumetric_Depth_Map_Fusion_with_Long-Range_Constraints_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08715-b31b1b.svg)](https://arxiv.org/abs/2308.08715) | :heavy_minus_sign: |
| Consistent Depth Prediction for Transparent Object Reconstruction from RGB-D Camera | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Consistent_Depth_Prediction_for_Transparent_Object_Reconstruction_from_RGB-D_Camera_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| FaceCLIPNeRF: Text-Driven 3D Face Manipulation using Deformable Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://faceclipnerf.github.io/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hwang_FaceCLIPNeRF_Text-driven_3D_Face_Manipulation_using_Deformable_Neural_Radiance_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11418-b31b1b.svg)](https://arxiv.org/abs/2307.11418) | :heavy_minus_sign: |
| HollowNeRF: Pruning Hashgrid-based NeRFs with Trainable Collision Mitigation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_HollowNeRF_Pruning_Hashgrid-Based_NeRFs_with_Trainable_Collision_Mitigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10122-b31b1b.svg)](https://arxiv.org/abs/2308.10122) | :heavy_minus_sign: |
| ICE-NeRF: Interactive Color Editing of NeRFs via Decomposition-Aware Weight Optimization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_ICE-NeRF_Interactive_Color_Editing_of_NeRFs_via_Decomposition-Aware_Weight_Optimization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| FULLER: Unified Multi-Modality Multi-Task 3D Perception via Multi-Level Gradient Calibration | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_FULLER_Unified_Multi-modality_Multi-task_3D_Perception_via_Multi-level_Gradient_Calibration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16617-b31b1b.svg)](https://arxiv.org/abs/2307.16617) | :heavy_minus_sign: |
| Neural Fields for Structured Lighting | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shandilya_Neural_Fields_for_Structured_Lighting_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| CO-Net: Learning Multiple Point Cloud Tasks at Once with a Cohesive Network | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xie_CO-Net_Learning_Multiple_Point_Cloud_Tasks_at_Once_with_A_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Pose-Free Neural Radiance Fields via Implicit Pose Regularization | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Pose-Free_Neural_Radiance_Fields_via_Implicit_Pose_Regularization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15049-b31b1b.svg)](https://arxiv.org/abs/2308.15049) | :heavy_minus_sign: |
| TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pansanity666.github.io/TransHuman/) <br /> [![GitHub](https://img.shields.io/github/stars/pansanity666/TransHuman)](https://github.com/pansanity666/TransHuman) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pan_TransHuman_A_Transformer-based_Human_Representation_for_Generalizable_Neural_Human_Rendering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12291-b31b1b.svg)](https://arxiv.org/abs/2307.12291) | :heavy_minus_sign: |
| S-VolSDF: Sparse Multi-View Stereo Regularization of Neural Implicit Surfaces | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hao-yu-wu.github.io/s-volsdf/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_S-VolSDF_Sparse_Multi-View_Stereo_Regularization_of_Neural_Implicit_Surfaces_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17712-b31b1b.svg)](https://arxiv.org/abs/2303.17712) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3_4PeVHWliY) |
| DPS-Net: Deep Polarimetric Stereo Depth Estimation | [![GitHub](https://img.shields.io/github/stars/Ethereal-Tian/DPS-Net)](https://github.com/Ethereal-Tian/DPS-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tian_DPS-Net_Deep_Polarimetric_Stereo_Depth_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| 3DPPE: 3D Point Positional Encoding for Transformer-based Multi-Camera 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/drilistbox/3DPPE)](https://github.com/drilistbox/3DPPE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shu_3DPPE_3D_Point_Positional_Encoding_for_Transformer-based_Multi-Camera_3D_Object_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14710-b31b1b.svg)](https://arxiv.org/abs/2211.14710) | :heavy_minus_sign: |
| Deformable Neural Radiance Fields using RGB and Event Cameras | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://qimaqi.github.io/DE-NeRF.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/qimaqi/DE-NeRF)](https://github.com/qimaqi/DE-NeRF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Deformable_Neural_Radiance_Fields_using_RGB_and_Event_Cameras_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08416-b31b1b.svg)](https://arxiv.org/abs/2309.08416) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K-hINgoSPKU) |
| NeILF++: Inter-Reflectable Light Fields for Geometry and Material Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yoyo000.github.io/NeILF_pp/) <br /> [![GitHub](https://img.shields.io/github/stars/apple/ml-neilfpp)](https://github.com/apple/ml-neilfpp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_NeILF_Inter-Reflectable_Light_Fields_for_Geometry_and_Material_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17147-b31b1b.svg)](https://arxiv.org/abs/2303.17147) | :heavy_minus_sign: |
| Hierarchical Prior Mining for Non-Local Multi-View Stereo | [![GitHub](https://img.shields.io/github/stars/CLinvx/HPM-MVS)](https://github.com/CLinvx/HPM-MVS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Hierarchical_Prior_Mining_for_Non-local_Multi-View_Stereo_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09758-b31b1b.svg)](https://arxiv.org/abs/2303.09758) | :heavy_minus_sign: |
| Exploring Object-Centric Temporal Modeling for Efficient Multi-View 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/exiawsh/StreamPETR)](https://github.com/exiawsh/StreamPETR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Exploring_Object-Centric_Temporal_Modeling_for_Efficient_Multi-View_3D_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11926-b31b1b.svg)](https://arxiv.org/abs/2303.11926) | :heavy_minus_sign: |
| Re-ReND: Real-Time Rendering of NeRFs Across Devices | [![GitHub](https://img.shields.io/github/stars/sararoma95/Re-ReND)](https://github.com/sararoma95/Re-ReND) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Rojas_Re-ReND_Real-Time_Rendering_of_NeRFs_across_Devices_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08717-b31b1b.svg)](https://arxiv.org/abs/2303.08717) | :heavy_minus_sign: |
| Learning Shape Primitives via Implicit Convexity Regularization | [![GitHub](https://img.shields.io/github/stars/seanywang0408/ICR)](https://github.com/seanywang0408/ICR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Learning_Shape_Primitives_via_Implicit_Convexity_Regularization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Geometry-Guided Feature Learning and Fusion for Indoor Scene Reconstruction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yin_Geometry-guided_Feature_Learning_and_Fusion_for_Indoor_Scene_Reconstruction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment | [![GitHub](https://img.shields.io/github/stars/zhangzw12319/lcps)](https://github.com/zhangzw12319/lcps) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_LiDAR-Camera_Panoptic_Segmentation_via_Geometry-Consistent_and_Semantic-Aware_Alignment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.01686-b31b1b.svg)](https://arxiv.org/abs/2308.01686) | :heavy_minus_sign: |
| PivotNet: Vectorized Pivot Learning for End-to-end HD Map Construction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_PivotNet_Vectorized_Pivot_Learning_for_End-to-end_HD_Map_Construction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16477-b31b1b.svg)](https://arxiv.org/abs/2308.16477) | :heavy_minus_sign: |
| Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sat2density.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/qianmingduowan/Sat2Density)](https://github.com/qianmingduowan/Sat2Density) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qian_Sat2Density_Faithful_Density_Learning_from_Satellite-Ground_Image_Pairs_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14672-b31b1b.svg)](https://arxiv.org/abs/2303.14672) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=mf00PRXUpTU) |
| Mask-Attention-Free Transformer for 3D Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/dvlab-research/Mask-Attention-Free-Transformer)](https://github.com/dvlab-research/Mask-Attention-Free-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_Mask-Attention-Free_Transformer_for_3D_Instance_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01692-b31b1b.svg)](https://arxiv.org/abs/2309.01692) | :heavy_minus_sign: |
| Scene-Aware Feature Matching | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Scene-Aware_Feature_Matching_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09949-b31b1b.svg)](https://arxiv.org/abs/2308.09949) | :heavy_minus_sign: |
| Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-Balanced Pseudo-Labeling | [![GitHub](https://img.shields.io/github/stars/zhuoxiao-chen/ReDB-DA-3Ddet)](https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Revisiting_Domain-Adaptive_3D_Object_Detection_by_Reliable_Diverse_and_Class-balanced_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07944-b31b1b.svg)](https://arxiv.org/abs/2307.07944) | :heavy_minus_sign: |
| GO-SLAM: Global Optimization for Consistent 3D Instant Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://youmi-zym.github.io/projects/GO-SLAM/) <br /> [![GitHub](https://img.shields.io/github/stars/youmi-zym/GO-SLAM)](https://github.com/youmi-zym/GO-SLAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_GO-SLAM_Global_Optimization_for_Consistent_3D_Instant_Reconstruction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02436-b31b1b.svg)](https://arxiv.org/abs/2309.02436) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=MbGn94Y4l8Y) |
| BANSAC: A dynamic BAyesian Network for adaptive SAmple Consensus | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pmiraldo.github.io/projects/bansac/bansac.html) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Piedade_BANSAC_A_Dynamic_BAyesian_Network_for_Adaptive_SAmple_Consensus_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08690-b31b1b.svg)](https://arxiv.org/abs/2309.08690) | :heavy_minus_sign: |
| Theoretical and Numerical Analysis of 3D Reconstruction using Point and Line Incidences | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Rydell_Theoretical_and_Numerical_Analysis_of_3D_Reconstruction_Using_Point_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.13593-b31b1b.svg)](https://arxiv.org/abs/2303.13593) | :heavy_minus_sign: |
| RealGraph: A Multiview Dataset for 4D Real-World Context Graph Generation | [![GitHub](https://img.shields.io/github/stars/THU-luvision/RealGraph)](https://github.com/THU-luvision/RealGraph) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lin_RealGraph_A_Multiview_Dataset_for_4D_Real-world_Context_Graph_Generation_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://rqhuang88.github.io/html/RealGraph.html) | :heavy_minus_sign: |
| CL-MVSNet: Unsupervised Multi-View Stereo with Dual-Level Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/KaiqiangXiong/CL-MVSNet)](https://github.com/KaiqiangXiong/CL-MVSNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiong_CL-MVSNet_Unsupervised_Multi-View_Stereo_with_Dual-Level_Contrastive_Learning_ICCV_2023_paper.pdf) <br /> [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://jianbojiao.com/pdfs/iccv23_clmvs.pdf) | :heavy_minus_sign: |
| Temporal Enhanced Training of Multi-View 3D Object Detector via Historical Object Prediction | [![GitHub](https://img.shields.io/github/stars/Sense-X/HoP)](https://github.com/Sense-X/HoP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zong_Temporal_Enhanced_Training_of_Multi-view_3D_Object_Detector_via_Historical_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00967-b31b1b.svg)](https://arxiv.org/abs/2304.00967) | :heavy_minus_sign: |
| Object as Query: Lifting any 2D Object Detector to 3D Detection | [![GitHub](https://img.shields.io/github/stars/tusen-ai/MV2D)](https://github.com/tusen-ai/MV2D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Object_as_Query_Lifting_Any_2D_Object_Detector_to_3D_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02364-b31b1b.svg)](https://arxiv.org/abs/2301.02364) | :heavy_minus_sign: |
| PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nie_PARTNER_Level_up_the_Polar_Representation_for_LiDAR_3D_Object_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03982-b31b1b.svg)](https://arxiv.org/abs/2308.03982) | :heavy_minus_sign: |
| Not Every Side is Equal: Localization Uncertainty Estimation for Semi-Supervised 3D Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Not_Every_Side_Is_Equal_Localization_Uncertainty_Estimation_for_Semi-Supervised_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/SparseBEV)](https://github.com/MCG-NJU/SparseBEV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_SparseBEV_High-Performance_Sparse_3D_Object_Detection_from_Multi-Camera_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09244-b31b1b.svg)](https://arxiv.org/abs/2308.09244) | :heavy_minus_sign: |
| LiveHand: Real-Time and Photorealistic Neural Hand Rendering | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vcai.mpi-inf.mpg.de/projects/LiveHand/) <br /> [![GitHub](https://img.shields.io/github/stars/amundra15/livehand)](https://github.com/amundra15/livehand) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.07672-b31b1b.svg)](https://arxiv.org/abs/2302.07672) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Adversarial Attack and Defense

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Robust Mixture-of-Expert Training for Convolutional Neural Networks | [![GitHub](https://img.shields.io/github/stars/OPTML-Group/Robust-MoE-CNN)](https://github.com/OPTML-Group/Robust-MoE-CNN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Robust_Mixture-of-Expert_Training_for_Convolutional_Neural_Networks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10110-b31b1b.svg)](https://arxiv.org/abs/2308.10110) | :heavy_minus_sign: |
| Set-Level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-Training Models | [![GitHub](https://img.shields.io/github/stars/Zoky-2020/SGA)](https://github.com/Zoky-2020/SGA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lu_Set-level_Guidance_Attack_Boosting_Adversarial_Transferability_of_Vision-Language_Pre-training_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.14061-b31b1b.svg)](https://arxiv.org/abs/2307.14061) | :heavy_minus_sign: |
| CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning | [![GitHub](https://img.shields.io/github/stars/nishadsinghi/CleanCLIP)](https://github.com/nishadsinghi/CleanCLIP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bansal_CleanCLIP_Mitigating_Data_Poisoning_Attacks_in_Multimodal_Contrastive_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.03323-b31b1b.svg)](https://arxiv.org/abs/2303.03323) | :heavy_minus_sign: |
| CGBA: Curvature-Aware Geometric Black-Box Attack | [![GitHub](https://img.shields.io/github/stars/Farhamdur/CGBA)](https://github.com/Farhamdur/CGBA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Reza_CGBA_Curvature-aware_Geometric_Black-box_Attack_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03163-b31b1b.svg)](https://arxiv.org/abs/2308.03163) | :heavy_minus_sign: |
| Robust Evaluation of Diffusion-based Adversarial Purification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Robust_Evaluation_of_Diffusion-Based_Adversarial_Purification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09051-b31b1b.svg)](https://arxiv.org/abs/2303.09051) | :heavy_minus_sign: |
| Advancing Example Exploitation can Alleviate Critical Challenges in Adversarial Training | [![GitHub](https://img.shields.io/github/stars/geyao1995/advancing-example-exploitation-in-adversarial-training)](https://github.com/geyao1995/advancing-example-exploitation-in-adversarial-training) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ge_Advancing_Example_Exploitation_Can_Alleviate_Critical_Challenges_in_Adversarial_Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| The Victim and the Beneficiary: Exploiting a Poisoned Model to Train a Clean Model on Poisoned Data | [![GitHub](https://img.shields.io/github/stars/Zixuan-Zhu/VaB)](https://github.com/Zixuan-Zhu/VaB) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_The_Victim_and_The_Beneficiary_Exploiting_a_Poisoned_Model_to_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models | [![GitHub](https://img.shields.io/github/stars/SRI-CSL/TIJO)](https://github.com/SRI-CSL/TIJO) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03906-b31b1b.svg)](https://arxiv.org/abs/2308.03906) | :heavy_minus_sign: |
| SAGA: Spectral Adversarial Geometric Attack on 3D Meshes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stoliktomer.github.io/SAGA/) <br /> [![GitHub](https://img.shields.io/github/stars/StolikTomer/SAGA)](https://github.com/StolikTomer/SAGA) | [![arXiv](https://img.shields.io/badge/arXiv-2211.13775-b31b1b.svg)](https://arxiv.org/abs/2211.13775) | :heavy_minus_sign: |
| Benchmarking and Analyzing Robust Point Cloud Recognition: Bag of Tricks for Defending Adversarial Examples | [![GitHub](https://img.shields.io/github/stars/qiufan319/benchmark_pc_attack)](https://github.com/qiufan319/benchmark_pc_attack) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16361-b31b1b.svg)](https://arxiv.org/abs/2307.16361) | :heavy_minus_sign: |
| ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://islab-ai.github.io/active-iccv2023/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07009-b31b1b.svg)](https://arxiv.org/abs/2308.07009) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=m6m90kX0O3w) |
| Frequency-Aware GAN for Adversarial Manipulation Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Breaking Temporal Consistency: Generating Video Universal Adversarial Perturbations using Image Models | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Tracing the Origin of Adversarial Attack for Forensic Investigation and Deterrence | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.01218-b31b1b.svg)](https://arxiv.org/abs/2301.01218) | :heavy_minus_sign: |
| Downstream-Agnostic Adversarial Examples | [![GitHub](https://img.shields.io/github/stars/CGCL-codes/AdvEncoder)](https://github.com/CGCL-codes/AdvEncoder) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12280-b31b1b.svg)](https://arxiv.org/abs/2307.12280) | :heavy_minus_sign: |
| Hiding Visual Information via Obfuscating Adversarial Perturbations | [![GitHub](https://img.shields.io/github/stars/suzhigangssz/AVIH)](https://github.com/suzhigangssz/AVIH) | [![arXiv](https://img.shields.io/badge/arXiv-2209.15304-b31b1b.svg)](https://arxiv.org/abs/2209.15304) | :heavy_minus_sign: |
| An Embarrassingly Simple Self-Supervised Trojan Attack | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Efficient Decision-based Black-Box Patch Attacks on Video Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11917-b31b1b.svg)](https://arxiv.org/abs/2303.11917) | :heavy_minus_sign: |
| Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.16454-b31b1b.svg)](https://arxiv.org/abs/2308.16454) | :heavy_minus_sign: |
| Towards Building more Robust Models with Frequency Bias | [![GitHub](https://img.shields.io/github/stars/retsuh-bqw/ICCV23-Towards-Building-More-Robust-Models-with-Frequency-Bias)](https://github.com/retsuh-bqw/ICCV23-Towards-Building-More-Robust-Models-with-Frequency-Bias) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09763-b31b1b.svg)](https://arxiv.org/abs/2307.09763) | :heavy_minus_sign: |
| Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/cav-sec/sysadv) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11894-b31b1b.svg)](https://arxiv.org/abs/2308.11894) | :heavy_minus_sign: |
| Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/microsoft/robustlearn)](https://github.com/microsoft/robustlearn) | [![arXiv](https://img.shields.io/badge/arXiv-2308.02533-b31b1b.svg)](https://arxiv.org/abs/2308.02533) | :heavy_minus_sign: |
| Enhancing Generalization of Universal Adversarial Perturbation through Gradient Aggregation | [![GitHub](https://img.shields.io/github/stars/liuxuannan/Stochastic-Gradient-Aggregation)](https://github.com/liuxuannan/Stochastic-Gradient-Aggregation) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06015-b31b1b.svg)](https://arxiv.org/abs/2308.06015) | :heavy_minus_sign: |
| Unified Adversarial Patch for Cross-Modal Attacks in the Physical World | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.07859-b31b1b.svg)](https://arxiv.org/abs/2307.07859) | :heavy_minus_sign: |
| RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World | [![GitHub](https://img.shields.io/github/stars/winterwindwang/RFLA)](https://github.com/winterwindwang/RFLA) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07653-b31b1b.svg)](https://arxiv.org/abs/2307.07653) | :heavy_minus_sign: |
| Enhancing Fine-Tuning based Backdoor Defense with Sharpness-Aware Minimization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.11823-b31b1b.svg)](https://arxiv.org/abs/2304.11823) | :heavy_minus_sign: |
| Conditional 360-Degree Image Synthesis for Immersive Indoor Scene Decoration | [![GitHub](https://img.shields.io/github/stars/kcshum/neural_360_decoration)](https://github.com/kcshum/neural_360_decoration) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09621-b31b1b.svg)](https://arxiv.org/abs/2307.09621) | :heavy_minus_sign: |
| An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability | [![GitHub](https://img.shields.io/github/stars/CHENBIN99/AdaEA)](https://github.com/CHENBIN99/AdaEA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.02897-b31b1b.svg)](https://arxiv.org/abs/2308.02897) | :heavy_minus_sign: |
| Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning | [![GitHub](https://img.shields.io/github/stars/ByungKwanLee/Double-Debiased-Adversary)](https://github.com/ByungKwanLee/Double-Debiased-Adversary) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07250-b31b1b.svg)](https://arxiv.org/abs/2307.07250) | :heavy_minus_sign: |
| LEA2: A Lightweight Ensemble Adversarial Attack via Non-Overlapping Vulnerable Frequency Regions | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Explaining Adversarial Robustness of Neural Networks from Clustering Effect Perspective | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| VertexSerum: Poisoning Graph Neural Networks for Link Inference | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01469-b31b1b.svg)](https://arxiv.org/abs/2308.01469) | :heavy_minus_sign: |
| How to Choose Your Best Allies for a Transferable Attack? | [![GitHub](https://img.shields.io/github/stars/t-maho/transferability_measure_fit)](https://github.com/t-maho/transferability_measure_fit) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02312-b31b1b.svg)](https://arxiv.org/abs/2304.02312) | :heavy_minus_sign: |
| Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/dyoony/SRST_AWR)](https://github.com/dyoony/SRST_AWR) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04061-b31b1b.svg)](https://arxiv.org/abs/2308.04061) | :heavy_minus_sign: |
| AdvDiffuser: Natural Adversarial Example Synthesis with Diffusion Models | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| FnF Attack Adversarial Attack against Multiple Object Trackers by Inducing False Negatives and False Positives | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/LukasStruppek/Rickrolling-the-Artist)](https://github.com/LukasStruppek/Rickrolling-the-Artist) | [![arXiv](https://img.shields.io/badge/arXiv-2211.02408-b31b1b.svg)](https://arxiv.org/abs/2211.02408) | :heavy_minus_sign: |
| Hard No-Box Adversarial Attack on Skeleton-based Human Action Recognition with Skeleton-Motion-Informed Gradient | [![GitHub](https://img.shields.io/github/stars/luyg45/HardNoBoxAttack)](https://github.com/luyg45/HardNoBoxAttack) | [![arXiv](https://img.shields.io/badge/arXiv-2308.05681-b31b1b.svg)](https://arxiv.org/abs/2308.05681) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hvniybZIiqA) |
| Structure Invariant Transformation for Better Adversarial Transferability | [![GitHub](https://img.shields.io/github/stars/xiaosen-wang/SIT)](https://github.com/xiaosen-wang/SIT) | :heavy_minus_sign: | :heavy_minus_sign: |
| Beating Backdoor Attack at its Own Game | [![GitHub](https://img.shields.io/github/stars/damianliumin/non-adversarial_backdoor)](https://github.com/damianliumin/non-adversarial_backdoor) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15539-b31b1b.svg)](https://arxiv.org/abs/2307.15539) | :heavy_minus_sign: |
| Transferable Adversarial Attack for Both Vision Transformers and Convolutional Networks via Momentum Integrated Gradients | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| REAP: A Large-Scale Realistic Adversarial Patch Benchmark | [![GitHub](https://img.shields.io/github/stars/wagner-group/reap-benchmark)](https://github.com/wagner-group/reap-benchmark) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05680-b31b1b.svg)](https://arxiv.org/abs/2212.05680) | :heavy_minus_sign: |
| Multi-Metrics Adaptively Identifies Backdoors in Federated Learning | [![GitHub](https://img.shields.io/github/stars/siquanhuang/Multi-metrics_against_backdoors_in_FL)](https://github.com/siquanhuang/Multi-metrics_against_backdoors_in_FL) | [![arXiv](https://img.shields.io/badge/arXiv-2303.06601-b31b1b.svg)](https://arxiv.org/abs/2303.06601) | :heavy_minus_sign: |
| Backpropagation Path Search on Adversarial Transferability | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.07625-b31b1b.svg)](https://arxiv.org/abs/2308.07625) | :heavy_minus_sign: |
| Fast Adaptation of Neural Networks using Test-Time Feedback | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| One-Bit Flip is All You Need: When Bit-Flip Attack Meets Model Training | [![GitHub](https://img.shields.io/github/stars/jianshuod/TBA)](https://github.com/jianshuod/TBA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07934-b31b1b.svg)](https://arxiv.org/abs/2308.07934) | :heavy_minus_sign: |
| PolicyCleanse: Backdoor Detection and Mitigation for Competitive Reinforcement Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2202.03609-b31b1b.svg)](https://arxiv.org/abs/2202.03609) | :heavy_minus_sign: |
| Towards Viewpoint-Invariant Visual Recognition via Adversarial Training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.10235-b31b1b.svg)](https://arxiv.org/abs/2307.10235) | :heavy_minus_sign: |
| Fast Adversarial Training with Smooth Convergence | [![GitHub](https://img.shields.io/github/stars/FAT-CS/ConvergeSmooth)](https://github.com/FAT-CS/ConvergeSmooth) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12857-b31b1b.svg)](https://arxiv.org/abs/2308.12857) | :heavy_minus_sign: |
| The Perils of Learning from Unlabeled Data: Backdoor Attacks on Semi-Supervised Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.00453-b31b1b.svg)](https://arxiv.org/abs/2211.00453) | :heavy_minus_sign: |
| Boosting Adversarial Transferability via Gradient Relevance Attack | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Towards Robust Model Watermark via Reducing Parametric Vulnerability | [![GitHub](https://img.shields.io/github/stars/GuanhaoGan/robust-model-watermarking)](https://github.com/GuanhaoGan/robust-model-watermarking) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04777-b31b1b.svg)](https://arxiv.org/abs/2309.04777) | :heavy_minus_sign: |
| TRM-UAP: Enhancing the Transferability of Data-Free Universal Adversarial Perturbation via Truncated Ratio Maximization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Robotics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Simoun: Synergizing Interactive Motion-Appearance Understanding for Vision-based Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Among Us: Adversarially Robust Collaborative Perception by Consensus | [![GitHub](https://img.shields.io/github/stars/coperception/ROBOSAC)](https://github.com/coperception/ROBOSAC) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09495-b31b1b.svg)](https://arxiv.org/abs/2303.09495) | :heavy_minus_sign: |
| Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://saltoricristiano.github.io/lidog/) <br /> [![GitHub](https://img.shields.io/github/stars/saltoricristiano/LiDOG)](https://github.com/saltoricristiano/LiDOG) | [![arXiv](https://img.shields.io/badge/arXiv-2304.11705-b31b1b.svg)](https://arxiv.org/abs/2304.11705) | :heavy_minus_sign: |
| Stabilizing Visual Reinforcement Learning via Asymmetric Interactive Cooperation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MAAL: Multimodality-Aware Autoencoder-based Affordance Learning for 3D Articulated Objects | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Range View Representation for LiDAR Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.05367-b31b1b.svg)](https://arxiv.org/abs/2303.05367) | :heavy_minus_sign: |
| PourIt!: Weakly-Supervised Liquid Perception from a Single Image for Visual Closed-Loop Robotic Pouring | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hetolin.github.io/PourIt/) <br /> [![GitHub](https://img.shields.io/github/stars/hetolin/PourIt)](https://github.com/hetolin/PourIt) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11299-b31b1b.svg)](https://arxiv.org/abs/2307.11299) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=R5SpiV0658Q) |
| CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.04869-b31b1b.svg)](https://arxiv.org/abs/2303.04869) | :heavy_minus_sign: |
| Environment Agnostic Representation for Visual Reinforcement Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Test-Time Personalizable Forecasting of 3D Human Poses | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HM-ViT: Hetero-Modal Vehicle-to-Vehicle Cooperative Perception with Vision Transformer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.10628-b31b1b.svg)](https://arxiv.org/abs/2304.10628) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Graphics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Efficient Neural Supersampling on a Novel Gaming Dataset | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01483-b31b1b.svg)](https://arxiv.org/abs/2308.01483) | :heavy_minus_sign: |
| Locally Stylized Neural Radiance Fields | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| NEMTO: Neural Environment Matting for Novel View and Relighting Synthesis of Transparent Objects | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11963-b31b1b.svg)](https://arxiv.org/abs/2303.11963) | :heavy_minus_sign: |
| DDColor: Towards Photo-Realistic and Semantic-Aware Image Colorization via Dual Decoders | [![GitHub](https://img.shields.io/github/stars/piddnad/DDColor)](https://github.com/piddnad/DDColor) <br /> [![ModelScope](https://img.shields.io/badge/ModelScope-DDColor-614BFF.svg)](https://www.modelscope.cn/models/damo/cv_ddcolor_image-colorization/summary) | [![arXiv](https://img.shields.io/badge/arXiv-2212.11613-b31b1b.svg)](https://arxiv.org/abs/2212.11613) | :heavy_minus_sign: |
| IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable Novel View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zju3dv.github.io/intrinsic_nerf/) <br /> [![GitHub](https://img.shields.io/github/stars/zju3dv/IntrinsicNeRF)](https://github.com/zju3dv/IntrinsicNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2210.00647-b31b1b.svg)](https://arxiv.org/abs/2210.00647) | :heavy_minus_sign: |
| PARIS: Part-Level Reconstruction and Motion Analysis for Articulated Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dlg-hcvc.github.io/paris/) <br /> [![GitHub](https://img.shields.io/github/stars/3dlg-hcvc/paris)](https://github.com/3dlg-hcvc/paris) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07391-b31b1b.svg)](https://arxiv.org/abs/2308.07391) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tDSrROPCgUc) |
| ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html) <br /> [![GitHub](https://img.shields.io/github/stars/mingyuan-zhang/ReMoDiffuse)](https://github.com/mingyuan-zhang/ReMoDiffuse) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01116-b31b1b.svg)](https://arxiv.org/abs/2304.01116) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wSddrIA_2p8) |
| DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ds-fusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/tmaham/DS-Fusion)](https://github.com/tmaham/DS-Fusion) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/tmaham/DS-Fusion-Express) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09604-b31b1b.svg)](https://arxiv.org/abs/2303.09604) | :heavy_minus_sign: |
| Dynamic Mesh-Aware Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mesh-aware-rf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/YilingQiao/DMRF)](https://github.com/YilingQiao/DMRF) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1uXg76v0CNVxgrQfBHPR5SbxIMXyPLFfQ/view) | :heavy_minus_sign: |
| Neural Reconstruction of Relightable Human Model from Monocular Video | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Neural Microfacet Fields for Inverse Rendering | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://half-potato.gitlab.io/posts/nmf/) <br /> [![GitHub](https://img.shields.io/github/stars/half-potato/nmf)](https://github.com/half-potato/nmf) | [![arXiv](https://img.shields.io/badge/arXiv-2303.17806-b31b1b.svg)](https://arxiv.org/abs/2303.17806) | :heavy_minus_sign: |
| A Theory of Topological Derivatives for Inverse Rendering of Geometry | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ishit.github.io/td/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09865-b31b1b.svg)](https://arxiv.org/abs/2308.09865) | :heavy_minus_sign: |
| Vox-E: Text-Guided Voxel Editing of 3D Objects | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tau-vailab.github.io/Vox-E/) <br /> [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/Vox-E)](https://github.com/TAU-VAILab/Vox-E) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12048-b31b1b.svg)](https://arxiv.org/abs/2303.12048) | :heavy_minus_sign: |
| StegaNeRF: Embedding Invisible Information within Neural Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xggnet.github.io/StegaNeRF/) <br /> [![GitHub](https://img.shields.io/github/stars/XGGNet/StegaNeRF)](https://github.com/XGGNet/StegaNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2212.01602-b31b1b.svg)](https://arxiv.org/abs/2212.01602) | :heavy_minus_sign: |
| GlobalMapper: Arbitrary-Shaped Urban Layout Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://arking1995.github.io/GlobalMapper/) <br /> [![GitHub](https://img.shields.io/github/stars/Arking1995/GlobalMapper)](https://github.com/Arking1995/GlobalMapper) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/He_GlobalMapper_Arbitrary-Shaped_Urban_Layout_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09693-b31b1b.svg)](https://arxiv.org/abs/2307.09693) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=T_Zp91FCoFw) |
| Urban Radiance Field Representation with Deformable Neural Mesh Primitives | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dnmp.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/DNMP/DNMP)](https://github.com/DNMP/DNMP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10776-b31b1b.svg)](https://arxiv.org/abs/2307.10776) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JABhlaVq4VA) |
| End2End Multi-View Feature Matching with Differentiable Pose Optimization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://barbararoessle.github.io/e2e_multi_view_matching/) | [![arXiv](https://img.shields.io/badge/arXiv-2205.01694-b31b1b.svg)](https://arxiv.org/abs/2205.01694) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5bFIIDOHRZY) |
| Tree-Structured Shading Decomposition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chen-geng.com/inv-shade-trees/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/gcgeng/inv-shade-trees)](https://github.com/gcgeng/inv-shade-trees) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://chen-geng.com/files/inv-shade-trees.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=L7zD9zM_zcg) |
| Lens Parameter Estimation for Realistic Depth of Field Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/inversedof/) | :heavy_minus_sign: | :heavy_minus_sign: |
| AttT2M: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Cross-Modal Latent Space Alignment for Image to Avatar Translation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Computationally Efficient Neural Image Compression with Shallow Decoders | [![GitHub](https://img.shields.io/github/stars/mandt-lab/shallow-ntc)](https://github.com/mandt-lab/shallow-ntc) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06244-b31b1b.svg)](https://arxiv.org/abs/2304.06244) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Segmentation, Grouping and Shape Analysis

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Enhancing Spatial and Semantic Supervision for Hybrid-based 3D Instance Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Neural Eigenfunctions for Unsupervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/thudzj/NeuralEigenfunctionSegmentor)](https://github.com/thudzj/NeuralEigenfunctionSegmentor) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02841-b31b1b.svg)](https://arxiv.org/abs/2304.02841) | :heavy_minus_sign: |
| Divide and Conquer: 3D Point Cloud Instance Segmentation with Point-Wise Binarization | [![GitHub](https://img.shields.io/github/stars/weiguangzhao/PBNet)](https://github.com/weiguangzhao/PBNet) | [![arXiv](https://img.shields.io/badge/arXiv-2207.11209-b31b1b.svg)](https://arxiv.org/abs/2207.11209) | :heavy_minus_sign: |
| Point2Mask: Point-Supervised Panoptic Segmentation via Optimal Transport | [![GitHub](https://img.shields.io/github/stars/LiWentomng/Point2Mask)](https://github.com/LiWentomng/Point2Mask) | [![arXiv](https://img.shields.io/badge/arXiv-2308.01779-b31b1b.svg)](https://arxiv.org/abs/2308.01779) | :heavy_minus_sign: |
| Handwritten and Printed Text Segmentation: A Signature Case Study | [![SignaTR6K](https://img.shields.io/badge/SignaTR6K-dataset-20BEFF.svg)](https://forms.office.com/r/2a5RDg7cAY) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07887-b31b1b.svg)](https://arxiv.org/abs/2307.07887) | :heavy_minus_sign: |
| Semantic-Aware Template Learning via Part Deformation Consistency | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11916-b31b1b.svg)](https://arxiv.org/abs/2308.11916) | :heavy_minus_sign: |
| LeaF: Learning Frames for 4D Point Cloud Sequence Understanding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MARS: Model-Agnostic Biased Object Removal without Additional Supervision for Weakly-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/shjo-april/MARS)](https://github.com/shjo-april/MARS) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09913-b31b1b.svg)](https://arxiv.org/abs/2304.09913) | :heavy_minus_sign: |
| USAGE: A Unified Seed Area Generation Paradigm for Weakly Supervised Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.07806-b31b1b.svg)](https://arxiv.org/abs/2303.07806) | :heavy_minus_sign: |
| Production-Level Video Segmentation from Few Annotated Frames | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://max810.github.io/xmem2-project-page/) <br /> [![GitHub](https://img.shields.io/github/stars/max810/XMem2)](https://github.com/max810/XMem2) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15958-b31b1b.svg)](https://arxiv.org/abs/2307.15958) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=3X3TUP4vKcc) |
| ΣIGMA: Scale-Invariant Global Sparse Shape Matching | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.08393-b31b1b.svg)](https://arxiv.org/abs/2308.08393) | :heavy_minus_sign: |
| Self-Calibrated Cross Attention Network for Few-Shot Segmentation | [![GitHub](https://img.shields.io/github/stars/Sam1224/SCCAN)](https://github.com/Sam1224/SCCAN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09294-b31b1b.svg)](https://arxiv.org/abs/2308.09294) | :heavy_minus_sign: |
| Multi-Granularity Interaction Simulation for Unsupervised Interactive Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.13399-b31b1b.svg)](https://arxiv.org/abs/2303.13399) | :heavy_minus_sign: |
| Texture Learning Domain Randomization for Domain Generalized Segmentation | [![GitHub](https://img.shields.io/github/stars/ssssshwan/TLDR)](https://github.com/ssssshwan/TLDR) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11546-b31b1b.svg)](https://arxiv.org/abs/2303.11546) | :heavy_minus_sign: |
| Unsupervised Video Object Segmentation with Online Adversarial Self-Tuning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Exploring Open-Vocabulary Semantic Segmentation without Human Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.00450-b31b1b.svg)](https://arxiv.org/abs/2306.00450) | :heavy_minus_sign: |
| RbA: Segmenting Unknown Regions Rejected by All | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/RbA/) <br /> [![GitHub](https://img.shields.io/github/stars/NazirNayal8/RbA)](https://github.com/NazirNayal8/RbA) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14293-b31b1b.svg)](https://arxiv.org/abs/2211.14293) | :heavy_minus_sign: |
| SEMPART: Self-Supervised Multi-Resolution Partitioning of Image Semantics | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Multi-Object Discovery by Low-Dimensional Object Motion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/multi-object-segmentation/) <br /> [![GitHub](https://img.shields.io/github/stars/sadrasafa/multi-object-segmentation)](https://github.com/sadrasafa/multi-object-segmentation) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08027-b31b1b.svg)](https://arxiv.org/abs/2307.08027) | :heavy_minus_sign: |
| MemorySeg: Online LiDAR Semantic Segmentation with a Latent Memory | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Treating Pseudo-Labels Generation as Image Matting for Weakly Supervised Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| BoxSnake: Polygonal Instance Segmentation with Box Supervision | [![GitHub](https://img.shields.io/github/stars/Yangr116/BoxSnake)](https://github.com/Yangr116/BoxSnake) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11630-b31b1b.svg)](https://arxiv.org/abs/2303.11630) | :heavy_minus_sign: |
| Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01045-b31b1b.svg)](https://arxiv.org/abs/2308.01045) | :heavy_minus_sign: |
| Instance Neural Radiance Field | [![GitHub](https://img.shields.io/github/stars/lyclyc52/Instance_NeRF)](https://github.com/lyclyc52/Instance_NeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2304.04395-b31b1b.svg)](https://arxiv.org/abs/2304.04395) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wW9Bme73coI) |
| Global Knowledge Calibration for Fast Open-Vocabulary Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.09181-b31b1b.svg)](https://arxiv.org/abs/2303.09181) | :heavy_minus_sign: |
| Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.12350-b31b1b.svg)](https://arxiv.org/abs/2308.12350) | :heavy_minus_sign: |
| Boosting Semantic Segmentation from an Explicit Class Embedding's Perspective | [![gitee](https://gitee-badge.vercel.app/svg/stars/mindspore/models)](https://gitee.com/mindspore/models) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12894-b31b1b.svg)](https://arxiv.org/abs/2308.12894) | :heavy_minus_sign: |
| The Making and Breaking of Camouflage | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://meowuu7.github.io/few-arti-obj-gen/) <br /> [![GitHub](https://img.shields.io/github/stars/Meowuu7/few-arti-gen)](https://github.com/Meowuu7/few-arti-gen) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10898-b31b1b.svg)](https://arxiv.org/abs/2308.10898) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=p8x3GN3VSPE) |
| HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.10460-b31b1b.svg)](https://arxiv.org/abs/2301.10460) | :heavy_minus_sign: |
| FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation | [![GitHub](https://img.shields.io/github/stars/TY-Shi/FreeCOS)](https://github.com/TY-Shi/FreeCOS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07245-b31b1b.svg)](https://arxiv.org/abs/2307.07245) | :heavy_minus_sign: |
| MasQCLIP for Open-Vocabulary Universal Image Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CTVIS: Consistent Training for Online Video Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/KainingYing/CTVIS)](https://github.com/KainingYing/CTVIS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12616-b31b1b.svg)](https://arxiv.org/abs/2307.12616) | :heavy_minus_sign: |
| A Simple Framework for Panoptic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Spectrum-Guided Multi-Granularity Referring Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/bo-miao/SgMg)](https://github.com/bo-miao/SgMg) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13537-b31b1b.svg)](https://arxiv.org/abs/2307.13537) | :heavy_minus_sign: |
| Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/WangChangqi98/CSS)](https://github.com/WangChangqi98/CSS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09755-b31b1b.svg)](https://arxiv.org/abs/2307.09755) | :heavy_minus_sign: |
| Adaptive Superpixel for Active Learning in Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.16817-b31b1b.svg)](https://arxiv.org/abs/2303.16817) | :heavy_minus_sign: |
| Multimodal Variational Auto-Encoder based Audio-Visual Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Isomer: Isomerous Transformer for Zero-Shot Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/DLUT-yyc/Isomer)](https://github.com/DLUT-yyc/Isomer) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06693-b31b1b.svg)](https://arxiv.org/abs/2308.06693) | :heavy_minus_sign: |
| 2D-3D Interlaced Transformer for Point Cloud Segmentation with Scene-Level Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jimmy15923.github.io/mit_web/) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://vllab.cs.nctu.edu.tw/images/paper/iccv-yang23.pdf) | :heavy_minus_sign: |
| Foreground-Background Separation through Concept Distillation from Generative Image Foundation Models | [![GitHub](https://img.shields.io/github/stars/MischaD/fobadiffusion)](https://github.com/MischaD/fobadiffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2212.14306-b31b1b.svg)](https://arxiv.org/abs/2212.14306) | :heavy_minus_sign: |
| SegPrompt: Boosting Open-World Segmentation via Category-Level Prompt Learning | [![GitHub](https://img.shields.io/github/stars/aim-uofa/SegPrompt)](https://github.com/aim-uofa/SegPrompt) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06531-b31b1b.svg)](https://arxiv.org/abs/2308.06531) | :heavy_minus_sign: |
| Monte Carlo Linear Clustering with Single-Point Supervision is Enough for Infrared Small Target Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yeren123455.github.io/SIRST-Single-Point-Supervision/) <br /> [![GitHub](https://img.shields.io/github/stars/YeRen123455/SIRST-Single-Point-Supervision)](https://github.com/YeRen123455/SIRST-Single-Point-Supervision) | [![arXiv](https://img.shields.io/badge/arXiv-2304.04442-b31b1b.svg)](https://arxiv.org/abs/2304.04442) | :heavy_minus_sign: |
| A Simple Framework for Open-Vocabulary Segmentation and Detection | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/OpenSeeD)](https://github.com/IDEA-Research/OpenSeeD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08131-b31b1b.svg)](https://arxiv.org/abs/2303.08131) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=z4gsQw2n7iM) |
| Source-Free Depth for Object Pop-Out | [![GitHub](https://img.shields.io/github/stars/Zongwei97/PopNet)](https://github.com/Zongwei97/PopNet) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05370-b31b1b.svg)](https://arxiv.org/abs/2212.05370) | :heavy_minus_sign: |
| DynaMITe: Dynamic Query Bootstrapping for Multi-Object Interactive Segmentation Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://amitrana001.github.io/DynaMITe/) <br /> [![GitHub](https://img.shields.io/github/stars/amitrana001/DynaMITe)](https://github.com/amitrana001/DynaMITe) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06668-b31b1b.svg)](https://arxiv.org/abs/2304.06668) | :heavy_minus_sign: |
| Atmospheric Transmission and Thermal Inertia Induced Blind Road Segmentation with a Large-Scale Dataset TBRSD | [![GitHub](https://img.shields.io/github/stars/chenjzBUAA/TBRSD)](https://github.com/chenjzBUAA/TBRSD) | :heavy_minus_sign: | :heavy_minus_sign: |
| Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Homography Guided Temporal Fusion for Road Line and Marking Segmentation | [![GitHub](https://img.shields.io/github/stars/ShanWang-Shan/HomoFusion)](https://github.com/ShanWang-Shan/HomoFusion) | :heavy_minus_sign: | :heavy_minus_sign: |
| Zero-Shot Semantic Segmentation with Decoupled One-Shot Network | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TCOVIS: Temporally Consistent Online Video Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/jun-long-li/TCOVIS)](https://github.com/jun-long-li/TCOVIS) | :heavy_minus_sign: | :heavy_minus_sign: |
| FPR: False Positive Rectification for Weakly Supervised Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/mt-cly/FPR)](https://github.com/mt-cly/FPR) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://www4.comp.polyu.edu.hk/~cslzhang/paper/ICCV23-FPR.pdf) | :heavy_minus_sign: |
| Stochastic Segmentation with Conditional Categorical Diffusion Models | [![GitHub](https://img.shields.io/github/stars/LarsDoorenbos/ccdm-stochastic-segmentation)](https://github.com/LarsDoorenbos/ccdm-stochastic-segmentation) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08888-b31b1b.svg)](https://arxiv.org/abs/2303.08888) | :heavy_minus_sign: |
| SegGPT: Segmenting Everything in Context | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/baaivision/Painter/tree/main/SegGPT) <br /> [![GitHub](https://img.shields.io/github/stars/baaivision/Painter)](https://github.com/baaivision/Painter) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/BAAI/SegGPT) | [![arXiv](https://img.shields.io/badge/arXiv-2304.03284-b31b1b.svg)](https://arxiv.org/abs/2304.03284) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zxwH0dUBKis) |
| Open-Vocabulary Panoptic Segmentation with Embedding Modulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11324-b31b1b.svg)](https://arxiv.org/abs/2303.11324) | :heavy_minus_sign: |
| Residual Pattern Learning for Pixel-Wise Out-of-Distribution Detection in Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/yyliu01/RPL)](https://github.com/yyliu01/RPL) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14512-b31b1b.svg)](https://arxiv.org/abs/2211.14512) | :heavy_minus_sign: |
| Zero-Guidance Segmentation using Zero Segment Labels | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zero-guide-seg.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13396-b31b1b.svg)](https://arxiv.org/abs/2303.13396) | :heavy_minus_sign: |
| Model Calibration in Dense Classification with Adaptive Label Perturbation | [![GitHub](https://img.shields.io/github/stars/Carlisle-Liu/ASLP)](https://github.com/Carlisle-Liu/ASLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13539-b31b1b.svg)](https://arxiv.org/abs/2307.13539) | :heavy_minus_sign: |
| Enhanced Soft Label for Semi-Supervised Semantic Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04829-b31b1b.svg)](https://arxiv.org/abs/2308.04829) | :heavy_minus_sign: |
| DiffuMask: Synthesizing Images with Pixel-Level Annotations for Semantic Segmentation using Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://weijiawu.github.io/DiffusionMask/) <br /> [![GitHub](https://img.shields.io/github/stars/weijiawu/DiffuMask)](https://github.com/weijiawu/DiffuMask) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11681-b31b1b.svg)](https://arxiv.org/abs/2303.11681) | :heavy_minus_sign: |
| Alignment Before Aggregation: Trajectory Memory Retrieval Network for Video Object Segmentation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Semi-Supervised Semantic Segmentation under Label Noise via Diverse Learning Groups | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets | [![GitHub](https://img.shields.io/github/stars/csimo005/SUMMIT)](https://github.com/csimo005/SUMMIT) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11880-b31b1b.svg)](https://arxiv.org/abs/2308.11880) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LDlLq9IdoAw) |
| Class-Incremental Continual Learning for Instance Segmentation with Image-Level Weak Supervision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Coarse-to-Fine Amodal Segmentation with Shape Prior | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jianxgao.github.io/C2F-Seg/) <br /> [![GitHub](https://img.shields.io/github/stars/JianxGao/C2F-Seg)](https://github.com/JianxGao/C2F-Seg) | [![arXiv](https://img.shields.io/badge/arXiv-2308.16825-b31b1b.svg)](https://arxiv.org/abs/2308.16825) | :heavy_minus_sign: |
| Rethinking Amodal Video Segmentation from Learning Supervised Signals with Object-Centric Representation | [![GitHub](https://img.shields.io/github/stars/kfan21/EoRaS)](https://github.com/kfan21/EoRaS) | :heavy_minus_sign: | :heavy_minus_sign: |
| DVIS: Decoupled Video Instance Segmentation Framework | [![GitHub](https://img.shields.io/github/stars/zhang-tao-whu/DVIS)](https://github.com/zhang-tao-whu/DVIS) | [![arXiv](https://img.shields.io/badge/arXiv-2306.03413-b31b1b.svg)](https://arxiv.org/abs/2306.03413) | :heavy_minus_sign: |
| 3D Segmentation of Humans in Point Clouds with Synthetic Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://human-3d.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.00786-b31b1b.svg)](https://arxiv.org/abs/2212.00786) | :heavy_minus_sign: |
| WaterMask: Instance Segmentation for Underwater Imagery | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Decoupled or End-to-End Trained Video Segmentation if Target Data is Scarce? | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Categorization

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Cross Contrasting Feature Perturbation for Domain Generalization | [![GitHub](https://img.shields.io/github/stars/hackmebroo/CCFP)](https://github.com/hackmebroo/CCFP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12502-b31b1b.svg)](https://arxiv.org/abs/2307.12502) | :heavy_minus_sign: |
| Flexible Visual Recognition by Evidential Modeling of Confusion and Ignorance | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.07403-b31b1b.svg)](https://arxiv.org/abs/2309.07403) | :heavy_minus_sign: |
| CDUL: CLIP-Driven Unsupervised Learning for Multi-Label Image Classification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.16634-b31b1b.svg)](https://arxiv.org/abs/2307.16634) | :heavy_minus_sign: |
| RankMixup: Ranking-based Mixup Training for Network Calibration | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.yonsei.ac.kr/projects/RankMixup/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11990-b31b1b.svg)](https://arxiv.org/abs/2308.11990) | :heavy_minus_sign: |
| Label-Noise Learning with Intrinsically Long-Tailed Data | [![GitHub](https://img.shields.io/github/stars/Wakings/TABASCO)](https://github.com/Wakings/TABASCO) | [![arXiv](https://img.shields.io/badge/arXiv-2208.09833-b31b1b.svg)](https://arxiv.org/abs/2208.09833) | :heavy_minus_sign: |
| Parallel Attention Interaction Network for Few-Shot Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/starrycos/PAINet)](https://github.com/starrycos/PAINet) | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Mobile Block for Efficient Attention-based Models | [![GitHub](https://img.shields.io/github/stars/zhangzjn/EMO)](https://github.com/zhangzjn/EMO) | [![arXiv](https://img.shields.io/badge/arXiv-2301.01146-b31b1b.svg)](https://arxiv.org/abs/2301.01146) | :heavy_minus_sign: |
| Read-Only Prompt Optimization for Vision-Language Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/mlvlab/RPO)](https://github.com/mlvlab/RPO) | [![arXiv](https://img.shields.io/badge/arXiv-2308.14960-b31b1b.svg)](https://arxiv.org/abs/2308.14960) | :heavy_minus_sign: |
| Understanding Self-Attention Mechanism via Dynamical System Perspective | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09939-b31b1b.svg)](https://arxiv.org/abs/2308.09939) | :heavy_minus_sign: |
| Learning in Imperfect Environment: Multi-Label Classification with Long-Tailed Distribution and Partial Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.10539-b31b1b.svg)](https://arxiv.org/abs/2304.10539) | :heavy_minus_sign: |
| What do Neural Networks Learn in Image Classification? A Frequency Shortcut Perspective | [![GitHub](https://img.shields.io/github/stars/nis-research/nn-frequency-shortcuts)](https://github.com/nis-research/nn-frequency-shortcuts) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09829-b31b1b.svg)](https://arxiv.org/abs/2307.09829) | :heavy_minus_sign: |
| Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity | [![GitHub](https://img.shields.io/github/stars/ltong1130ztr/HAFrame)](https://github.com/ltong1130ztr/HAFrame) | [![arXiv](https://img.shields.io/badge/arXiv-2303.05689-b31b1b.svg)](https://arxiv.org/abs/2303.05689) | :heavy_minus_sign: |
| Unified Out-of-Distribution Detection: A Model-Specific Perspective | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.06813-b31b1b.svg)](https://arxiv.org/abs/2304.06813) | :heavy_minus_sign: |
| A Unified Framework for Robustness on Diverse Sampling Errors | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Scene-Aware Label Graph Learning for Multi-Label Image Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Holistic Label Correction for Noisy Multi-Label Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Strip-MLP: Efficient Token Interaction for Vision MLP | [![GitHub](https://img.shields.io/github/stars/Med-Process/Strip_MLP)](https://github.com/Med-Process/Strip_MLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11458-b31b1b.svg)](https://arxiv.org/abs/2307.11458) | :heavy_minus_sign: |
| EQ-Net: Elastic Quantization Neural Networks | [![GitHub](https://img.shields.io/github/stars/xuke225/EQ-Net)](https://github.com/xuke225/EQ-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07650-b31b1b.svg)](https://arxiv.org/abs/2308.07650) | :heavy_minus_sign: |
| Data-Free Knowledge Distillation for Fine-Grained Vision Categorization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Shift from Texture-Bias to Shape-Bias: edge Deformation-based Augmentation for Robust Object Recognition | [![GitHub](https://img.shields.io/github/stars/C0notSilly/-ICCV-23-Edge-Deformation-based-Online-Augmentation)](https://github.com/C0notSilly/-ICCV-23-Edge-Deformation-based-Online-Augmentation) | :heavy_minus_sign: | :heavy_minus_sign: |
| Latent-OFER: Detect, Mask, and Reconstruct with Latent Vectors for Occluded Facial Expression Recognition | [![GitHub](https://img.shields.io/github/stars/leeisack/Latent-OFER)](https://github.com/leeisack/Latent-OFER) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11404-b31b1b.svg)](https://arxiv.org/abs/2307.11404) | :heavy_minus_sign: |
| DR-Tune: Improving Fine-Tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration | [![GitHub](https://img.shields.io/github/stars/weeknan/DR-Tune)](https://github.com/weeknan/DR-Tune) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12058-b31b1b.svg)](https://arxiv.org/abs/2308.12058) | :heavy_minus_sign: |
| Understanding the Feature Norm for Out-of-Distribution Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Multi-View Active Fine-Grained Visual Recognition | [![GitHub](https://img.shields.io/github/stars/PRIS-CV/AFGR)](https://github.com/PRIS-CV/AFGR) | [![arXiv](https://img.shields.io/badge/arXiv-2206.01153-b31b1b.svg)](https://arxiv.org/abs/2206.01153) | :heavy_minus_sign: |
| DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection using Pre-Trained Diffusion Models | [![GitHub](https://img.shields.io/github/stars/cure-lab/DiffGuard)](https://github.com/cure-lab/DiffGuard) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07687-b31b1b.svg)](https://arxiv.org/abs/2308.07687) | :heavy_minus_sign: |
| Task-Aware Adaptive Learning for Cross-Domain Few-Shot Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Improving Adversarial Robustness of Masked Autoencoders via Test-Time Frequency-Domain Prompting | [![GitHub](https://img.shields.io/github/stars/shikiw/RobustMAE)](https://github.com/shikiw/RobustMAE) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10315-b31b1b.svg)](https://arxiv.org/abs/2308.10315) | :heavy_minus_sign: |
| Saliency Regularization for Self-Training with Partial Annotations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Gabor Texture Features for Fine-Grained Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.05396-b31b1b.svg)](https://arxiv.org/abs/2308.05396) | :heavy_minus_sign: |
| UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/UniFormerV2)](https://github.com/OpenGVLab/UniFormerV2) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09552-b31b1b.svg)](https://arxiv.org/abs/2211.09552) | :heavy_minus_sign: |
| RankMatch: Fostering Confidence and Consistency in Learning with Noisy Labels | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MetaGCD: Learning to Continually Learn in Generalized Category Discovery | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11063-b31b1b.svg)](https://arxiv.org/abs/2308.11063) | :heavy_minus_sign: |
| FerKD: Surgical Label Adaptation for Efficient Distillation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Point-Query Quadtree for Crowd Counting, Localization, and more | [![GitHub](https://img.shields.io/github/stars/cxliu0/PET)](https://github.com/cxliu0/PET) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13814-b31b1b.svg)](https://arxiv.org/abs/2308.13814) | :heavy_minus_sign: |
| Nearest Neighbor Guidance for Out-of-Distribution Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Bayesian Optimization Meets Self-Distillation | [![GitHub](https://img.shields.io/github/stars/sooperset/boss)](https://github.com/sooperset/boss) | [![arXiv](https://img.shields.io/badge/arXiv-2304.12666-b31b1b.svg)](https://arxiv.org/abs/2304.12666) | :heavy_minus_sign: |
| When Prompt-based Incremental Learning does not Meet Strong Pretraining | [![GitHub](https://img.shields.io/github/stars/TOM-tym/APG)](https://github.com/TOM-tym/APG) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10445-b31b1b.svg)](https://arxiv.org/abs/2308.10445) | :heavy_minus_sign: |
| When to Learn what: Model-Adaptive Data Augmentation Curriculum | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.04747-b31b1b.svg)](https://arxiv.org/abs/2309.04747) | :heavy_minus_sign: |
| Parametric Information Maximization for Generalized Category Discovery | [![GitHub](https://img.shields.io/github/stars/ThalesGroup/pim-generalized-category-discovery)](https://github.com/ThalesGroup/pim-generalized-category-discovery) | [![arXiv](https://img.shields.io/badge/arXiv-2212.00334-b31b1b.svg)](https://arxiv.org/abs/2212.00334) | :heavy_minus_sign: |
| Boosting Few-Shot Action Recognition with Graph-Guided Hybrid Matching | [![GitHub](https://img.shields.io/github/stars/jiazheng-xing/GgHM)](https://github.com/jiazheng-xing/GgHM) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09346-b31b1b.svg)](https://arxiv.org/abs/2308.09346) | :heavy_minus_sign: |
| Domain Generalization via Rationale Invariance | [![GitHub](https://img.shields.io/github/stars/liangchen527/RIDG)](https://github.com/liangchen527/RIDG) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11158-b31b1b.svg)](https://arxiv.org/abs/2308.11158) | :heavy_minus_sign: |
| Masked Spiking Transformer | [![GitHub](https://img.shields.io/github/stars/bic-L/Masked-Spiking-Transformer)](https://github.com/bic-L/Masked-Spiking-Transformer) | [![arXiv](https://img.shields.io/badge/arXiv-2210.01208-b31b1b.svg)](https://arxiv.org/abs/2210.01208) | :heavy_minus_sign: |
| Prototype Reminiscence and Augmented Asymmetric Knowledge Aggregation for Non-Exemplar Class-Incremental Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distilled Reverse Attention Network for Open-World Compositional Zero-Shot Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.00404-b31b1b.svg)](https://arxiv.org/abs/2303.00404) | :heavy_minus_sign: |
| Candidate-Aware Selective Disambiguation based on Normalized Entropy for Instance-Dependent Partial-Label Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No | [![GitHub](https://img.shields.io/github/stars/xmed-lab/CLIPN)](https://github.com/xmed-lab/CLIPN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12213-b31b1b.svg)](https://arxiv.org/abs/2308.12213) | :heavy_minus_sign: |
| Self-Similarity Driven Scale-Invariant Learning for Weakly Supervised Person Search | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2302.12986-b31b1b.svg)](https://arxiv.org/abs/2302.12986) | :heavy_minus_sign: |
| Sample-Wise Label Confidence Incorporation for Learning with Noisy Labels | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Combating Noisy Labels with Sample Selection by Mining High-Discrepancy Examples | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Spatial-Aware Token for Weakly Supervised Object Localization | [![GitHub](https://img.shields.io/github/stars/wpy1999/SAT)](https://github.com/wpy1999/SAT) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10438-b31b1b.svg)](https://arxiv.org/abs/2303.10438) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Explainable AI for CV

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Towards Improved Input Masking for Convolutional Neural Networks | [![GitHub](https://img.shields.io/github/stars/SriramB-98/layer_masking)](https://github.com/SriramB-98/layer_masking) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14646-b31b1b.svg)](https://arxiv.org/abs/2211.14646) | :heavy_minus_sign: |
| PDiscoNet: Semantically Consistent Part Discovery for Fine-Grained Recognition | [![GitHub](https://img.shields.io/github/stars/robertdvdk/part_detection)](https://github.com/robertdvdk/part_detection) | [![HAL Science](https://img.shields.io/badge/hal-science-040060.svg)](https://hal.inrae.fr/hal-04183747) | :heavy_minus_sign: |
| Corrupting Neuron Explanations of Deep Visual Features | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ICICLE: Interpretable Class Incremental Continual Learning | [![GitHub](https://img.shields.io/github/stars/gmum/ICICLE)](https://github.com/gmum/ICICLE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07811-b31b1b.svg)](https://arxiv.org/abs/2303.07811) | :heavy_minus_sign: |
| ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.eml-unitue.de/publication/ProbVLM) <br /> [![GitHub](https://img.shields.io/github/stars/ExplainableML/ProbVLM)](https://github.com/ExplainableML/ProbVLM) | [![arXiv](https://img.shields.io/badge/arXiv-2307.00398-b31b1b.svg)](https://arxiv.org/abs/2307.00398) | :heavy_minus_sign: |
| Out-of-Distribution Detection for Monocular Depth Estimation | [![GitHub](https://img.shields.io/github/stars/jhornauer/mde_ood)](https://github.com/jhornauer/mde_ood) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06072-b31b1b.svg)](https://arxiv.org/abs/2308.06072) | :heavy_minus_sign: |
| Using Explanations to Guide Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.11932-b31b1b.svg)](https://arxiv.org/abs/2303.11932) | :heavy_minus_sign: |
| Rosetta Neurons: Mining the Common Units in a Model Zoo | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yossigandelsman.github.io/rosetta_neurons/) <br /> [![GitHub](https://img.shields.io/github/stars/yossigandelsman/rosetta_neurons)](https://github.com/yossigandelsman/rosetta_neurons) | [![arXiv](https://img.shields.io/badge/arXiv-2306.09346-b31b1b.svg)](https://arxiv.org/abs/2306.09346) | :heavy_minus_sign: |
| Prototype-based Dataset Comparison | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nanne.github.io/ProtoSim/) <br /> [![GitHub](https://img.shields.io/github/stars/Nanne/ProtoSim)](https://github.com/Nanne/ProtoSim) | [![arXiv](https://img.shields.io/badge/arXiv-2309.02401-b31b1b.svg)](https://arxiv.org/abs/2309.02401) | :heavy_minus_sign: |
| Learning to Identify Critical States for Reinforcement Learning from Videos | [![GitHub](https://img.shields.io/github/stars/AI-Initiative-KAUST/VideoRLCS)](https://github.com/AI-Initiative-KAUST/VideoRLCS) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07795-b31b1b.svg)](https://arxiv.org/abs/2308.07795) | :heavy_minus_sign: |
| Leaping Into Memories: Space-Time Deep Feature Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alexandrosstergiou.github.io/project_pages/LEAPS/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/alexandrosstergiou/Leaping-Into-Memories)](https://github.com/alexandrosstergiou/Leaping-Into-Memories) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09941-b31b1b.svg)](https://arxiv.org/abs/2303.09941) | :heavy_minus_sign: |
| MAGI: Multi-Annotated Explanation-Guided Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability | [![GitHub](https://img.shields.io/github/stars/havelhuang/Eval_XAI_Robustness)](https://github.com/havelhuang/Eval_XAI_Robustness) | [![arXiv](https://img.shields.io/badge/arXiv-2208.09418-b31b1b.svg)](https://arxiv.org/abs/2208.09418) | :heavy_minus_sign: |
| Do BLIP and Stable Diffusion Understand Each Other? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dalleflamingo.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.12249-b31b1b.svg)](https://arxiv.org/abs/2212.12249) | :heavy_minus_sign: |
| Evaluation and Improvement of Interpretability for Self-Explainable Part-Prototype Networks | [![GitHub](https://img.shields.io/github/stars/hqhQAQ/EvalProtoPNet)](https://github.com/hqhQAQ/EvalProtoPNet) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05946-b31b1b.svg)](https://arxiv.org/abs/2212.05946) | :heavy_minus_sign: |
| MoreauGrad: Sparse and Robust Interpretation of Neural Networks via Moreau Envelope | [![GitHub](https://img.shields.io/github/stars/buyeah1109/MoreauGrad)](https://github.com/buyeah1109/MoreauGrad) | [![arXiv](https://img.shields.io/badge/arXiv-2302.05294-b31b1b.svg)](https://arxiv.org/abs/2302.05294) | :heavy_minus_sign: |
| Towards Understanding the Generalization of Deepfake Detectors from a Game-Theoretical View | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Counterfactual-based Saliency Map: Towards Visual Contrastive Explanations for Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Beyond Single Path Integrated Gradients for Reliable Input Attribution via Randomized Path Sampling | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Support and Trivial Prototypes for Interpretable Image Classification | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.04011-b31b1b.svg)](https://arxiv.org/abs/2301.04011) | :heavy_minus_sign: |
| Visual Explanations via Iterated Integrated Gradients | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Neural Generative Models

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://energy-based-model.github.io/unsupervised-concept-discovery/) <br /> [![GitHub](https://img.shields.io/github/stars/nanlliu/Unsupervised-Compositional-Concepts-Discovery)](https://github.com/nanlliu/Unsupervised-Compositional-Concepts-Discovery) | [![arXiv](https://img.shields.io/badge/arXiv-2306.05357-b31b1b.svg)](https://arxiv.org/abs/2306.05357) | :heavy_minus_sign: |
| Better Aligning Text-to-Image Models with Human Preference | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tgxs002.github.io/align_sd_web/) <br /> [![GitHub](https://img.shields.io/github/stars/tgxs002/align_sd)](https://github.com/tgxs002/align_sd) | [![arXiv](https://img.shields.io/badge/arXiv-2303.14420-b31b1b.svg)](https://arxiv.org/abs/2303.14420) | :heavy_minus_sign: |
| DLT: Conditioned Layout Generation with Joint Discrete-Continuous Diffusion Layout Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wix-incubator.github.io/DLT/) <br /> [![GitHub](https://img.shields.io/github/stars/wix-incubator/DLT)](https://github.com/wix-incubator/DLT) | [![arXiv](https://img.shields.io/badge/arXiv-2303.03755-b31b1b.svg)](https://arxiv.org/abs/2303.03755) | :heavy_minus_sign: |
| Anti-DreamBooth: Protecting users from Personalized Text-to-Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://anti-dreambooth.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/VinAIResearch/Anti-DreamBooth)](https://github.com/VinAIResearch/Anti-DreamBooth) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15433-b31b1b.svg)](https://arxiv.org/abs/2303.15433) | :heavy_minus_sign: |
| GECCO: Geometrically-Conditioned Point Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jatentaki.github.io/publication/10-03-2023) | [![arXiv](https://img.shields.io/badge/arXiv-2303.05916-b31b1b.svg)](https://arxiv.org/abs/2303.05916) | :heavy_minus_sign: |
| DiffDreamer: Towards Consistent Unsupervised Single-View Scene Extrapolation with Conditional Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://primecai.github.io/diffdreamer) <br /> [![GitHub](https://img.shields.io/github/stars/primecai/DiffDreamer)](https://github.com/primecai/DiffDreamer) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12131-b31b1b.svg)](https://arxiv.org/abs/2211.12131) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=UukyiAqlwcw) |
| Controllable Human Motion Synthesis via Guided Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://korrawe.github.io/gmd-project/) <br /> [![GitHub](https://img.shields.io/github/stars/korrawe/guided-motion-diffusion)](https://github.com/korrawe/guided-motion-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12577-b31b1b.svg)](https://arxiv.org/abs/2305.12577) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=giw0pLIKdsA) |
| COOP: Decoupling and Coupling of Whole-Body Grasping Pose Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Zero-Shot Spatial Layout Conditioning for Text-to-Image Diffusion Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.13754-b31b1b.svg)](https://arxiv.org/abs/2306.13754) | :heavy_minus_sign: |
| StyleDomain: Efficient and Lightweight Parameterizations of StyleGAN for One-Shot and Few-Shot Domain Adaptation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.10229-b31b1b.svg)](https://arxiv.org/abs/2212.10229) | :heavy_minus_sign: |
| GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeffreyxiang.github.io/GRAM-HD/) | [![arXiv](https://img.shields.io/badge/arXiv-2206.07255-b31b1b.svg)](https://arxiv.org/abs/2206.07255) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Uqzs4uN6v8M) |
| Your Diffusion Model is Secretly a Zero-Shot Classifier | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://diffusion-classifier.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/diffusion-classifier/diffusion-classifier)](https://github.com/diffusion-classifier/diffusion-classifier) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16203-b31b1b.svg)](https://arxiv.org/abs/2303.16203) | :heavy_minus_sign: |
| Learning Hierarchical Features with Joint Latent Space Energy-based Prior | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ActFormer: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2203.07706-b31b1b.svg)](https://arxiv.org/abs/2203.07706) | :heavy_minus_sign: |
| Landscape Learning for Neural Network Inversion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2206.09027-b31b1b.svg)](https://arxiv.org/abs/2206.09027) | :heavy_minus_sign: |
| Diffusion in Style | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Diffusion-SDF: Conditional Generative Modeling of Signed Distance Functions | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://light.princeton.edu/publication/diffusion-sdf/) <br /> [![GitHub](https://img.shields.io/github/stars/princeton-computational-imaging/Diffusion-SDF)](https://github.com/princeton-computational-imaging/Diffusion-SDF) | [![arXiv](https://img.shields.io/badge/arXiv-2211.13757-b31b1b.svg)](https://arxiv.org/abs/2211.13757) | :heavy_minus_sign: |
| GETAvatar: Generative Textured Meshes for Animatable Human Avatars | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| A-STAR: Test-Time <i>A</i>ttention <i>S</i>egrega<i>t</i>ion and <i>R</i>etention for Text-to-Image Synthesis | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.14544-b31b1b.svg)](https://arxiv.org/abs/2306.14544) | :heavy_minus_sign: |
| TF-ICON: Diffusion-based Training-Free Cross-Domain Image Composition | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shilin-lu.github.io/tf-icon.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Shilin-LU/TF-ICON)](https://github.com/Shilin-LU/TF-ICON) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12493-b31b1b.svg)](https://arxiv.org/abs/2307.12493) | :heavy_minus_sign: |
| Breaking The Limits of Text-Conditioned 3D Motion Synthesis with Elaborative Descriptions | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://barquerogerman.github.io/BeLFusion/) <br /> [![GitHub](https://img.shields.io/github/stars/BarqueroGerman/BeLFusion)](https://github.com/BarqueroGerman/BeLFusion) | [![arXiv](https://img.shields.io/badge/arXiv-2211.14304-b31b1b.svg)](https://arxiv.org/abs/2211.14304) | :heavy_minus_sign: |
| Delta Denoising Score | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://delta-denoising-score.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.07090-b31b1b.svg)](https://arxiv.org/abs/2304.07090) | :heavy_minus_sign: |
| Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://seanchenxy.github.io/Mimic3DWeb/) <br /> [![GitHub](https://img.shields.io/github/stars/SeanChenxy/Mimic3D)](https://github.com/SeanChenxy/Mimic3D) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09036-b31b1b.svg)](https://arxiv.org/abs/2303.09036) | :heavy_minus_sign: |
| DreamBooth3D: Subject-Driven Text-to-3D Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dreambooth3d.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13508-b31b1b.svg)](https://arxiv.org/abs/2303.13508) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=kKVDrbfvOoA) |
| Feature Proliferation the Cancer in StyleGAN and its Treatments | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unsupervised Facial Performance Editing via Vector-Quantized StyleGAN Representations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| 3D-Aware Image Generation using 2D Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jeffreyxiang.github.io/ivid/) <br /> [![GitHub](https://img.shields.io/github/stars/JeffreyXiang/ivid)](https://github.com/JeffreyXiang/ivid) | [![arXiv](https://img.shields.io/badge/arXiv-2303.17905-b31b1b.svg)](https://arxiv.org/abs/2303.17905) | :heavy_minus_sign: |
| Neural Collage Transfer: Artistic Reconstruction via Material Manipulation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption | [![GitHub](https://img.shields.io/github/stars/sjtuplayer/few-shot-diffusion)](https://github.com/sjtuplayer/few-shot-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2309.03729-b31b1b.svg)](https://arxiv.org/abs/2309.03729) | :heavy_minus_sign: |
| Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lakonik.github.io/ssdnerf/) <br /> [![GitHub](https://img.shields.io/github/stars/Lakonik/SSDNeRF)](https://github.com/Lakonik/SSDNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06714-b31b1b.svg)](https://arxiv.org/abs/2304.06714) | :heavy_minus_sign: |
| Erasing Concepts from Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://erasing.baulab.info/) <br /> [![GitHub](https://img.shields.io/github/stars/rohitgandikota/erasing)](https://github.com/rohitgandikota/erasing) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07345-b31b1b.svg)](https://arxiv.org/abs/2303.07345) | :heavy_minus_sign: |
| Make Encoder Great Again in 3D GAN Inversion through Geometry and Occlusion-Aware Encoding | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://eg3d-goae.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/jiangyzy/GOAE)](https://github.com/jiangyzy/GOAE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12326-b31b1b.svg)](https://arxiv.org/abs/2303.12326) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CptQDMqM9Pc) |
| HairNeRF: Geometry-Aware Hair Swapped Image Synthesis | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Language

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| SMAUG: Sparse Masked Autoencoder for Efficient Video-Language Pre-Training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.11446-b31b1b.svg)](https://arxiv.org/abs/2211.11446) | :heavy_minus_sign: |
| DiffusionRet: Generative Text-Video Retrieval with Diffusion Model | [![GitHub](https://img.shields.io/github/stars/jpthu17/DiffusionRet)](https://github.com/jpthu17/DiffusionRet) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09867-b31b1b.svg)](https://arxiv.org/abs/2303.09867) | :heavy_minus_sign: |
| Explore and Tell: Embodied Visual Captioning in 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aim3-ruc.github.io/ExploreAndTell/) <br /> [![GitHub](https://img.shields.io/github/stars/HAWLYQ/ET-Cap)](https://github.com/HAWLYQ/ET-Cap) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10447-b31b1b.svg)](https://arxiv.org/abs/2308.10447) | :heavy_minus_sign: |
| Distilling Large Vision-Language Model with Out-of-Distribution Generalizability | [![GitHub](https://img.shields.io/github/stars/xuanlinli17/large_vlm_distillation_ood)](https://github.com/xuanlinli17/large_vlm_distillation_ood) | [![arXiv](https://img.shields.io/badge/arXiv-2307.03135-b31b1b.svg)](https://arxiv.org/abs/2307.03135) | :heavy_minus_sign: |
| Learning Trajectory-Word Alignments for Video-Language Tasks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.01953-b31b1b.svg)](https://arxiv.org/abs/2301.01953) | :heavy_minus_sign: |
| Variational Causal Inference Network for Explanatory Visual Question Answering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TextManiA: Enriching Visual Feature by Text-Driven Manifold Augmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://textmania.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/postech-ami/TextManiA)](https://github.com/postech-ami/TextManiA) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14611-b31b1b.svg)](https://arxiv.org/abs/2307.14611) | :heavy_minus_sign: |
| UniRef: A Unified Model for Reference-based Object Segmentation Tasks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.06571-b31b1b.svg)](https://arxiv.org/abs/2303.06571) | :heavy_minus_sign: |
| Misalign, Contrast then Distill: Rethinking Misalignments in Language-Image Pre-Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Toward Multi-Granularity Decision-Making: Explicit Visual Reasoning with Hierarchical Knowledge | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Moment Detection in Long Tutorial Videos | [![GitHub](https://img.shields.io/github/stars/ioanacroi/longmoment-detr)](https://github.com/ioanacroi/longmoment-detr) | :heavy_minus_sign: | :heavy_minus_sign: |
| Not All Features Matter: Enhancing Few-Shot CLIP with Adaptive Prior Refinement | [![GitHub](https://img.shields.io/github/stars/yangyangyang127/APE)](https://github.com/yangyangyang127/APE) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01195-b31b1b.svg)](https://arxiv.org/abs/2304.01195) | :heavy_minus_sign: |
| Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://whoops-benchmark.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07274-b31b1b.svg)](https://arxiv.org/abs/2303.07274) | :heavy_minus_sign: |
| Advancing Referring Expression Segmentation Beyond Single Image | [![GitHub](https://img.shields.io/github/stars/yixuan730/group-res)](https://github.com/yixuan730/group-res) | [![arXiv](https://img.shields.io/badge/arXiv-2305.12452-b31b1b.svg)](https://arxiv.org/abs/2305.12452) | :heavy_minus_sign: |
| CLIPoint: Adapting CLIP for Powerful 3D Open-World Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unsupervised Prompt Tuning for Text-Driven Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distilling Coarse-to-Fine Semantic Matching Knowledge for Weakly Supervised 3D Visual Grounding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.09267-b31b1b.svg)](https://arxiv.org/abs/2307.09267) | :heavy_minus_sign: |
| I can't Believe there's no Images! Learning Visual Tasks using Only Language Data | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://prior.allenai.org/projects/close) <br /> [![GitHub](https://img.shields.io/github/stars/allenai/close)](https://github.com/allenai/close) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09778-b31b1b.svg)](https://arxiv.org/abs/2211.09778) | :heavy_minus_sign: |
| Learning Cross-Modal Affinity for Referring Video Object Segmentation Targeting Limited Samples | [![GitHub](https://img.shields.io/github/stars/hengliusky/Few_shot_RVOS)](https://github.com/hengliusky/Few_shot_RVOS) | [![arXiv](https://img.shields.io/badge/arXiv-2309.02041-b31b1b.svg)](https://arxiv.org/abs/2309.02041) | :heavy_minus_sign: |
| MeViS: A Large-Scale Benchmark for Video Segmentation with Motion Expressions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://henghuiding.github.io/MeViS/) <br /> [![GitHub](https://img.shields.io/github/stars/henghuiding/MeViS)](https://github.com/henghuiding/MeViS) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08544-b31b1b.svg)](https://arxiv.org/abs/2308.08544) | :heavy_minus_sign: |
| Diverse Data Augmentation with Diffusions for Effective Test-Time Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/chunmeifeng/DiffTPT)](https://github.com/chunmeifeng/DiffTPT) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06038-b31b1b.svg)](https://arxiv.org/abs/2308.06038) | :heavy_minus_sign: |
| ShapeScaffolder: Structure-Aware 3D Shape Generation from Text | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://www.yongliangyang.net/docs/shapescaffolder_iccv23.pdf) | :heavy_minus_sign: |
| SuS-X: Training-Free Name-Only Transfer of Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vishaal27.github.io/SuS-X-webpage/) <br /> [![GitHub](https://img.shields.io/github/stars/vishaal27/SuS-X)](https://github.com/vishaal27/SuS-X) | [![arXiv](https://img.shields.io/badge/arXiv-2211.16198-b31b1b.svg)](https://arxiv.org/abs/2211.16198) | :heavy_minus_sign: |
| BEVBert: Multimodal Map Pre-Training for Language-Guided Navigation | [![GitHub](https://img.shields.io/github/stars/MarSaKi/VLN-BEVBert)](https://github.com/MarSaKi/VLN-BEVBert) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04385-b31b1b.svg)](https://arxiv.org/abs/2212.04385) | :heavy_minus_sign: |
| X-Mesh: Towards Fast and Accurate Text-Driven 3D Stylization via Dynamic Textual Guidance | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xmu-xiaoma666.github.io/Projects/X-Mesh/) <br /> [![GitHub](https://img.shields.io/github/stars/xmu-xiaoma666/X-Mesh)](https://github.com/xmu-xiaoma666/X-Mesh) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15764-b31b1b.svg)](https://arxiv.org/abs/2303.15764) | :heavy_minus_sign: |
| OnlineRefer: A Simple Online Baseline for Referring Video Object Segmentation | [![GitHub](https://img.shields.io/github/stars/wudongming97/OnlineRefer)](https://github.com/wudongming97/OnlineRefer) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09356-b31b1b.svg)](https://arxiv.org/abs/2307.09356) | :heavy_minus_sign: |
| Attentive Mask CLIP | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.08653-b31b1b.svg)](https://arxiv.org/abs/2212.08653) | :heavy_minus_sign: |
| Knowledge Proxy Intervention for Deconfounded Video Question Answering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| UniVTG: Towards Unified Video-Language Temporal Grounding | [![GitHub](https://img.shields.io/github/stars/showlab/UniVTG)](https://github.com/showlab/UniVTG) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16715-b31b1b.svg)](https://arxiv.org/abs/2307.16715) | :heavy_minus_sign: |
| Self-Supervised Cross-View Representation Reconstruction for Change Captioning | [![GitHub](https://img.shields.io/github/stars/tuyunbin/SCORER)](https://github.com/tuyunbin/SCORER) | :heavy_minus_sign: | :heavy_minus_sign: |
| Unified Coarse-to-Fine Alignment for Video-Text Retrieval | [![GitHub](https://img.shields.io/github/stars/Ziyang412/UCoFiA)](https://github.com/Ziyang412/UCoFiA) | [![arXiv](https://img.shields.io/badge/arXiv-2309.10091-b31b1b.svg)](https://arxiv.org/abs/2309.10091) | :heavy_minus_sign: |
| Confidence-Aware Pseudo-Label Learning for Weakly Supervised Visual Grounding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TextPSG: Panoptic Scene Graph Generation from Textual Descriptions | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wlin-at.github.io/maxi) <br /> [![GitHub](https://img.shields.io/github/stars/wlin-at/MAXI)](https://github.com/wlin-at/MAXI) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08914-b31b1b.svg)](https://arxiv.org/abs/2303.08914) | :heavy_minus_sign: |
| Unify, Align and Refine: Multi-Level Semantic Alignment for Radiology Report Generation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.15932-b31b1b.svg)](https://arxiv.org/abs/2303.15932) | :heavy_minus_sign: |
| Transferring Visual Knowledge with Pre-Trained Models for Multimodal Machine Translation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://devaansh100.github.io/projects/cliptrans/) <br /> [![GitHub](https://img.shields.io/github/stars/devaansh100/CLIPTrans)](https://github.com/devaansh100/CLIPTrans) | [![arXiv](https://img.shields.io/badge/arXiv-2308.15226-b31b1b.svg)](https://arxiv.org/abs/2308.15226) | :heavy_minus_sign: |
| Learning Human-Human Interactions in Images from Weak Textual Supervision | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tau-vailab.github.io/learning-interactions/) <br /> [![GitHub](https://img.shields.io/github/stars/TAU-VAILab/learning-interactions)](https://github.com/TAU-VAILab/learning-interactions) | [![arXiv](https://img.shields.io/badge/arXiv-2304.14104-b31b1b.svg)](https://arxiv.org/abs/2304.14104) | :heavy_minus_sign: |
| BUS: Efficient and Effective Vision-Language Pretraining with Bottom-Up Patch Summarization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08504-b31b1b.svg)](https://arxiv.org/abs/2307.08504) | :heavy_minus_sign: |
| 3D-VisTA: Pre-Trained Transformer for 3D Vision and Text Alignment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3d-vista.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/3d-vista/3D-VisTA)](https://github.com/3d-vista/3D-VisTA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04352-b31b1b.svg)](https://arxiv.org/abs/2308.04352) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uUtMaoif8DQ&t=1s) |
| ALIP: Adaptive Language-Image Pre-Training with Synthetic Caption | [![GitHub](https://img.shields.io/github/stars/deepglint/ALIP)](https://github.com/deepglint/ALIP) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08428-b31b1b.svg)](https://arxiv.org/abs/2308.08428) | :heavy_minus_sign: |
| LoGoPrompt: Synthetic Text Images can be Good Visual Prompts for Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chengshiest.github.io/logo/) | [![arXiv](https://img.shields.io/badge/arXiv-2309.01155-b31b1b.svg)](https://arxiv.org/abs/2309.01155) | :heavy_minus_sign: |
| Noise-Aware Learning from Web-Crawled Image-Text Data for Image Captioning | [![GitHub](https://img.shields.io/github/stars/kakaobrain/noc)](https://github.com/kakaobrain/noc) | [![arXiv](https://img.shields.io/badge/arXiv-2212.13563-b31b1b.svg)](https://arxiv.org/abs/2212.13563) | :heavy_minus_sign: |
| Decouple Before Interact: Multi-Modal Prompt Learning for Continual Visual Question Answering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Prompt-Guided Image Captioning for VQA with GPT-3 | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yushi-hu.github.io/promptcap_demo/) <br /> [![GitHub](https://img.shields.io/github/stars/Yushi-Hu/PromptCap)](https://github.com/Yushi-Hu/PromptCap) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09699-b31b1b.svg)](https://arxiv.org/abs/2211.09699) | :heavy_minus_sign: |
| Grounded Image Text Matching with Mismatched Relation Reasoning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.01236-b31b1b.svg)](https://arxiv.org/abs/2308.01236) | :heavy_minus_sign: |
| GePSAn: Generative Procedure Step Anticipation in Cooking Videos | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dki-lab.github.io/LLM-Planner/) <br /> [![GitHub](https://img.shields.io/github/stars/OSU-NLP-Group/LLM-Planner)](https://github.com/OSU-NLP-Group/LLM-Planner) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04088-b31b1b.svg)](https://arxiv.org/abs/2212.04088) | :heavy_minus_sign: |
| VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control | [![GitHub](https://img.shields.io/github/stars/HenryHZY/VL-PET)](https://github.com/HenryHZY/VL-PET) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09804-b31b1b.svg)](https://arxiv.org/abs/2308.09804) | :heavy_minus_sign: |
| With a Little Help from Your own Past: Prototypical Memory Networks for Image Captioning | [![GitHub](https://img.shields.io/github/stars/aimagelab/PMA-Net)](https://github.com/aimagelab/PMA-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12383-b31b1b.svg)](https://arxiv.org/abs/2308.12383) | :heavy_minus_sign: |
| Improving Zero-Shot Generalization for CLIP with Synthesized Prompts | [![GitHub](https://img.shields.io/github/stars/mrflogs/SHIP)](https://github.com/mrflogs/SHIP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07397-b31b1b.svg)](https://arxiv.org/abs/2307.07397) | :heavy_minus_sign: |
| DALL-Eval: Probing the Reasoning Skills and Social Biases of Text-to-Image Generation Models | [![GitHub](https://img.shields.io/github/stars/j-min/DallEval)](https://github.com/j-min/DallEval) | [![arXiv](https://img.shields.io/badge/arXiv-2202.04053-b31b1b.svg)](https://arxiv.org/abs/2202.04053) | :heavy_minus_sign: |
| Learning Navigational Visual Representations with Semantic Map Supervision | [![GitHub](https://img.shields.io/github/stars/YicongHong/Ego2Map-NaViT)](https://github.com/YicongHong/Ego2Map-NaViT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12335-b31b1b.svg)](https://arxiv.org/abs/2307.12335) | :heavy_minus_sign: |
| CoTDet: Affordance Knowledge Prompting for Task Driven Object Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://toneyaya.github.io/cotdet/) | [![arXiv](https://img.shields.io/badge/arXiv-2309.01093-b31b1b.svg)](https://arxiv.org/abs/2309.01093) | :heavy_minus_sign: |
| Open Set Video HOI detection from Action-Centric Chain-of-Look Prompting | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Concise and Descriptive Attributes for Visual Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.03685-b31b1b.svg)](https://arxiv.org/abs/2308.03685) | :heavy_minus_sign: |
| Open-Vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models | [![GitHub](https://img.shields.io/github/stars/mlvlab/OVQA)](https://github.com/mlvlab/OVQA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09363-b31b1b.svg)](https://arxiv.org/abs/2308.09363) | :heavy_minus_sign: |
| Encyclopedic VQA: Visual Questions About Detailed Properties of Fine-Grained Categories | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/google-research/tree/master/encyclopedic_vqa) | [![arXiv](https://img.shields.io/badge/arXiv-2306.09224-b31b1b.svg)](https://arxiv.org/abs/2306.09224) | :heavy_minus_sign: |
| Story Visualization by Online Text Augmentation with Context Memory | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://dcahn12.github.io/projects/CMOTA/) <br /> [![GitHub](https://img.shields.io/github/stars/yonseivnl/cmota)](https://github.com/yonseivnl/cmota) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07575-b31b1b.svg)](https://arxiv.org/abs/2308.07575) | :heavy_minus_sign: |
| Transferable Decoding with Visual Entities for Zero-Shot Image Captioning | [![GitHub](https://img.shields.io/github/stars/FeiElysia/ViECap)](https://github.com/FeiElysia/ViECap) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16525-b31b1b.svg)](https://arxiv.org/abs/2307.16525) | :heavy_minus_sign: |
| Too Large; Data Reduction for Vision-Language Pre-Training | [![GitHub](https://img.shields.io/github/stars/showlab/datacentric.vlp)](https://github.com/showlab/datacentric.vlp) | [![arXiv](https://img.shields.io/badge/arXiv-2305.20087-b31b1b.svg)](https://arxiv.org/abs/2305.20087) | :heavy_minus_sign: |
| ViLTA: Enhancing Vision-Language Pre-Training through Textual Augmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.16689-b31b1b.svg)](https://arxiv.org/abs/2308.16689) | :heavy_minus_sign: |
| Zero-Shot Composed Image Retrieval with Textual Inversion | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://circo.micc.unifi.it/demo) <br /> [![GitHub](https://img.shields.io/github/stars/miccunifi/SEARLE)](https://github.com/miccunifi/SEARLE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15247-b31b1b.svg)](https://arxiv.org/abs/2303.15247) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision, Graphics, and Robotics

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Adding Conditional Control to Text-to-Image Diffusion Models | [![GitHub](https://img.shields.io/github/stars/lllyasviel/ControlNet)](https://github.com/lllyasviel/ControlNet) | [![arXiv](https://img.shields.io/badge/arXiv-2302.05543-b31b1b.svg)](https://arxiv.org/abs/2302.05543) | :heavy_minus_sign: |
| Factorized Inverse Path Tracing for Efficient and Accurate Material-Lighting Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jerrypiglet.github.io/fipt-ucsd/) <br /> [![GitHub](https://img.shields.io/github/stars/lwwu2/fipt)](https://github.com/lwwu2/fipt) | [![arXiv](https://img.shields.io/badge/arXiv-2304.05669-b31b1b.svg)](https://arxiv.org/abs/2304.05669) |  |
| Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://agi-labs.github.io/manipulate-by-seeing/) <br /> [![GitHub](https://img.shields.io/github/stars/AGI-Labs/manipulate-by-seeing)](https://github.com/AGI-Labs/manipulate-by-seeing) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08135-b31b1b.svg)](https://arxiv.org/abs/2303.08135) | :heavy_minus_sign: |
| 3D Implicit Transporter for Temporally Consistent Keypoint Discovery | [![GitHub](https://img.shields.io/github/stars/zhongcl-thu/3D-Implicit-Transporter)](https://github.com/zhongcl-thu/3D-Implicit-Transporter) | [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/373328882_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery) | :heavy_minus_sign: |
| Chordal Averaging on Flag Manifolds and its Applications | [![GitHub](https://img.shields.io/github/stars/nmank/FlagAveraging)](https://github.com/nmank/FlagAveraging) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13501-b31b1b.svg)](https://arxiv.org/abs/2303.13501) | :heavy_minus_sign: |
| UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-Aware Curriculum and Iterative Generalist-Specialist Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.00464-b31b1b.svg)](https://arxiv.org/abs/2304.00464) | :heavy_minus_sign: |
| GameFormer: Game-Theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mczhi.github.io/GameFormer/) <br /> [![GitHub](https://img.shields.io/github/stars/MCZhi/GameFormer)](https://github.com/MCZhi/GameFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2303.05760-b31b1b.svg)](https://arxiv.org/abs/2303.05760) | :heavy_minus_sign: |
| PPR: Physically Plausible Reconstruction from Monocular Videos | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://gengshan-y.github.io/ppr/) <br /> [![GitHub](https://img.shields.io/github/stars/gengshan-y/ppr)](https://github.com/gengshan-y/ppr) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://gengshan-y.github.io/ppr/PPR.pdf) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Privacy, Security, Fairness, and Explainability

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Zolly: Zoom Focal Length Correctly for Perspective-Distorted Human Mesh Reconstruction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wenjiawang0312.github.io/projects/zolly/) <br /> [![GitHub](https://img.shields.io/github/stars/WenjiaWang0312/Zolly)](https://github.com/WenjiaWang0312/Zolly) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13796-b31b1b.svg)](https://arxiv.org/abs/2303.13796) | :heavy_minus_sign: |
| ACLS: Adaptive and Conditional Label Smoothing for Network Calibration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cvlab.yonsei.ac.kr/projects/ACLS/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11911-b31b1b.svg)](https://arxiv.org/abs/2308.11911) | :heavy_minus_sign: |
| PGFed: Personalize Each Client's Global Objective for Federated Learning | [![GitHub](https://img.shields.io/github/stars/ljaiverson/pgfed)](https://github.com/ljaiverson/pgfed) | [![arXiv](https://img.shields.io/badge/arXiv-2212.01448-b31b1b.svg)](https://arxiv.org/abs/2212.01448) | :heavy_minus_sign: |
| Overwriting Pretrained Bias with Finetuning Data | [![GitHub](https://img.shields.io/github/stars/princetonvisualai/overcoming-pretraining-bias)](https://github.com/princetonvisualai/overcoming-pretraining-bias) | [![arXiv](https://img.shields.io/badge/arXiv-2303.06167-b31b1b.svg)](https://arxiv.org/abs/2303.06167) <br /> [![ResearchGate](https://img.shields.io/badge/Research-Gate-D7E7F5.svg)](https://www.researchgate.net/publication/369199104_Overcoming_Bias_in_Pretrained_Models_by_Manipulating_the_Finetuning_Dataset) | :heavy_minus_sign: |
| ITI-GEN: Inclusive Text-to-Image Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://czhang0528.github.io/iti-gen) <br /> [![GitHub](https://img.shields.io/github/stars/humansensinglab/ITI-GEN)](https://github.com/humansensinglab/ITI-GEN) | [![arXiv](https://img.shields.io/badge/arXiv-2309.05569-b31b1b.svg)](https://arxiv.org/abs/2309.05569) | :heavy_minus_sign: |
| FunnyBirds: A Synthetic Vision Dataset for a Part-based Analysis of Explainable AI Methods | [![GitHub](https://img.shields.io/github/stars/visinf/funnybirds)](https://github.com/visinf/funnybirds) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06248-b31b1b.svg)](https://arxiv.org/abs/2308.06248) | :heavy_minus_sign: |
| X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events | [![GitHub](https://img.shields.io/github/stars/daibopku/X-VoE)](https://github.com/daibopku/X-VoE) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10441-b31b1b.svg)](https://arxiv.org/abs/2308.10441) | :heavy_minus_sign: |
| Adaptive Testing of Computer Vision Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.02774-b31b1b.svg)](https://arxiv.org/abs/2212.02774) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Enhancing Privacy Preservation in Federated Learning via Learning Rate Perturbation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation | [![GitHub](https://img.shields.io/github/stars/zj-jayzhang/Federated-Class-Continual-Learning)](https://github.com/zj-jayzhang/Federated-Class-Continual-Learning) | [![arXiv](https://img.shields.io/badge/arXiv-2303.06937-b31b1b.svg)](https://arxiv.org/abs/2303.06937) | :heavy_minus_sign: |
| FACTS: First Amplify Correlations and then Slice to Discover Bias | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Computation and Data Efficient Backdoor Attacks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Global Balanced Experts for Federated Long-Tailed Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Source-Free Domain Adaptive Human Pose Estimation | [![GitHub](https://img.shields.io/github/stars/davidpengucf/SFDAHPE)](https://github.com/davidpengucf/SFDAHPE) | [![arXiv](https://img.shields.io/badge/arXiv-2308.03202-b31b1b.svg)](https://arxiv.org/abs/2308.03202) | :heavy_minus_sign: |
| Gender Artifacts in Visual Datasets | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://princetonvisualai.github.io/gender-artifacts/) <br /> [![GitHub](https://img.shields.io/github/stars/princetonvisualai/gender-artifacts)](https://github.com/princetonvisualai/gender-artifacts) | [![arXiv](https://img.shields.io/badge/arXiv-2206.09191-b31b1b.svg)](https://arxiv.org/abs/2206.09191) | :heavy_minus_sign: |
| FRAug: Tackling Federated Learning with Non-IID Features via Representation Augmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2205.14900-b31b1b.svg)](https://arxiv.org/abs/2205.14900) | :heavy_minus_sign: |
| zPROBE: Zero Peek Robustness Checks for Federated Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2206.12100-b31b1b.svg)](https://arxiv.org/abs/2206.12100) | :heavy_minus_sign: |
| Practical Membership Inference Attacks Against Large-Scale Multi-Modal Models: A Pilot Study | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| FedPD: Federated Open Set Recognition with Parameter Disentanglement | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MUter: Machine Unlearning for Adversarial Training Models | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Beyond Skin Tone: A Multidimensional Measure of Apparent Skin Color | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.05148-b31b1b.svg)](https://arxiv.org/abs/2309.05148) | :heavy_minus_sign: |
| A Multidimensional Analysis of Social Biases in Vision Transformers | [![GitHub](https://img.shields.io/github/stars/jannik-brinkmann/social-biases-in-vision-transformers)](https://github.com/jannik-brinkmann/social-biases-in-vision-transformers) | [![arXiv](https://img.shields.io/badge/arXiv-2308.01948-b31b1b.svg)](https://arxiv.org/abs/2308.01948) | :heavy_minus_sign: |
| Partition-and-Debias: Agnostic Biases Mitigation via a Mixture of Biases-Specific Experts | [![GitHub](https://img.shields.io/github/stars/Jiaxuan-Li/PnD)](https://github.com/Jiaxuan-Li/PnD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10005-b31b1b.svg)](https://arxiv.org/abs/2308.10005) | :heavy_minus_sign: |
| Rethinking Data Distillation: Do not Overlook Calibration | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.12463-b31b1b.svg)](https://arxiv.org/abs/2307.12463) | :heavy_minus_sign: |
| Mining Bias-Target Alignment from Voronoi Cells | [![GitHub](https://img.shields.io/github/stars/renahon/mining_bias_target_alignment_from_voronoi_cells)](https://github.com/renahon/mining_bias_target_alignment_from_voronoi_cells) | [![arXiv](https://img.shields.io/badge/arXiv-2305.03691-b31b1b.svg)](https://arxiv.org/abs/2305.03691) | :heavy_minus_sign: |
| Better May not be Fairer: A Study on Subgroup Discrepancy in Image Classification | [![GitHub](https://img.shields.io/github/stars/charismaticchiu/CIFAR-B)](https://github.com/charismaticchiu/CIFAR-B) | :heavy_minus_sign: | :heavy_minus_sign: |
| GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization | [![GitHub](https://img.shields.io/github/stars/ffhibnese/GIFD)](https://github.com/ffhibnese/GIFD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04699-b31b1b.svg)](https://arxiv.org/abs/2308.04699) | :heavy_minus_sign: |
| Benchmarking Algorithmic Bias in Face Recognition: An Experimental Approach using Synthetic Faces and Human Evaluation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.05441-b31b1b.svg)](https://arxiv.org/abs/2308.05441) | :heavy_minus_sign: |
| FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning | [![GitHub](https://img.shields.io/github/stars/imguangyu/FedPerfix)](https://github.com/imguangyu/FedPerfix) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09160-b31b1b.svg)](https://arxiv.org/abs/2308.09160) | :heavy_minus_sign: |
| Towards Attack-Tolerant Federated Learning via Critical Parameter Analysis | [![GitHub](https://img.shields.io/github/stars/Sungwon-Han/FEDCPA)](https://github.com/Sungwon-Han/FEDCPA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09318-b31b1b.svg)](https://arxiv.org/abs/2308.09318) | :heavy_minus_sign: |
| What can Discriminator do? Towards Box-Free Ownership Verification of Generative Adversarial Networks | [![GitHub](https://img.shields.io/github/stars/AbstractTeen/gan_ownership_verification)](https://github.com/AbstractTeen/gan_ownership_verification) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15860-b31b1b.svg)](https://arxiv.org/abs/2307.15860) | :heavy_minus_sign: |
| Robust Heterogeneous Federated Learning under Data Corruption | [![GitHub](https://img.shields.io/github/stars/FangXiuwen/AugHFL)](https://github.com/FangXiuwen/AugHFL) | :heavy_minus_sign: | :heavy_minus_sign: |
| Communication-Efficient Federated Learning with Single-Step Synthetic Features Compressor for Faster Convergence | [![GitHub](https://img.shields.io/github/stars/Soptq/iccv23-3sfc)](https://github.com/Soptq/iccv23-3sfc) | [![arXiv](https://img.shields.io/badge/arXiv-2302.13562-b31b1b.svg)](https://arxiv.org/abs/2302.13562) | :heavy_minus_sign: |
| GPFL: Simultaneously Learning Global and Personalized Feature Information for Personalized Federated Learning | [![GitHub](https://img.shields.io/github/stars/TsingZ0/GPFL)](https://github.com/TsingZ0/GPFL) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10279-b31b1b.svg)](https://arxiv.org/abs/2308.10279) | :heavy_minus_sign: |
| MPCViT: Searching for Accurate and Efficient MPC-Friendly Vision Transformer with Heterogeneous Attention | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.13955-b31b1b.svg)](https://arxiv.org/abs/2211.13955) | :heavy_minus_sign: |
| Identification of Systematic Errors of Image Classifiers on Rare Subgroups | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.05072-b31b1b.svg)](https://arxiv.org/abs/2303.05072) | :heavy_minus_sign: |
| Adaptive Image Anonymization in the Context of Image Classification with Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| When do Curricula Work in Federated Learning? | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.12712-b31b1b.svg)](https://arxiv.org/abs/2212.12712) | :heavy_minus_sign: |
| Domain Specified Optimization for Deployment Authorization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| STPrivacy: Spatio-Temporal Privacy-Preserving Action Recognition |  | [![arXiv](https://img.shields.io/badge/arXiv-2301.03046-b31b1b.svg)](https://arxiv.org/abs/2301.03046) | :heavy_minus_sign: |
| SAL-ViT: Towards Latency Efficient Private Inference on ViT using Selective Attention Search with a Learnable Softmax Approximation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Generative Gradient Inversion without Prior | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Inspecting the Geographical Representativeness of Images from Text-to-Image Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11080-b31b1b.svg)](https://arxiv.org/abs/2305.11080) | :heavy_minus_sign: |
| Divide and Conquer: A Two-Step Method for High Quality Face De-Identification with Model Explainability | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Exploring the Benefits of Visual Prompting in Differential Privacy | [![GitHub](https://img.shields.io/github/stars/EzzzLi/Prom-PATE)](https://github.com/EzzzLi/Prom-PATE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12247-b31b1b.svg)](https://arxiv.org/abs/2303.12247) | :heavy_minus_sign: |
| Towards Fairness-Aware Adversarial Network Pruning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| AutoReP: Automatic ReLU Replacement for Fast Private Network Inference | [![GitHub](https://img.shields.io/github/stars/HarveyP123/AutoReP)](https://github.com/HarveyP123/AutoReP) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10134-b31b1b.svg)](https://arxiv.org/abs/2308.10134) | :heavy_minus_sign: |
| Flatness-Aware Minimization for Domain Generalization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.11108-b31b1b.svg)](https://arxiv.org/abs/2307.11108) | :heavy_minus_sign: |
| Communication-Efficient Vertical Federated Learning with Limited Overlapping Samples | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/NVIDIA/NVFlare/tree/main/research/one-shot-vfl) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16270-b31b1b.svg)](https://arxiv.org/abs/2303.16270) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### First Person (Egocentric) Vision

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multimodal Distillation for Egocentric Action Recognition | [![GitHub](https://img.shields.io/github/stars/gorjanradevski/multimodal-distillation)](https://github.com/gorjanradevski/multimodal-distillation) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07483-b31b1b.svg)](https://arxiv.org/abs/2307.07483) | :heavy_minus_sign: |
| Self-Supervised Object Detection from Egocentric Videos | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Multi-Label Affordance Mapping from Egocentric Vision | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.02120-b31b1b.svg)](https://arxiv.org/abs/2309.02120) | :heavy_minus_sign: |
| Ego-Only: Egocentric Action Detection without Exocentric Transferring | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2301.01380-b31b1b.svg)](https://arxiv.org/abs/2301.01380) | :heavy_minus_sign: |
| COPILOT: Human-Environment Collision Prediction and Localization from Egocentric Videos | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/stanford.edu/copilot) | [![arXiv](https://img.shields.io/badge/arXiv-2210.01781-b31b1b.svg)](https://arxiv.org/abs/2210.01781) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=lxRTPeac8Oo) |
| EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mvig-rhos.com/ego_pca) | [![arXiv](https://img.shields.io/badge/arXiv-2309.02423-b31b1b.svg)](https://arxiv.org/abs/2309.02423) | :heavy_minus_sign: |
| EgoVLPv2: Egocentric Video-Language Pre-Training with Fusion in the Backbone | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shramanpramanick.github.io/EgoVLPv2/) | [![arXiv](https://img.shields.io/badge/arXiv-2307.05463-b31b1b.svg)](https://arxiv.org/abs/2307.05463) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Representation Learning

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis | [![GitHub](https://img.shields.io/github/stars/ivalab/WDiscOOD)](https://github.com/ivalab/WDiscOOD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.07543-b31b1b.svg)](https://arxiv.org/abs/2303.07543) | :heavy_minus_sign: |
| Pairwise Similarity Learning is SimPLE | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier | [![GitHub](https://img.shields.io/github/stars/ZexiLee/ICCV-2023-FedETF)](https://github.com/ZexiLee/ICCV-2023-FedETF) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10058-b31b1b.svg)](https://arxiv.org/abs/2303.10058) | :heavy_minus_sign: |
| Generalizable Neural Fields as Partially Observed Neural Processes | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.06660-b31b1b.svg)](https://arxiv.org/abs/2309.06660) | :heavy_minus_sign: |
| M2T: Masking Transformers Twice for Faster Decoding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.07313-b31b1b.svg)](https://arxiv.org/abs/2304.07313) | :heavy_minus_sign: |
| Keep it SimPool: Who Said Supervised Transformers Suffer from Attention Deficit? | [![GitHub](https://img.shields.io/github/stars/billpsomas/simpool)](https://github.com/billpsomas/simpool) | [![arXiv](https://img.shields.io/badge/arXiv-2309.06891-b31b1b.svg)](https://arxiv.org/abs/2309.06891) | :heavy_minus_sign: |
| Improving Pixel-based MIM by Reducing Wasted Modeling Capability | [![GitHub](https://img.shields.io/github/stars/open-mmlab/mmpretrain)](https://github.com/open-mmlab/mmpretrain) | [![arXiv](https://img.shields.io/badge/arXiv-2308.00261-b31b1b.svg)](https://arxiv.org/abs/2308.00261) | :heavy_minus_sign: |
| Learning Image-Adaptive Codebooks for Class-Agnostic Image Restoration | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.06513-b31b1b.svg)](https://arxiv.org/abs/2306.06513) | :heavy_minus_sign: |
| Quality Diversity for Visual Pre-Training | [![GitHub](https://img.shields.io/github/stars/ruchikachavhan/quality-diversity-pretraining)](https://github.com/ruchikachavhan/quality-diversity-pretraining) | :heavy_minus_sign: | :heavy_minus_sign: |
| Subclass-Balancing Contrastive Learning for Long-Tailed Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.15925-b31b1b.svg)](https://arxiv.org/abs/2306.15925) | :heavy_minus_sign: |
| Mastering Spatial Graph Prediction of Road Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.00828-b31b1b.svg)](https://arxiv.org/abs/2210.00828) | :heavy_minus_sign: |
| Poincaré ResNet | [![GitHub](https://img.shields.io/github/stars/maxvanspengler/poincare-resnet)](https://github.com/maxvanspengler/poincare-resnet) | [![arXiv](https://img.shields.io/badge/arXiv-2303.14027-b31b1b.svg)](https://arxiv.org/abs/2303.14027) | :heavy_minus_sign: |
| Exploring Model Transferability through the Lens of Potential Energy | [![GitHub](https://img.shields.io/github/stars/lixiaotong97/PED)](https://github.com/lixiaotong97/PED) | [![arXiv](https://img.shields.io/badge/arXiv-2308.15074-b31b1b.svg)](https://arxiv.org/abs/2308.15074) | :heavy_minus_sign: |
| Improving CLIP Fine-Tuning Performance | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unsupervised Manifold Linearizing and Clustering |  | [![arXiv](https://img.shields.io/badge/arXiv-2301.01805-b31b1b.svg)](https://arxiv.org/abs/2301.01805) | :heavy_minus_sign: |
| Generalized Sum Pooling for Metric Learning | [![GitHub](https://img.shields.io/github/stars/yetigurbuz/generalized-sum-pooling)](https://github.com/yetigurbuz/generalized-sum-pooling) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09228-b31b1b.svg)](https://arxiv.org/abs/2308.09228) | :heavy_minus_sign: |
| Partition Speeds Up Learning Implicit Neural Representations based on Exponential-Increase Hypothesis | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.13496-b31b1b.svg)](https://arxiv.org/abs/2303.13496) | :heavy_minus_sign: |
| Token-Label Alignment for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/Euphoria16/TL-Align)](https://github.com/Euphoria16/TL-Align) | [![arXiv](https://img.shields.io/badge/arXiv-2210.06455-b31b1b.svg)](https://arxiv.org/abs/2210.06455) | :heavy_minus_sign: |
| Efficiently Robustify Pre-Trained Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.07499-b31b1b.svg)](https://arxiv.org/abs/2309.07499) | :heavy_minus_sign: |
| OFVL-MS: Once for Visual Localization Across Multiple Indoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/mooncake199809/UFVL-Net/tree/main/configs/ofvl_ms) <br /> [![GitHub](https://img.shields.io/github/stars/mooncake199809/UFVL-Net)](https://github.com/mooncake199809/UFVL-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11928-b31b1b.svg)](https://arxiv.org/abs/2308.11928) | :heavy_minus_sign: |
| Feature Prediction Diffusion Model for Video Anomaly Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Joint Implicit Neural Representation for High-Fidelity and Compact Vector Fonts | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| How Far Pre-Trained Models are from Neural Collapse on the Target Dataset Informs their Transferability | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| OPERA: Omni-Supervised Representation Learning with Hierarchical Supervisions | [![GitHub](https://img.shields.io/github/stars/wangck20/OPERA)](https://github.com/wangck20/OPERA) | [![arXiv](https://img.shields.io/badge/arXiv-2210.05557-b31b1b.svg)](https://arxiv.org/abs/2210.05557) | :heavy_minus_sign: |
| Perceptual Grouping in Contrastive Vision-Language Models | [![GitHub](https://img.shields.io/github/stars/kahnchana/clippy)](https://github.com/kahnchana/clippy) | [![arXiv](https://img.shields.io/badge/arXiv-2210.09996-b31b1b.svg)](https://arxiv.org/abs/2210.09996) | :heavy_minus_sign: |
| Fully Attentional Networks with Self-Emerging Token Labeling | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Instance and Category Supervision are Alternate Learners for Continual Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-Training | [![GitHub](https://img.shields.io/github/stars/HongYan1123/SkeletonMAE)](https://github.com/HongYan1123/SkeletonMAE) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08476-b31b1b.svg)](https://arxiv.org/abs/2307.08476) | :heavy_minus_sign: |
| Motion-Guided Masking for Spatiotemporal Representation Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.12962-b31b1b.svg)](https://arxiv.org/abs/2308.12962) <br /> [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/motion-guided-masking-for-spatiotemporal-representation-learning) | :heavy_minus_sign: |
| Data Augmented Flatness-Aware Gradient Projection for Continual Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Take-a-Photo: 3D-to-2D Generative Pre-Training of Point Cloud Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://tap.ivg-research.xyz/) <br /> [![GitHub](https://img.shields.io/github/stars/wangzy22/TAP)](https://github.com/wangzy22/TAP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14971-b31b1b.svg)](https://arxiv.org/abs/2307.14971) | :heavy_minus_sign: |
| BiViT: Extremely Compressed Binary Vision Transformers | [![GitHub](https://img.shields.io/github/stars/ThisisBillhe/BiViT)](https://github.com/ThisisBillhe/BiViT) | [![arXiv](https://img.shields.io/badge/arXiv-2211.07091-b31b1b.svg)](https://arxiv.org/abs/2211.07091) | :heavy_minus_sign: |
| Spatio-Temporal Crop Aggregation for Video Representation Learning | [![GitHub](https://img.shields.io/github/stars/Separius/SCALE)](https://github.com/Separius/SCALE) | [![arXiv](https://img.shields.io/badge/arXiv-2211.17042-b31b1b.svg)](https://arxiv.org/abs/2211.17042) | :heavy_minus_sign: |
| Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning | [![GitHub](https://img.shields.io/github/stars/HanjaeKim98/CoT)](https://github.com/HanjaeKim98/CoT) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04016-b31b1b.svg)](https://arxiv.org/abs/2308.04016) | :heavy_minus_sign: |
| Semantic Information in Contrastive Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Cross-Domain Product Representation Learning for Rich-Content E-Commerce | [![GitHub](https://img.shields.io/github/stars/adxcreative/COPE)](https://github.com/adxcreative/COPE) | [![arXiv](https://img.shields.io/badge/arXiv-2308.05550-b31b1b.svg)](https://arxiv.org/abs/2308.05550) | :heavy_minus_sign: |
| Contrastive Continuity on Augmentation Stability Rehearsal for Continual Self-Supervised Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness | [![GitHub](https://img.shields.io/github/stars/MKYucel/hybrid_augment)](https://github.com/MKYucel/hybrid_augment) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11823-b31b1b.svg)](https://arxiv.org/abs/2307.11823) | :heavy_minus_sign: |
| Unleashing Text-to-Image Diffusion Models for Visual Perception | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://vpd.ivg-research.xyz/) <br /> [![GitHub](https://img.shields.io/github/stars/wl-zhao/VPD)](https://github.com/wl-zhao/VPD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.02153-b31b1b.svg)](https://arxiv.org/abs/2303.02153) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Deep Learning Architectures

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Efficient Controllable Multi-Task Architectures | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11744-b31b1b.svg)](https://arxiv.org/abs/2308.11744) | :heavy_minus_sign: |
| ParCNetV2: Oversized Kernel with Enhanced Attention | [![GitHub](https://img.shields.io/github/stars/XuRuihan/ParCNetV2)](https://github.com/XuRuihan/ParCNetV2) | [![arXiv](https://img.shields.io/badge/arXiv-2211.07157-b31b1b.svg)](https://arxiv.org/abs/2211.07157) | :heavy_minus_sign: |
| Unleashing the Power of Gradient Signal-to-Noise Ratio for Zero-Shot NAS | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MMST-ViT: Climate Change-Aware Crop Yield Prediction via Multi-Modal Spatial-Temporal Vision Transformer | [![GitHub](https://img.shields.io/github/stars/fudong03/MMST-ViT)](https://github.com/fudong03/MMST-ViT) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1xc_8KkOxVUVsHUiz9Vgv1nqqOa2O_t-2/view) | :heavy_minus_sign: |
| FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization | [![GitHub](https://img.shields.io/github/stars/apple/ml-fastvit)](https://github.com/apple/ml-fastvit) | [![arXiv](https://img.shields.io/badge/arXiv-2303.14189-b31b1b.svg)](https://arxiv.org/abs/2303.14189) | :heavy_minus_sign: |
| IIEU: Rethinking Neural Feature Activation from Decision-Making | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Scratching Visual Transformer's Back with Uniform Attention | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2210.08457-b31b1b.svg)](https://arxiv.org/abs/2210.08457) | :heavy_minus_sign: |
| SpaceEvo: Hardware-Friendly Search Space Design for Efficient INT8 Inference | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.08308-b31b1b.svg)](https://arxiv.org/abs/2303.08308) | :heavy_minus_sign: |
| ElasticViT: Conflict-Aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.09730-b31b1b.svg)](https://arxiv.org/abs/2303.09730) | :heavy_minus_sign: |
| Gramian Attention Heads are Strong yet Efficient Vision Learners | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| EfficientTrain: Exploring Generalized Curriculum Learning for Training Visual Backbones | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/EfficientTrain)](https://github.com/LeapLabTHU/EfficientTrain) | [![arXiv](https://img.shields.io/badge/arXiv-2211.09703-b31b1b.svg)](https://arxiv.org/abs/2211.09703) | :heavy_minus_sign: |
| Ord2Seq: Regarding Ordinal Regression as Label Sequence Prediction | [![GitHub](https://img.shields.io/github/stars/wjh892521292/Ord2Seq)](https://github.com/wjh892521292/Ord2Seq) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09004-b31b1b.svg)](https://arxiv.org/abs/2307.09004) | :heavy_minus_sign: |
| Unified Data-Free Compression: Pruning and Quantization without Fine-Tuning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.07209-b31b1b.svg)](https://arxiv.org/abs/2308.07209) | :heavy_minus_sign: |
| LaPE: Layer-Adaptive Position Embedding for Vision Transformers with Independent Layer Normalization | [![GitHub](https://img.shields.io/github/stars/Ingrid725/LaPE)](https://github.com/Ingrid725/LaPE) | [![arXiv](https://img.shields.io/badge/arXiv-2212.05262-b31b1b.svg)](https://arxiv.org/abs/2212.05262) | :heavy_minus_sign: |
| Exemplar-Free Continual Transformer with Convolutions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cvir.github.io/projects/contracon) <br /> [![GitHub](https://img.shields.io/github/stars/CVIR/contracon)](https://github.com/CVIR/contracon) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11357-b31b1b.svg)](https://arxiv.org/abs/2308.11357) | :heavy_minus_sign: |
| Building Vision Transformers with Hierarchy Aware Feature Aggregation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ShiftNAS: Improving One-Shot NAS via Probability Shift | [![GitHub](https://img.shields.io/github/stars/bestfleer/ShiftNAS)](https://github.com/bestfleer/ShiftNAS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08300-b31b1b.svg)](https://arxiv.org/abs/2307.08300) | :heavy_minus_sign: |
| DarSwin: Distortion Aware Radial Swin Transformer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/darswin/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09691-b31b1b.svg)](https://arxiv.org/abs/2304.09691) | :heavy_minus_sign: |
| ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2011.11233-b31b1b.svg)](https://arxiv.org/abs/2011.11233) | :heavy_minus_sign: |
| FDViT: Improve the Hierarchical Architecture of Vision Transformer | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| FLatten Transformer: Vision Transformer using Focused Linear Attention | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/FLatten-Transformer)](https://github.com/LeapLabTHU/FLatten-Transformer) | [![arXiv](https://img.shields.io/badge/arXiv-2308.00442-b31b1b.svg)](https://arxiv.org/abs/2308.00442) | :heavy_minus_sign: |
| MixPath: A Unified Approach for One-Shot Neural Architecture Search | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2001.05887-b31b1b.svg)](https://arxiv.org/abs/2001.05887) | :heavy_minus_sign: |
| SSF: Accelerating Training of Spiking Neural Networks with Stabilized Spiking Flow | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Dynamic Perceiver for Efficient Visual Recognition | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Dynamic_Perceiver)](https://github.com/LeapLabTHU/Dynamic_Perceiver) | [![arXiv](https://img.shields.io/badge/arXiv-2306.11248-b31b1b.svg)](https://arxiv.org/abs/2306.11248) | :heavy_minus_sign: |
| SG-Former: Self-Guided Transformer with Evolving Token Reallocation | [![GitHub](https://img.shields.io/github/stars/OliverRensu/SG-Former)](https://github.com/OliverRensu/SG-Former) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12216-b31b1b.svg)](https://arxiv.org/abs/2308.12216) | :heavy_minus_sign: |
| Scale-Aware Modulation Meet Transformer | [![GitHub](https://img.shields.io/github/stars/AFeng-x/SMT)](https://github.com/AFeng-x/SMT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08579-b31b1b.svg)](https://arxiv.org/abs/2307.08579) | :heavy_minus_sign: |
| Learning to Upsample by Learning to Sample | [![GitHub](https://img.shields.io/github/stars/tiny-smart/dysample)](https://github.com/tiny-smart/dysample) | [![arXiv](https://img.shields.io/badge/arXiv-2308.15085-b31b1b.svg)](https://arxiv.org/abs/2308.15085) | :heavy_minus_sign: |
| GET: Group Event Transformer for Event-based Vision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Adaptive Frequency Filters as Efficient Global Token Mixers | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/microsoft/TokenMixers/tree/main/Adaptive%20Frequency%20Filters) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14008-b31b1b.svg)](https://arxiv.org/abs/2307.14008) | :heavy_minus_sign: |
| Fcaformer: Forward Cross Attention in Hybrid Vision Transformer | [![GitHub](https://img.shields.io/github/stars/hkzhang-git/FcaFormer)](https://github.com/hkzhang-git/FcaFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2211.07198-b31b1b.svg)](https://arxiv.org/abs/2211.07198) | :heavy_minus_sign: |
| Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yaoleiqi.github.io/pub_homepage/2023_ICCV/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/YaoleiQi/DSCNet)](https://github.com/YaoleiQi/DSCNet) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08388-b31b1b.svg)](https://arxiv.org/abs/2307.08388) | :heavy_minus_sign: |
| Sentence Attention Blocks for Answer Grounding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MST-Compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.13735-b31b1b.svg)](https://arxiv.org/abs/2308.13735) | :heavy_minus_sign: |
| EGformer: Equirectangular Geometry-biased Transformer for 360 Depth Estimation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.07803-b31b1b.svg)](https://arxiv.org/abs/2304.07803) | :heavy_minus_sign: |
| SPANet: Frequency-Balancing Token Mixer using Spectral Pooling Aggregation Modulation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://doranlyong.github.io/projects/spanet/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11568-b31b1b.svg)](https://arxiv.org/abs/2308.11568) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=wEVuA9-jv00) |
| ModelGiF: Gradient Fields for Model Functional Distance | [![GitHub](https://img.shields.io/github/stars/zju-vipa/modelgif)](https://github.com/zju-vipa/modelgif) | :heavy_minus_sign: | :heavy_minus_sign: |
| ClusT3: Information Invariant Test-Time Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Cumulative Spatial Knowledge Distillation for Vision Transformers | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08500-b31b1b.svg)](https://arxiv.org/abs/2307.08500) | :heavy_minus_sign: |
| Luminance-Aware Color Transform for Multiple Exposure Correction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks | [![GitHub](https://img.shields.io/github/stars/qymeng94/SLTT)](https://github.com/qymeng94/SLTT) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14311-b31b1b.svg)](https://arxiv.org/abs/2302.14311) | :heavy_minus_sign: |
| Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DOT: A Distillation-Oriented Trainer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08436-b31b1b.svg)](https://arxiv.org/abs/2307.08436) | :heavy_minus_sign: |
| Extensible and Efficient Proxy for Neural Architecture Search | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning to Transform for Generalizable Instance-Wise Invariance | [![GitHub](https://img.shields.io/github/stars/sutkarsh/flow_inv)](https://github.com/sutkarsh/flow_inv) | :heavy_minus_sign: | :heavy_minus_sign: |
| Convolutional Networks with Oriented 1D Kernels | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Detection

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Random Boxes are Open-World Object Detectors | [![GitHub](https://img.shields.io/github/stars/scuwyh2000/RandBox)](https://github.com/scuwyh2000/RandBox) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08249-b31b1b.svg)](https://arxiv.org/abs/2307.08249) | :heavy_minus_sign: |
| Unleashing Vanilla Vision Transformer with Masked Image Modeling for Object Detection | [![GitHub](https://img.shields.io/github/stars/hustvl/MIMDet)](https://github.com/hustvl/MIMDet) | [![arXiv](https://img.shields.io/badge/arXiv-2204.02964-b31b1b.svg)](https://arxiv.org/abs/2204.02964) | :heavy_minus_sign: |
| CoIn: Contrastive Instance Feature Mining for Outdoor 3D Object Detection with Very Limited Annotations | [![GitHub](https://img.shields.io/github/stars/xmuqimingxia/CoIn)](https://github.com/xmuqimingxia/CoIn) | :heavy_minus_sign: | :heavy_minus_sign: |
| A Dynamic Dual-Processing Object Detection Framework Inspired by the Brain's Recognition Mechanism | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Anchor-Intermediate Detector: Decoupling and Coupling Bounding Boxes for Accurate Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Inter-Realization Channels: Unsupervised Anomaly Detection Beyond One-Class Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Deep Equilibrium Object Detection | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/DEQDet)](https://github.com/MCG-NJU/DEQDet) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09564-b31b1b.svg)](https://arxiv.org/abs/2308.09564) | :heavy_minus_sign: |
| RecursiveDet: End-to-End Region-based Recursive Object Detection | [![GitHub](https://img.shields.io/github/stars/bravezzzzzz/RecursiveDet)](https://github.com/bravezzzzzz/RecursiveDet) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13619-b31b1b.svg)](https://arxiv.org/abs/2307.13619) | :heavy_minus_sign: |
| Small Object Detection via Coarse-to-Fine Proposal Generation and Imitation Learning | [![GitHub](https://img.shields.io/github/stars/shaunyuan22/CFINet)](https://github.com/shaunyuan22/CFINet) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09534-b31b1b.svg)](https://arxiv.org/abs/2308.09534) |  |
| ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation | [![GitHub](https://img.shields.io/github/stars/iSEE-Laboratory/ASAG)](https://github.com/iSEE-Laboratory/ASAG) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09242-b31b1b.svg)](https://arxiv.org/abs/2308.09242) | :heavy_minus_sign: |
| COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/alibaba/easyrobust/tree/main/benchmarks/coco_o) <br /> [![GitHub](https://img.shields.io/github/stars/alibaba/easyrobust)](https://github.com/alibaba/easyrobust) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12730-b31b1b.svg)](https://arxiv.org/abs/2307.12730) | :heavy_minus_sign: |
| Generative Prompt Model for Weakly Supervised Object Localization | [![GitHub](https://img.shields.io/github/stars/callsys/GenPromp)](https://github.com/callsys/GenPromp) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09756-b31b1b.svg)](https://arxiv.org/abs/2307.09756) | :heavy_minus_sign: |
| UniKD: Universal Knowledge Distillation for Mimicking Homogeneous or Heterogeneous Object Detectors | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| PNI: Industrial Anomaly Detection using Position and Neighborhood Information | [![GitHub](https://img.shields.io/github/stars/wogur110/PNI_Anomaly_Detection)](https://github.com/wogur110/PNI_Anomaly_Detection) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12634-b31b1b.svg)](https://arxiv.org/abs/2211.12634) | :heavy_minus_sign: |
| Masked Autoencoders are Stronger Knowledge Distillers | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| GPA-3D: Geometry-Aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds | [![GitHub](https://img.shields.io/github/stars/Liz66666/GPA3D)](https://github.com/Liz66666/GPA3D) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08140-b31b1b.svg)](https://arxiv.org/abs/2308.08140) | :heavy_minus_sign: |
| ADNet: Lane Shape Prediction via Anchor Decomposition | [![GitHub](https://img.shields.io/github/stars/Sephirex-X/ADNet)](https://github.com/Sephirex-X/ADNet) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10481-b31b1b.svg)](https://arxiv.org/abs/2308.10481) | :heavy_minus_sign: |
| Periodically Exchange Teacher-Student for Source-Free Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Towards Fair and Comprehensive Comparisons for Image-based 3D Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Monocular 3D Object Detection with Bounding Box Denoising in 3D by Perceiver | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.01289-b31b1b.svg)](https://arxiv.org/abs/2304.01289) | :heavy_minus_sign: |
| Template-Guided Hierarchical Feature Restoration for Anomaly Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ALWOD: Active Learning for Weakly-Supervised Object Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.07914-b31b1b.svg)](https://arxiv.org/abs/2309.07914) | :heavy_minus_sign: |
| ProtoFL: Unsupervised Federated Learning via Prototypical Distillation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.12450-b31b1b.svg)](https://arxiv.org/abs/2307.12450) | :heavy_minus_sign: |
| Efficient Adaptive Human-Object Interaction Detection with Concept-Guided Memory | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ltttpku.github.io/ADA-CM/) <br /> [![GitHub](https://img.shields.io/github/stars/ltttpku/ADA-CM)](https://github.com/ltttpku/ADA-CM) | [![arXiv](https://img.shields.io/badge/arXiv-2309.03696-b31b1b.svg)](https://arxiv.org/abs/2309.03696) | :heavy_minus_sign: |
| Detection Transformer with Stable Matching | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/Stable-DINO)](https://github.com/IDEA-Research/Stable-DINO) | [![arXiv](https://img.shields.io/badge/arXiv-2304.04742-b31b1b.svg)](https://arxiv.org/abs/2304.04742) | :heavy_minus_sign: |
| Distilling DETR with Visual-Linguistic Knowledge for Open-Vocabulary Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Anomaly Detection Under Distribution Shift | [![GitHub](https://img.shields.io/github/stars/mala-lab/ADShift)](https://github.com/mala-lab/ADShift) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13845-b31b1b.svg)](https://arxiv.org/abs/2303.13845) | :heavy_minus_sign: |
| Detecting Objects with Context-Likelihood Graphs and Graph Refinement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2212.12395-b31b1b.svg)](https://arxiv.org/abs/2212.12395) | :heavy_minus_sign: |
| Unsupervised Object Localization with Representer Point Selection | [![GitHub](https://img.shields.io/github/stars/yeonghwansong/UOLwRPS)](https://github.com/yeonghwansong/UOLwRPS) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04172-b31b1b.svg)](https://arxiv.org/abs/2309.04172) | :heavy_minus_sign: |
| DETR does not Need Multi-Scale or Locality Design | [![GitHub](https://img.shields.io/github/stars/impiga/Plain-DETR)](https://github.com/impiga/Plain-DETR) | [![arXiv](https://img.shields.io/badge/arXiv-2308.01904-b31b1b.svg)](https://arxiv.org/abs/2308.01904) | :heavy_minus_sign: |
| Deep Directly-Trained Spiking Neural Networks for Object Detection | [![GitHub](https://img.shields.io/github/stars/BICLab/EMS-YOLO)](https://github.com/BICLab/EMS-YOLO) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11411-b31b1b.svg)](https://arxiv.org/abs/2307.11411) | :heavy_minus_sign: |
| GACE: Geometry Aware Confidence Enhancement for Black-Box 3D Object Detectors on LiDAR-Data | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| StageInteractor: Query-based Object Detector with Cross-Stage Interaction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.04978-b31b1b.svg)](https://arxiv.org/abs/2304.04978) | :heavy_minus_sign: |
| Adaptive Rotated Convolution for Rotated Object Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.07820-b31b1b.svg)](https://arxiv.org/abs/2303.07820) | :heavy_minus_sign: |
| Decoupled DETR: Spatially Disentangling Localization and Classification for Improved End-to-End Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Exploring Transformers for Open-World Instance Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04206-b31b1b.svg)](https://arxiv.org/abs/2308.04206) | :heavy_minus_sign: |
| DDG-Net: Discriminability-Driven Graph Network for Weakly-Supervised Temporal Action Localization | [![GitHub](https://img.shields.io/github/stars/XiaojunTang22/ICCV2023-DDGNet)](https://github.com/XiaojunTang22/ICCV2023-DDGNet) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16415-b31b1b.svg)](https://arxiv.org/abs/2307.16415) | :heavy_minus_sign: |
| Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment | [![GitHub](https://img.shields.io/github/stars/Atten4Vis/GroupDETR)](https://github.com/Atten4Vis/GroupDETR) | [![arXiv](https://img.shields.io/badge/arXiv-2207.13085-b31b1b.svg)](https://arxiv.org/abs/2207.13085) | :heavy_minus_sign: |
| Category-Aware Allocation Transformer for Weakly Supervised Object Localization | [![GitHub](https://img.shields.io/github/stars/zhiweichen0012/CATR)](https://github.com/zhiweichen0012/CATR) | :heavy_minus_sign: | :heavy_minus_sign: |
| The Devil is in the Crack Orientation: A New Perspective for Crack Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Clusterformer: Cluster-based Transformer for 3D Object Detection in Point Clouds | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Less is more: Focus Attention for Efficient DETR | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/huawei-noah/noah-research/tree/master/Focus-DETR) <br /> [![Gitee Page](https://img.shields.io/badge/Gitee-Page-303643.svg)](https://gitee.com/mindspore/models/tree/master/research/cv/Focus-DETR) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12612-b31b1b.svg)](https://arxiv.org/abs/2307.12612) | :heavy_minus_sign: |
| DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting | [![GitHub](https://img.shields.io/github/stars/IDEA-Research/3D-deformable-attention)](https://github.com/IDEA-Research/3D-deformable-attention) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12972-b31b1b.svg)](https://arxiv.org/abs/2307.12972) | :heavy_minus_sign: |
| Multi-Label Self-Supervised Learning with Scene Images | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.03286-b31b1b.svg)](https://arxiv.org/abs/2308.03286) | :heavy_minus_sign: |
| Cascade-DETR: Delving into High-Quality Universal Object Detection | [![GitHub](https://img.shields.io/github/stars/SysCV/cascade-detr)](https://github.com/SysCV/cascade-detr) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11035-b31b1b.svg)](https://arxiv.org/abs/2307.11035) | :heavy_minus_sign: |
| Representation Disparity-Aware Distillation for 3D Object Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.10308-b31b1b.svg)](https://arxiv.org/abs/2308.10308) | :heavy_minus_sign: |
| FeatEnHancer: Enhancing Hierarchical Features for Object Detection and Beyond Under Low-Light Vision | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.03594-b31b1b.svg)](https://arxiv.org/abs/2308.03594) | :heavy_minus_sign: |
| DetZero: Rethinking Offboard 3D Object Detection with Long-Term Sequential Point Clouds | [![GitHub](https://img.shields.io/github/stars/PJLab-ADG/DetZero)](https://github.com/PJLab-ADG/DetZero) | [![arXiv](https://img.shields.io/badge/arXiv-2306.06023-b31b1b.svg)](https://arxiv.org/abs/2306.06023) | :heavy_minus_sign: |
| DETRs with Collaborative Hybrid Assignments Training | [![GitHub](https://img.shields.io/github/stars/Sense-X/Co-DETR)](https://github.com/Sense-X/Co-DETR) | [![arXiv](https://img.shields.io/badge/arXiv-2211.12860-b31b1b.svg)](https://arxiv.org/abs/2211.12860) | :heavy_minus_sign: |
| Open-Vocabulary Object Detection with an Open Corpus | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SparseDet: Improving Sparsely Annotated Object Detection with Pseudo-Positive Mining | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.cs.umd.edu/~sakshams/SparseDet/) <br /> [![GitHub](https://img.shields.io/github/stars/saksham-s/SparseDet)](https://github.com/saksham-s/SparseDet) | [![arXiv](https://img.shields.io/badge/arXiv-2201.04620-b31b1b.svg)](https://arxiv.org/abs/2201.04620) | :heavy_minus_sign: |
| Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| UniTR: A Unified and Efficient Multi-Modal Transformer for Bird's-Eye-View Representation | [![GitHub](https://img.shields.io/github/stars/Haiyang-W/UniTR)](https://github.com/Haiyang-W/UniTR) | [![arXiv](https://img.shields.io/badge/arXiv-2308.07732-b31b1b.svg)](https://arxiv.org/abs/2308.07732) | :heavy_minus_sign: |
| Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection | [![GitHub](https://img.shields.io/github/stars/xcyao00/FOD)](https://github.com/xcyao00/FOD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.02983-b31b1b.svg)](https://arxiv.org/abs/2308.02983) | :heavy_minus_sign: |
| MonoNeRD: NeRF-Like Representations for Monocular 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/cskkxjk/MonoNeRD)](https://github.com/cskkxjk/MonoNeRD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09421-b31b1b.svg)](https://arxiv.org/abs/2308.09421) | :heavy_minus_sign: |
| Integrally Migrating Pre-Trained Transformer Encoder-Decoders for Visual Object Detection | [![GitHub](https://img.shields.io/github/stars/LiewFeng/imTED)](https://github.com/LiewFeng/imTED) | [![arXiv](https://img.shields.io/badge/arXiv-2205.09613-b31b1b.svg)](https://arxiv.org/abs/2205.09613) | :heavy_minus_sign: |
| Generating Dynamic Kernels via Transformers for Lane Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Meta-ZSDETR: Zero-Shot DETR with Meta-Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09540-b31b1b.svg)](https://arxiv.org/abs/2308.09540) | :heavy_minus_sign: |
| Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det) <br /> [![GitHub](https://img.shields.io/github/stars/ucas-vg/PointTinyBenchmark)](https://github.com/ucas-vg/PointTinyBenchmark) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12101-b31b1b.svg)](https://arxiv.org/abs/2307.12101) | :heavy_minus_sign: |
| AlignDet: Aligning Pre-Training and Fine-Tuning in Object Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://liming-ai.github.io/AlignDet) <br /> [![GitHub](https://img.shields.io/github/stars/liming-ai/AlignDet)](https://github.com/liming-ai/AlignDet) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11077-b31b1b.svg)](https://arxiv.org/abs/2307.11077) | :heavy_minus_sign: |
| MULLER: Multilayer Laplacian Resizer for Vision | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.02859-b31b1b.svg)](https://arxiv.org/abs/2304.02859) | :heavy_minus_sign: |
| Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.10155-b31b1b.svg)](https://arxiv.org/abs/2308.10155) | :heavy_minus_sign: |
| DETRDistill: A Universal Knowledge Distillation Framework for DETR-Families | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.10156-b31b1b.svg)](https://arxiv.org/abs/2211.10156) | :heavy_minus_sign: |
| Delving into Motion-Aware Matching for Monocular 3D Object Tracking | [![GitHub](https://img.shields.io/github/stars/kuanchihhuang/MoMA-M3T)](https://github.com/kuanchihhuang/MoMA-M3T) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11607-b31b1b.svg)](https://arxiv.org/abs/2308.11607) | :heavy_minus_sign: |
| FB-BEV: BEV Representation from Forward-Backward View Transformations | [![GitHub](https://img.shields.io/github/stars/NVlabs/FB-BEV)](https://github.com/NVlabs/FB-BEV) | [![arXiv](https://img.shields.io/badge/arXiv-2308.02236-b31b1b.svg)](https://arxiv.org/abs/2308.02236) | :heavy_minus_sign: |
| Learning from Noisy Data for Semi-Supervised 3D Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Boosting Long-Tailed Object Detection via Step-Wise Learning on Smooth-Tail Data | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.12833-b31b1b.svg)](https://arxiv.org/abs/2305.12833) | :heavy_minus_sign: |
| Objects do not Disappear: Video Object Detection by Single-Frame Object Location Anticipation | [![GitHub](https://img.shields.io/github/stars/L-KID/Video-object-detection-by-location-anticipation)](https://github.com/L-KID/Video-object-detection-by-location-anticipation) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04770-b31b1b.svg)](https://arxiv.org/abs/2308.04770) | :heavy_minus_sign: |
| Unified Visual Relationship Detection with Vision and Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/google-research/scenic/tree/main/scenic/projects/univrd) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08998-b31b1b.svg)](https://arxiv.org/abs/2303.08998) | :heavy_minus_sign: |
| Universal Domain Adaptation via Compressive Attention Matching | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.11862-b31b1b.svg)](https://arxiv.org/abs/2304.11862) | :heavy_minus_sign: |
| Unsupervised Domain Adaptive Detection with Network Stability Analysis | [![GitHub](https://img.shields.io/github/stars/tiankongzhang/NSA)](https://github.com/tiankongzhang/NSA) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08182-b31b1b.svg)](https://arxiv.org/abs/2308.08182) | :heavy_minus_sign: |
| ImGeoNet: Image-Induced Geometry-Aware Voxel Representation for Multi-View 3D Object Detection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ttaoretw.github.io/imgeonet/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09098-b31b1b.svg)](https://arxiv.org/abs/2308.09098) | :heavy_minus_sign: |
| Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection | [![GitHub](https://img.shields.io/github/stars/Yinyf0804/WSOD-CBL)](https://github.com/Yinyf0804/WSOD-CBL) | [![arXiv](https://img.shields.io/badge/arXiv-2308.05991-b31b1b.svg)](https://arxiv.org/abs/2308.05991) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Synthesis

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Text-Driven Generative Domain Adaptation with Spectral Consistency Regularization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.11096-b31b1b.svg)](https://arxiv.org/abs/2308.11096) | :heavy_minus_sign: |
| Controllable Visual-Tactile Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://visual-tactile-synthesis.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/RuihanGao/visual-tactile-synthesis)](https://github.com/RuihanGao/visual-tactile-synthesis) | [![arXiv](https://img.shields.io/badge/arXiv-2305.03051-b31b1b.svg)](https://arxiv.org/abs/2305.03051) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TdwPfwsGX3I) |
| Editing Implicit Assumptions in Text-to-Image Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://time-diffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/bahjat-kawar/time-diffusion)](https://github.com/bahjat-kawar/time-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08084-b31b1b.svg)](https://arxiv.org/abs/2303.08084) | :heavy_minus_sign: |
| DINAR: Diffusion Inpainting of Neural Textures for One-Shot Human Avatars | [![GitHub](https://img.shields.io/github/stars/SamsungLabs/DINAR)](https://github.com/SamsungLabs/DINAR) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09375-b31b1b.svg)](https://arxiv.org/abs/2303.09375) | :heavy_minus_sign: |
| Smoothness Similarity Regularization for Few-Shot GAN Adaptation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09717-b31b1b.svg)](https://arxiv.org/abs/2308.09717) | :heavy_minus_sign: |
| HSR-Diff: Hyperspectral Image Super-Resolution via Conditional Diffusion Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.12085-b31b1b.svg)](https://arxiv.org/abs/2306.12085) | :heavy_minus_sign: |
| Long-Term Photometric Consistent Novel View Synthesis with Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yorkucvil.github.io/Photoconsistent-NVS/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.10700-b31b1b.svg)](https://arxiv.org/abs/2304.10700) | :heavy_minus_sign: |
| AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.10438-b31b1b.svg)](https://arxiv.org/abs/2309.10438) | :heavy_minus_sign: |
| GaFET: Learning Geometry-Aware Facial Expression Translation from in-the-Wild Images | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.03413-b31b1b.svg)](https://arxiv.org/abs/2308.03413) | :heavy_minus_sign: |
| Collecting the Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures | [![GitHub](https://img.shields.io/github/stars/NannanLi999/pt_square)](https://github.com/NannanLi999/pt_square) | [![arXiv](https://img.shields.io/badge/arXiv-2210.01887-b31b1b.svg)](https://arxiv.org/abs/2210.01887) | :heavy_minus_sign: |
| Multi-Directional Subspace Editing in Style-Space | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chennaveh.github.io/MDSE/) <br /> [![GitHub](https://img.shields.io/github/stars/chennaveh/MDSE)](https://github.com/chennaveh/MDSE) | [![arXiv](https://img.shields.io/badge/arXiv-2211.11825-b31b1b.svg)](https://arxiv.org/abs/2211.11825) | :heavy_minus_sign: |
| HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stelabou.github.io/hyperreenact.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/StelaBou/HyperReenact)](https://github.com/StelaBou/HyperReenact) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10797-b31b1b.svg)](https://arxiv.org/abs/2307.10797) | :heavy_minus_sign: |
| Generating Realistic Images from in-the-Wild Sounds | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.02405-b31b1b.svg)](https://arxiv.org/abs/2309.02405) | :heavy_minus_sign: |
| CC3D: Layout-Conditioned Generation of Compositional 3D Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sherwinbahmani.github.io/cc3d/) <br /> [![GitHub](https://img.shields.io/github/stars/sherwinbahmani/cc3d)](https://github.com/sherwinbahmani/cc3d) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12074-b31b1b.svg)](https://arxiv.org/abs/2303.12074) | :heavy_minus_sign: |
| UMFuse: Unified Multi View Fusion for Human Editing Applications | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.10157-b31b1b.svg)](https://arxiv.org/abs/2211.10157) | :heavy_minus_sign: |
| Evaluating Data Attribution for Text-to-Image Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://peterwang512.github.io/GenDataAttribution/) <br /> [![GitHub](https://img.shields.io/github/stars/PeterWang512/GenDataAttribution)](https://github.com/PeterWang512/GenDataAttribution) | [![arXiv](https://img.shields.io/badge/arXiv-2306.09345-b31b1b.svg)](https://arxiv.org/abs/2306.09345) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=iO6fiSyyv40) |
| Neural Characteristic Function Learning for Conditional Image Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| WaveIPT: Joint Attention and Flow Alignment in the Wavelet Domain for Pose Transfer | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| LayoutDiffusion: Improving Graphic Layout Generation by Discrete Diffusion Probabilistic Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://github.com/microsoft/LayoutGeneration/tree/main/LayoutDiffusion) <br /> [![GitHub](https://img.shields.io/github/stars/microsoft/LayoutGeneration)](https://github.com/microsoft/LayoutGeneration) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11589-b31b1b.svg)](https://arxiv.org/abs/2303.11589) | :heavy_minus_sign: |
| Human-Inspired Facial Sketch Synthesis with Dynamic Adaptation | [![GitHub](https://img.shields.io/github/stars/AiArt-HDU/HIDA)](https://github.com/AiArt-HDU/HIDA) | [![arXiv](https://img.shields.io/badge/arXiv-2309.00216-b31b1b.svg)](https://arxiv.org/abs/2309.00216) | :heavy_minus_sign: |
| Conceptual and Hierarchical Latent Space Decomposition for Face Editing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.10554-b31b1b.svg)](https://arxiv.org/abs/2308.10554) | :heavy_minus_sign: |
| BallGAN: 3D-Aware Image Synthesis with a Spherical Background | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://minjung-s.github.io/ballgan) <br /> [![GitHub](https://img.shields.io/github/stars/minjung-s/BallGAN)](https://github.com/minjung-s/BallGAN) | [![arXiv](https://img.shields.io/badge/arXiv-2301.09091-b31b1b.svg)](https://arxiv.org/abs/2301.09091) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=RUIWWMiomuY) |
| End-to-End Diffusion Latent Optimization Improves Classifier Guidance | [![GitHub](https://img.shields.io/github/stars/salesforce/DOODL)](https://github.com/salesforce/DOODL) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13703-b31b1b.svg)](https://arxiv.org/abs/2303.13703) | :heavy_minus_sign: |
| Deep Geometrized Cartoon Line Inbetweening | [![GitHub](https://img.shields.io/github/stars/lisiyao21/AnimeInbet)](https://github.com/lisiyao21/AnimeInbet) | :heavy_minus_sign: | :heavy_minus_sign: |
| UnitedHuman: Harnessing Multi-Source Data for High-Resolution Human Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://unitedhuman.github.io/) | :heavy_minus_sign: | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pdsfUYFDLSw) |
| Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08996-b31b1b.svg)](https://arxiv.org/abs/2307.08996) | :heavy_minus_sign: |
| SVDiff: Compact Parameter Space for Diffusion Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/mkshing/svdiff-pytorch)](https://github.com/mkshing/svdiff-pytorch) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11305-b31b1b.svg)](https://arxiv.org/abs/2303.11305) | :heavy_minus_sign: |
| MI-GAN: A Simple Baseline for Image Inpainting on Mobile Devices | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Structure and Content-Guided Video Synthesis with Diffusion Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://research.runwayml.com/gen1) | [![arXiv](https://img.shields.io/badge/arXiv-2302.03011-b31b1b.svg)](https://arxiv.org/abs/2302.03011) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Y2_JmgzTeeo) |
| Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yuxinn-j.github.io/projects/Scenimefy.html) <br /> [![GitHub](https://img.shields.io/github/stars/Yuxinn-J/Scenimefy)](https://github.com/Yuxinn-J/Scenimefy) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/YuxinJ/Scenimefy) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12968-b31b1b.svg)](https://arxiv.org/abs/2308.12968) | :heavy_minus_sign: |
| Efficient-VQGAN: Towards High-Resolution Image Generation with Efficient Vision Transformers | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| A Latent Space of Stochastic Diffusion Models for Zero-Shot Image Editing and Guidance | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Generative Multiplane Neural Radiance for 3D-Aware Image Generation | [![GitHub](https://img.shields.io/github/stars/VIROBO-15/GMNR)](https://github.com/VIROBO-15/GMNR) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01172-b31b1b.svg)](https://arxiv.org/abs/2304.01172) | :heavy_minus_sign: |
| Parallax-Tolerant Unsupervised Deep Image Stitching | [![GitHub](https://img.shields.io/github/stars/nie-lang/UDIS2)](https://github.com/nie-lang/UDIS2) | [![arXiv](https://img.shields.io/badge/arXiv-2302.08207-b31b1b.svg)](https://arxiv.org/abs/2302.08207) | :heavy_minus_sign: |
| GAIT: Generating Aesthetic Indoor Tours with Deep Reinforcement Learning | [![GitHub](https://img.shields.io/github/stars/desaixie/gait)](https://github.com/desaixie/gait) | :heavy_minus_sign: | :heavy_minus_sign: |
| EverLight: Indoor-Outdoor Editable HDR Lighting Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/everlight/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.13207-b31b1b.svg)](https://arxiv.org/abs/2304.13207) | :heavy_minus_sign: |
| Prompt Tuning Inversion for Text-Driven Image Editing using Diffusion Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.04441-b31b1b.svg)](https://arxiv.org/abs/2305.04441) | :heavy_minus_sign: |
| Efficient Diffusion Training via Min-SNR Weighting Strategy | [![GitHub](https://img.shields.io/github/stars/TiankaiHang/Min-SNR-Diffusion-Training)](https://github.com/TiankaiHang/Min-SNR-Diffusion-Training) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09556-b31b1b.svg)](https://arxiv.org/abs/2303.09556) | :heavy_minus_sign: |
| BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion | [![GitHub](https://img.shields.io/github/stars/showlab/BoxDiff)](https://github.com/showlab/BoxDiff) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10816-b31b1b.svg)](https://arxiv.org/abs/2307.10816) | :heavy_minus_sign: |
| Improving Sample Quality of Diffusion Models using Self-Attention Guidance | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ku-cvlab.github.io/Self-Attention-Guidance/) <br /> [![GitHub](https://img.shields.io/github/stars/KU-CVLAB/Self-Attention-Guidance)](https://github.com/KU-CVLAB/Self-Attention-Guidance) | [![arXiv](https://img.shields.io/badge/arXiv-2210.00939-b31b1b.svg)](https://arxiv.org/abs/2210.00939) | :heavy_minus_sign: |
| Not All Steps are Created Equal: Selective Diffusion Distillation for Image Manipulation | [![GitHub](https://img.shields.io/github/stars/EnVision-Research/Selective-Diffusion-Distillation)](https://github.com/EnVision-Research/Selective-Diffusion-Distillation) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08448-b31b1b.svg)](https://arxiv.org/abs/2307.08448) | :heavy_minus_sign: |
| Deep Image Harmonization with Learnable Augmentation | [![GitHub](https://img.shields.io/github/stars/bcmi/SycoNet-Adaptive-Image-Harmonization)](https://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization) | [![arXiv](https://img.shields.io/badge/arXiv-2308.00376-b31b1b.svg)](https://arxiv.org/abs/2308.00376) | :heavy_minus_sign: |
| Out-of-Domain GAN Inversion via Invertibility Decomposition for Photo-Realistic Human Face Manipulation | [![GitHub](https://img.shields.io/github/stars/AbnerVictor/OOD-GAN-inversion)](https://github.com/AbnerVictor/OOD-GAN-inversion) | [![arXiv](https://img.shields.io/badge/arXiv-2212.09262-b31b1b.svg)](https://arxiv.org/abs/2212.09262) | :heavy_minus_sign: |
| Bidirectionally Deformable Motion Modulation for Video-based Human Pose Transfer | [![GitHub](https://img.shields.io/github/stars/rocketappslab/bdmm)](https://github.com/rocketappslab/bdmm) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07754-b31b1b.svg)](https://arxiv.org/abs/2307.07754) | :heavy_minus_sign: |
| Size does Matter: Size-Aware Virtual Try-On via Clothing-Oriented Transformation Try-On Network | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| VidStyleODE: Disentangled Video Editing via StyleGAN and NeuralODEs | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cyberiada.github.io/VidStyleODE/) <br /> [![GitHub](https://img.shields.io/github/stars/MoayedHajiAli/VidStyleODE-official)](https://github.com/MoayedHajiAli/VidStyleODE-official) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06020-b31b1b.svg)](https://arxiv.org/abs/2304.06020) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Cfh-mgr1isc) |
| Learning Global-Aware Kernel for Image Harmonization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2305.11676-b31b1b.svg)](https://arxiv.org/abs/2305.11676) | :heavy_minus_sign: |
| Expressive Text-to-Image Generation with Rich Text | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://rich-text-to-image.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/SongweiGe/rich-text-to-image)](https://github.com/SongweiGe/rich-text-to-image) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/songweig/rich-text-to-image) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06720-b31b1b.svg)](https://arxiv.org/abs/2304.06720) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ihDbAUh0LXk) |
| A Large-Scale Outdoor Multi-Modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://ommo.luchongshan.com/) <br /> [![GitHub](https://img.shields.io/github/stars/luchongshan/OMMO)](https://github.com/luchongshan/OMMO) | [![arXiv](https://img.shields.io/badge/arXiv-2301.06782-b31b1b.svg)](https://arxiv.org/abs/2301.06782) | [![Loom](https://a11ybadges.com/badge?logo=loom)](https://www.loom.com/share/7b9ed35bfb3649eda051398d3a51cda7) |
| Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://fictionarry.github.io/ER-NeRF/) <br /> [![GitHub](https://img.shields.io/github/stars/Fictionarry/ER-NeRF)](https://github.com/Fictionarry/ER-NeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2307.09323-b31b1b.svg)](https://arxiv.org/abs/2307.09323) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Gc2d3Z8MMuI) |
| Perceptual Artifacts Localization for Image Synthesis Tasks | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://owenzlz.github.io/PAL4VST/) <br /> [![GitHub](https://img.shields.io/github/stars/owenzlz/PAL4VST)](https://github.com/owenzlz/PAL4VST) | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pmh9960.github.io/research/GCDP/) <br /> [![GitHub](https://img.shields.io/github/stars/pmh9960/GCDP)](https://github.com/pmh9960/GCDP) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08157-b31b1b.svg)](https://arxiv.org/abs/2308.08157) | :heavy_minus_sign: |
| StylerDALLE: Language-Guided Style Transfer using a Vector-Quantized Tokenizer of a Large-Scale Generative Model | [![GitHub](https://img.shields.io/github/stars/zipengxuc/StylerDALLE)](https://github.com/zipengxuc/StylerDALLE) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09268-b31b1b.svg)](https://arxiv.org/abs/2303.09268) | :heavy_minus_sign: |
| Shortcut-V2V: Compression Framework for Video-to-Video Translation based on Temporal Redundancy Reduction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shortcut-v2v.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08011-b31b1b.svg)](https://arxiv.org/abs/2308.08011) | :heavy_minus_sign: |
| Tune-a-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tuneavideo.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/showlab/Tune-A-Video)](https://github.com/showlab/Tune-A-Video) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/Tune-A-Video-library/Tune-A-Video-Training-UI) | [![arXiv](https://img.shields.io/badge/arXiv-2212.11565-b31b1b.svg)](https://arxiv.org/abs/2212.11565) | :heavy_minus_sign: |
| BlendFace: Re-Designing Identity Encoders for Face-Swapping | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mapooon.github.io/BlendFacePage/) <br /> [![GitHub](https://img.shields.io/github/stars/mapooon/BlendFace)](https://github.com/mapooon/BlendFace) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10854-b31b1b.svg)](https://arxiv.org/abs/2307.10854) | :heavy_minus_sign: |
| Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zxyin.github.io/TH-PAD/) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04248-b31b1b.svg)](https://arxiv.org/abs/2212.04248) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=CrLXg7Cq8w8) |
| LinkGAN: Linking GAN Latents to Pixels for Controllable Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhujiapeng.github.io/linkgan/) <br /> [![GitHub](https://img.shields.io/github/stars/zhujiapeng/linkgan)](https://github.com/zhujiapeng/linkgan) | [![arXiv](https://img.shields.io/badge/arXiv-2301.04604-b31b1b.svg)](https://arxiv.org/abs/2301.04604) | :heavy_minus_sign: |
| Open-Vocabulary Object Segmentation with Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lipurple.github.io/Grounded_Diffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/Lipurple/Grounded-Diffusion)](https://github.com/Lipurple/Grounded-Diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2301.05221-b31b1b.svg)](https://arxiv.org/abs/2301.05221) | :heavy_minus_sign: |
| StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.07863-b31b1b.svg)](https://arxiv.org/abs/2308.07863) | :heavy_minus_sign: |
| ToonTalker: Cross-Domain Face Reenactment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://opentalker.github.io/ToonTalker/) <br /> [![GitHub](https://img.shields.io/github/stars/OpenTalker/ToonTalker)](https://github.com/OpenTalker/ToonTalker) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12866-b31b1b.svg)](https://arxiv.org/abs/2308.12866) | :heavy_minus_sign: |
| Dense Text-to-Image Generation with Attention Modulation | [![GitHub](https://img.shields.io/github/stars/naver-ai/DenseDiffusion)](https://github.com/naver-ai/DenseDiffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12964-b31b1b.svg)](https://arxiv.org/abs/2308.12964) | :heavy_minus_sign: |
| Householder Projector for Unsupervised Latent Semantics Discovery | [![GitHub](https://img.shields.io/github/stars/KingJamesSong/HouseholderGAN)](https://github.com/KingJamesSong/HouseholderGAN) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08012-b31b1b.svg)](https://arxiv.org/abs/2307.08012) | :heavy_minus_sign: |
| Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation | [![GitHub](https://img.shields.io/github/stars/bcmi/Image-Harmonization-Dataset-ccHarmony)](https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony) | [![arXiv](https://img.shields.io/badge/arXiv-2308.00356-b31b1b.svg)](https://arxiv.org/abs/2308.00356) | :heavy_minus_sign: |
| One-Shot Generative Domain Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://genforce.github.io/genda/) <br /> [![GitHub](https://img.shields.io/github/stars/genforce/genda)](https://github.com/genforce/genda) | [![arXiv](https://img.shields.io/badge/arXiv-2111.09876-b31b1b.svg)](https://arxiv.org/abs/2111.09876) | :heavy_minus_sign: |
| Hashing Neural Video Decomposition with Multiplicative Residuals in Space-Time | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lightbulb12294.github.io/hashing-nvd/) | :heavy_minus_sign: | :heavy_minus_sign: |
| Versatile Diffusion: Text, Images and Variations All in One Diffusion Model | [![GitHub](https://img.shields.io/github/stars/SHI-Labs/Versatile-Diffusion)](https://github.com/SHI-Labs/Versatile-Diffusion) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/shi-labs/Versatile-Diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2211.08332-b31b1b.svg)](https://arxiv.org/abs/2211.08332) | :heavy_minus_sign: |
| Harnessing the Spatial-Temporal Attention of Diffusion Models for High-Fidelity Text-to-Image Synthesis | [![GitHub](https://img.shields.io/github/stars/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn)](https://github.com/UCSB-NLP-Chang/Diffusion-SpaceTime-Attn) | [![arXiv](https://img.shields.io/badge/arXiv-2304.03869-b31b1b.svg)](https://arxiv.org/abs/2304.03869) | :heavy_minus_sign: |
| FreeDoM: Training-Free Energy-Guided Conditional Diffusion Model | [![GitHub](https://img.shields.io/github/stars/vvictoryuki/FreeDoM)](https://github.com/vvictoryuki/FreeDoM) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09833-b31b1b.svg)](https://arxiv.org/abs/2303.09833) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision and Audio

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Sound Source Localization is All About Cross-Modal Alignment | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.10724-b31b1b.svg)](https://arxiv.org/abs/2309.10724) | :heavy_minus_sign: |
| Class-Incremental Grouping Network for Continual Audio-Visual Learning | [![GitHub](https://img.shields.io/github/stars/stoneMo/CIGN)](https://github.com/stoneMo/CIGN) | [![arXiv](https://img.shields.io/badge/arXiv-2309.05281-b31b1b.svg)](https://arxiv.org/abs/2309.05281) | :heavy_minus_sign: |
| Audio-Visual Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/weiguoPian/AV-CIL_ICCV2023)](https://github.com/weiguoPian/AV-CIL_ICCV2023) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11073-b31b1b.svg)](https://arxiv.org/abs/2308.11073) | :heavy_minus_sign: |
| DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-Guided Speaker Embedding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.07787-b31b1b.svg)](https://arxiv.org/abs/2308.07787) | :heavy_minus_sign: |
| The Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ku-vai.github.io/TPoS/) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04509-b31b1b.svg)](https://arxiv.org/abs/2309.04509) | :heavy_minus_sign: |
| SIDGAN: High-Resolution Dubbed Video Generation via Shift-Invariant Learning | :heavy_minus_sign: | [![Amazon Science](https://img.shields.io/badge/amazon-science-FE9901.svg)](https://www.amazon.science/publications/sidgan-high-resolution-dubbed-video-generation-via-shift-invariant-learning) | :heavy_minus_sign: |
| On the Audio-Visual Synchronization for Lip-to-Speech Synthesis | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.00502-b31b1b.svg)](https://arxiv.org/abs/2303.00502) | :heavy_minus_sign: |
| Be Everywhere - Hear Everything (BEE): Audio Scene Reconstruction by Sparse Audio-Visual Samples | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Dense 2D-3D Indoor Prediction with Sound via Aligned Cross-Modal Distillation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.11081-b31b1b.svg)](https://arxiv.org/abs/2309.11081) | :heavy_minus_sign: |
| Hyperbolic Audio-Visual Zero-Shot Learning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.12558-b31b1b.svg)](https://arxiv.org/abs/2308.12558) | :heavy_minus_sign: |
| AdVerb: Visually Guided Audio Dereverberation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.12370-b31b1b.svg)](https://arxiv.org/abs/2308.12370) | :heavy_minus_sign: |
| Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ificl.github.io/SLfM/) <br /> [![GitHub](https://img.shields.io/github/stars/IFICL/SLfM)](https://github.com/IFICL/SLfM) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11329-b31b1b.svg)](https://arxiv.org/abs/2303.11329) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition, Segmentation, and Shape Analysis

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Segment Anything | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://segment-anything.com/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/segment-anything)](https://github.com/facebookresearch/segment-anything) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02643-b31b1b.svg)](https://arxiv.org/abs/2304.02643) | :heavy_minus_sign: |
| Shape Analysis of Euclidean Curves under Frenet-Serret Framework | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Unmasking Anomalies in Road-Scene Segmentation | [![GitHub](https://img.shields.io/github/stars/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation)](https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation) <br /> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1iMF5lWj3J8zlIJFkekXC3ipQo2semJfL?usp=sharing) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13316-b31b1b.svg)](https://arxiv.org/abs/2307.13316) | :heavy_minus_sign: |
| High Quality Entity Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](http://luqi.info/entityv2.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/qqlu/Entity)](https://github.com/qqlu/Entity) | [![arXiv](https://img.shields.io/badge/arXiv-2211.05776-b31b1b.svg)](https://arxiv.org/abs/2211.05776) | :heavy_minus_sign: |
| Towards Open-Vocabulary Video Instance Segmentation | [![GitHub](https://img.shields.io/github/stars/haochenheheda/LVVIS)](https://github.com/haochenheheda/LVVIS) | [![arXiv](https://img.shields.io/badge/arXiv-2304.01715-b31b1b.svg)](https://arxiv.org/abs/2304.01715) | :heavy_minus_sign: |
| Beyond One-to-One: Rethinking the Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/toggle1995/RIS-DMMI)](https://github.com/toggle1995/RIS-DMMI) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13853-b31b1b.svg)](https://arxiv.org/abs/2308.13853) | :heavy_minus_sign: |
| Multiple Instance Learning Framework with Masked Hard Instance Mining for whole Slide Image Classification | [![GitHub](https://img.shields.io/github/stars/DearCaat/MHIM-MIL)](https://github.com/DearCaat/MHIM-MIL) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15254-b31b1b.svg)](https://arxiv.org/abs/2307.15254) | :heavy_minus_sign: |
| Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning | [![GitHub](https://img.shields.io/github/stars/bair-climate-initiative/scale-mae)](https://github.com/bair-climate-initiative/scale-mae) | [![arXiv](https://img.shields.io/badge/arXiv-2212.14532-b31b1b.svg)](https://arxiv.org/abs/2212.14532) | :heavy_minus_sign: |
| Progressive Spatio-Temporal Prototype Matching for Text-Video Retrieval | [![GitHub](https://img.shields.io/github/stars/IMCCretrieval/ProST)](https://github.com/IMCCretrieval/ProST) | :heavy_minus_sign: | :heavy_minus_sign: |
| Towards Deeply Unified Depth-Aware Panoptic Segmentation with Bi-Directional Guidance Learning | [![GitHub](https://img.shields.io/github/stars/jwh97nn/DeepDPS)](https://github.com/jwh97nn/DeepDPS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14786-b31b1b.svg)](https://arxiv.org/abs/2307.14786) | :heavy_minus_sign: |
| LogicSeg: Parsing Visual Semantics with Neural Logic Learning and Reasoning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.13556-b31b1b.svg)](https://arxiv.org/abs/2309.13556) | :heavy_minus_sign: |
| ASIC: Aligning Sparse in-the-Wild Image Collections | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kampta.github.io/asic/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16201-b31b1b.svg)](https://arxiv.org/abs/2303.16201) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fLjkkMriuoY) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Generative AI

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| CLIPascene: Scene Sketching with Different Types and Levels of Abstraction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://clipascene.github.io/CLIPascene/) <br /> [![GitHub](https://img.shields.io/github/stars/yael-vinker/SceneSketch)](https://github.com/yael-vinker/SceneSketch) | [![arXiv](https://img.shields.io/badge/arXiv-2211.17256-b31b1b.svg)](https://arxiv.org/abs/2211.17256) | :heavy_minus_sign: |
| LD-ZNet: A Latent Diffusion Approach for Text-based Image Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://koutilya-pnvr.github.io/LD-ZNet/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12343-b31b1b.svg)](https://arxiv.org/abs/2303.12343) | :heavy_minus_sign: |
| TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion Models | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| NeuRBF: A Neural Fields Representation with Adaptive Radial Basis Functions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://oppo-us-research.github.io/NeuRBF-website/) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://cse.buffalo.edu/~jsyuan/papers/2023/ICCV2023_zhang.pdf) | :heavy_minus_sign: |
| Scalable Diffusion Models with Transformers | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.wpeebles.com/DiT) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/DiT)](https://github.com/facebookresearch/DiT) | [![arXiv](https://img.shields.io/badge/arXiv-2212.09748-b31b1b.svg)](https://arxiv.org/abs/2212.09748) | :heavy_minus_sign: |
| Texture Generation on 3D Meshes with Point-UV Diffusion | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cvmi-lab.github.io/Point-UV-Diffusion/) <br /> [![GitHub](https://img.shields.io/github/stars/CVMI-Lab/Point-UV-Diffusion)](https://github.com/CVMI-Lab/Point-UV-Diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10490-b31b1b.svg)](https://arxiv.org/abs/2308.10490) | :heavy_minus_sign: |
| Generative Novel View Synthesis with 3D-Aware Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nvlabs.github.io/genvs/) <br /> [![GitHub](https://img.shields.io/github/stars/NVlabs/genvs)](https://github.com/NVlabs/genvs) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02602-b31b1b.svg)](https://arxiv.org/abs/2304.02602) | :heavy_minus_sign: |
| DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning | [![GitHub](https://img.shields.io/github/stars/mkshing/DiffFit-pytorch)](https://github.com/mkshing/DiffFit-pytorch) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06648-b31b1b.svg)](https://arxiv.org/abs/2304.06648) | :heavy_minus_sign: |
| VQ3D: Learning a 3D-Aware Generative Model on ImageNet | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kylesargent.github.io/vq3d) | [![arXiv](https://img.shields.io/badge/arXiv-2302.06833-b31b1b.svg)](https://arxiv.org/abs/2302.06833) | :heavy_minus_sign: |
| Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://g3956.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/EnVision-Research/Ref-NeuS)](https://github.com/EnVision-Research/Ref-NeuS) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10840-b31b1b.svg)](https://arxiv.org/abs/2303.10840) | :heavy_minus_sign: |
| A Complete Recipe for Diffusion Generative Models | [![GitHub](https://img.shields.io/github/stars/mandt-lab/PSLD)](https://github.com/mandt-lab/PSLD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.01748-b31b1b.svg)](https://arxiv.org/abs/2303.01748) | :heavy_minus_sign: |
| MMVP: Motion-Matrix-based Video Prediction | [![GitHub](https://img.shields.io/github/stars/Kay1794/MMVP-motion-matrix-based-video-prediction)](https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction) | [![arXiv](https://img.shields.io/badge/arXiv-2308.16154-b31b1b.svg)](https://arxiv.org/abs/2308.16154) | :heavy_minus_sign: |
| Simulating Fluids in Real-World Still Images | [![GitHub](https://img.shields.io/github/stars/simon3dv/SLR-SFS)](https://github.com/simon3dv/SLR-SFS) | [![arXiv](https://img.shields.io/badge/arXiv-2204.11335-b31b1b.svg)](https://arxiv.org/abs/2204.11335) | :heavy_minus_sign: |
| FateZero: Fusing Attentions for Zero-Shot Text-based Video Editing | [![GitHub](https://img.shields.io/github/stars/ChenyangQiQi/FateZero)](https://github.com/ChenyangQiQi/FateZero) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09535-b31b1b.svg)](https://arxiv.org/abs/2303.09535) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Humans, 3D Modeling, and Driving

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Text2Room: Extracting Textured 3D Meshes from 2D Text-to-Image Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lukashoel.github.io/text-to-room/) <br /> [![GitHub](https://img.shields.io/github/stars/lukasHoel/text2room)](https://github.com/lukasHoel/text2room) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hollein_Text2Room_Extracting_Textured_3D_Meshes_from_2D_Text-to-Image_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11989-b31b1b.svg)](https://arxiv.org/abs/2303.11989) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=fjRnFL91EZc) |
| LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses | [![GitHub](https://img.shields.io/github/stars/apple/ml-live-pose)](https://github.com/apple/ml-live-pose) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Stier_LivePose_Online_3D_Reconstruction_from_Monocular_Video_with_Dynamic_Camera_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.00054-b31b1b.svg)](https://arxiv.org/abs/2304.00054) | :heavy_minus_sign: |
| NDDepth: Normal-Distance Assisted Monocular Depth Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_NDDepth_Normal-Distance_Assisted_Monocular_Depth_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.10592-b31b1b.svg)](https://arxiv.org/abs/2309.10592) | :heavy_minus_sign: |
| LATR: 3D Lane Detection from Monocular Images with Transformer | [![GitHub](https://img.shields.io/github/stars/JMoonr/LATR)](https://github.com/JMoonr/LATR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LATR_3D_Lane_Detection_from_Monocular_Images_with_Transformer_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04583-b31b1b.svg)](https://arxiv.org/abs/2308.04583) | :heavy_minus_sign: |
| DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/OpenDriveLab/DriveAdapter)](https://github.com/OpenDriveLab/DriveAdapter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jia_DriveAdapter_Breaking_the_Coupling_Barrier_of_Perception_and_Planning_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.00398-b31b1b.svg)](https://arxiv.org/abs/2308.00398) | :heavy_minus_sign: |
| Dynamic Point Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sergeyprokudin.github.io/dpf/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Prokudin_Dynamic_Point_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02626-b31b1b.svg)](https://arxiv.org/abs/2304.02626) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=i-9eAgS8HEA) |
| Generalizing Neural Human Fitting to Unseen Poses with Articulated SE(3) Equivariance | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://arteq.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Generalizing_Neural_Human_Fitting_to_Unseen_Poses_With_Articulated_SE3_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10528-b31b1b.svg)](https://arxiv.org/abs/2304.10528) | :heavy_minus_sign: |
| Probabilistic Human Mesh Recovery in 3D Scenes from Egocentric Views | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sanweiliti.github.io/egohmr/egohmr.html) <br /> [![GitHub](https://img.shields.io/github/stars/sanweiliti/EgoHMR)](https://github.com/sanweiliti/EgoHMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Probabilistic_Human_Mesh_Recovery_in_3D_Scenes_from_Egocentric_Views_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.06024-b31b1b.svg)](https://arxiv.org/abs/2304.06024) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=K6m0BmfMG-E) |
| DECO: Dense Estimation of 3D Human-Scene Contact in the Wild | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://deco.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tripathi_DECO_Dense_Estimation_of_3D_Human-Scene_Contact_In_The_Wild_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image | [![GitHub](https://img.shields.io/github/stars/PengfeiRen96/DIR)](https://github.com/PengfeiRen96/DIR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Decoupled_Iterative_Refinement_Framework_for_Interacting_Hands_Reconstruction_from_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.02410-b31b1b.svg)](https://arxiv.org/abs/2302.02410) | :heavy_minus_sign: |
| Chasing Clouds: Differentiable Volumetric Rasterisation of Point Clouds as a Highly Efficient and Accurate Loss for Large-Scale Deformable 3D Registration | [![GitHub](https://img.shields.io/github/stars/mattiaspaul/ChasingClouds)](https://github.com/mattiaspaul/ChasingClouds) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Heinrich_Chasing_Clouds_Differentiable_Volumetric_Rasterisation_of_Point_Clouds_as_a_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Rehearsal-Free Domain Continual Face Anti-Spoofing: Generalize more and Forget Less | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Rehearsal-Free_Domain_Continual_Face_Anti-Spoofing_Generalize_More_and_Forget_Less_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09914-b31b1b.svg)](https://arxiv.org/abs/2303.09914) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level Vision and Theory

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| A 5-Point Minimal Solver for Event Camera Relative Motion Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mgaoling.github.io/eventail/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_A_5-Point_Minimal_Solver_for_Event_Camera_Relative_Motion_Estimation_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hyfGGzZQZh4) |
| General Planar Motion from a Pair of 3D Correspondences | [![GitHub](https://img.shields.io/github/stars//jdibenes/gpm)](https://github.com//jdibenes/gpm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dibene_General_Planar_Motion_from_a_Pair_of_3D_Correspondences_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Beyond the Pixel: A Photometrically Calibrated HDR Dataset for Luminance and Color Prediction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lvsn.github.io/beyondthepixel/) <br /> [![GitHub](https://img.shields.io/github/stars/lvsn/beyondthepixel)](https://github.com/lvsn/beyondthepixel) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bolduc_Beyond_the_Pixel_a_Photometrically_Calibrated_HDR_Dataset_for_Luminance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12372-b31b1b.svg)](https://arxiv.org/abs/2304.12372) | :heavy_minus_sign: |
| DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion | [![GitHub](https://img.shields.io/github/stars/Zhaozixiang1228/MMIF-DDFM)](https://github.com/Zhaozixiang1228/MMIF-DDFM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_DDFM_Denoising_Diffusion_Model_for_Multi-Modality_Image_Fusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06840-b31b1b.svg)](https://arxiv.org/abs/2303.06840) | :heavy_minus_sign: |
| Iterative Prompt Learning for Unsupervised Backlit Image Enhancement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhexinliang.github.io/CLIP_LIT_page/) <br /> [![GitHub](https://img.shields.io/github/stars/ZhexinLiang/CLIP-LIT)](https://github.com/ZhexinLiang/CLIP-LIT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Iterative_Prompt_Learning_for_Unsupervised_Backlit_Image_Enhancement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17569-b31b1b.svg)](https://arxiv.org/abs/2303.17569) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0qbkxNmkNWU) |
| Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/) <br /> [![GitHub](https://img.shields.io/github/stars/Red-Fairy/ZeroShotDayNightDA)](https://github.com/Red-Fairy/ZeroShotDayNightDA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Similarity_Min-Max_Zero-Shot_Day-Night_Domain_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.08779-b31b1b.svg)](https://arxiv.org/abs/2307.08779) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=_Urw6HBjzAk) |
| Multi-Interactive Feature Learning and a Full-Time Multi-Modality Benchmark for Image Fusion and Segmentation | [![GitHub](https://img.shields.io/github/stars/JinyuanLiu-CV/SegMiF)](https://github.com/JinyuanLiu-CV/SegMiF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Multi-interactive_Feature_Learning_and_a_Full-time_Multi-modality_Benchmark_for_Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02097-b31b1b.svg)](https://arxiv.org/abs/2308.02097) | :heavy_minus_sign: |
| Computational 3D Imaging with Position Sensors | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Klotz_Computational_3D_Imaging_with_Position_Sensors_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pL2puwXOY9c) |
| Passive Ultra-Wideband Single-Photon Imaging | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.dgp.toronto.edu/projects/ultra-wideband/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Passive_Ultra-Wideband_Single-Photon_Imaging_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Viewing Graph Solvability in Practice | [![GitHub](https://img.shields.io/github/stars/federica-arrigoni/finite-solvability)](https://github.com/federica-arrigoni/finite-solvability) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Arrigoni_Viewing_Graph_Solvability_in_Practice_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Minimal Solutions to Generalized Three-View Relative Pose Problem | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_Minimal_Solutions_to_Generalized_Three-View_Relative_Pose_Problem_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SoDaCam: Software-Defined Cameras via Single-Photon Imaging | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://wisionlab.com/project/sodacam/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Sundar_SoDaCam_Software-defined_Cameras_via_Single-Photon_Imaging_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.00066-b31b1b.svg)](https://arxiv.org/abs/2309.00066) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Navigation and Autonomous Driving

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Robust Monocular Depth Estimation under Challenging Conditions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://md4all.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/md4all/md4all)](https://github.com/md4all/md4all) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09711-b31b1b.svg)](https://arxiv.org/abs/2308.09711) | :heavy_minus_sign: |
| UMC: A Unified Bandwidth-Efficient and Multi-Resolution based Collaborative Perception Framework | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://tianhangwang.github.io/UMC/) <br /> [![GitHub](https://img.shields.io/github/stars/ispc-lab/UMC)](https://github.com/ispc-lab/UMC) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12400-b31b1b.svg)](https://arxiv.org/abs/2303.12400) | :heavy_minus_sign: |
| View Consistent Purification for Accurate Cross-View Localization | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shanwang-shan.github.io/PureACL-website/) <br /> [![GitHub](https://img.shields.io/github/stars/ShanWang-Shan/PureACL-website)](https://github.com/ShanWang-Shan/PureACL-website) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08110-b31b1b.svg)](https://arxiv.org/abs/2308.08110) | :heavy_minus_sign: |
| Semi-Supervised Semantics-Guided Adversarial Training for Robust Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/jrcblue/SSAT-for-Motion-Prediction)](https://github.com/jrcblue/SSAT-for-Motion-Prediction) | [![arXiv](https://img.shields.io/badge/arXiv-2205.14230-b31b1b.svg)](https://arxiv.org/abs/2205.14230) | :heavy_minus_sign: |
| NeRF-LOAM: Neural Implicit Representation for Large-Scale Incremental LiDAR Odometry and Mapping | [![GitHub](https://img.shields.io/github/stars/JunyuanDeng/NeRF-LOAM)](https://github.com/JunyuanDeng/NeRF-LOAM) | [![arXiv](https://img.shields.io/badge/arXiv-2303.10709-b31b1b.svg)](https://arxiv.org/abs/2303.10709) | :heavy_minus_sign: |
| MapPrior: Bird's-Eye View Map Layout Estimation with Generative Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mapprior.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/xiyuez2/MapPrior)](https://github.com/xiyuez2/MapPrior) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12963-b31b1b.svg)](https://arxiv.org/abs/2308.12963) | :heavy_minus_sign: |
| Hidden Biases of End-to-End Driving Models | [![GitHub](https://img.shields.io/github/stars/autonomousvision/carla_garage)](https://github.com/autonomousvision/carla_garage) | [![arXiv](https://img.shields.io/badge/arXiv-2306.07957-b31b1b.svg)](https://arxiv.org/abs/2306.07957) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ChrPW8RdqQU) |
| Search for or Navigate to? Dual Adaptive Thinking for Object Navigation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2208.00553-b31b1b.svg)](https://arxiv.org/abs/2208.00553) | :heavy_minus_sign: |
| BiFF: Bi-Level Future Fusion with Polyline-based Coordinate for Interactive Trajectory Prediction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2306.14161-b31b1b.svg)](https://arxiv.org/abs/2306.14161) | :heavy_minus_sign: |
| Towards Zero Domain Gap: A Comprehensive Study of Realistic LiDAR Simulation for Autonomy Testing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Clustering based Point Cloud Representation Learning for 3D Analysis | [![GitHub](https://img.shields.io/github/stars/FengZicai/Cluster3Dseg)](https://github.com/FengZicai/Cluster3Dseg) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14605-b31b1b.svg)](https://arxiv.org/abs/2307.14605) | :heavy_minus_sign: |
| ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kuis-ai.github.io/adapt/) <br /> [![GitHub](https://img.shields.io/github/stars/KUIS-AI/adapt)](https://github.com/KUIS-AI/adapt) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14187-b31b1b.svg)](https://arxiv.org/abs/2307.14187) | :heavy_minus_sign: |
| MV-DeepSDF: Implicit Modeling with Multi-Sweep Point Clouds for 3D Vehicle Reconstruction in Autonomous Driving | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Vision-and-Language Navigation from YouTube Videos | [![GitHub](https://img.shields.io/github/stars/JeremyLinky/YouTube-VLN)](https://github.com/JeremyLinky/YouTube-VLN) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11984-b31b1b.svg)](https://arxiv.org/abs/2307.11984) | :heavy_minus_sign: |
| TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.05985-b31b1b.svg)](https://arxiv.org/abs/2308.05985) | :heavy_minus_sign: |
| VAD: Vectorized Scene Representation for Efficient Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/hustvl/VAD)](https://github.com/hustvl/VAD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12077-b31b1b.svg)](https://arxiv.org/abs/2303.12077) | :heavy_minus_sign: |
| Traj-MAE: Masked Autoencoders for Trajectory Prediction | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.06697-b31b1b.svg)](https://arxiv.org/abs/2303.06697) | :heavy_minus_sign: |
| Sparse Point Guided 3D Lane Detection | [![GitHub](https://img.shields.io/github/stars/YaoChengTang/Sparse-Point-Guided-3D-Lane-Detection)](https://github.com/YaoChengTang/Sparse-Point-Guided-3D-Lane-Detection) | :heavy_minus_sign: | :heavy_minus_sign: |
| A Simple Vision Transformer for Weakly Semi-Supervised 3D Object Detection | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learn TAROT with MENTOR: A Meta-Learned Self-Supervised Approach for Trajectory Prediction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| FocalFormer3D: Focusing on Hard Instance for 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/NVlabs/FocalFormer3D)](https://github.com/NVlabs/FocalFormer3D) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04556-b31b1b.svg)](https://arxiv.org/abs/2308.04556) | :heavy_minus_sign: |
| Scene as Occupancy | [![GitHub](https://img.shields.io/github/stars/OpenDriveLab/OccNet)](https://github.com/OpenDriveLab/OccNet) | [![arXiv](https://img.shields.io/badge/arXiv-2306.02851-b31b1b.svg)](https://arxiv.org/abs/2306.02851) | :heavy_minus_sign: |
| Real-Time Neural Rasterization for Large Scenes | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| A Game of Bundle Adjustment - Learning Efficient Convergence | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.13270-b31b1b.svg)](https://arxiv.org/abs/2308.13270) | :heavy_minus_sign: |
| Efficient Transformer-based 3D Object Detection with Dynamic Token Halting | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2303.05078-b31b1b.svg)](https://arxiv.org/abs/2303.05078) | :heavy_minus_sign: |
| RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration | [![GitHub](https://img.shields.io/github/stars/IRMVLab/RegFormer)](https://github.com/IRMVLab/RegFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12384-b31b1b.svg)](https://arxiv.org/abs/2303.12384) | :heavy_minus_sign: |
| CASSPR: Cross Attention Single Scan Place Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.12542-b31b1b.svg)](https://arxiv.org/abs/2211.12542) | :heavy_minus_sign: |
| Recursive Video Lane Detection | [![GitHub](https://img.shields.io/github/stars/dongkwonjin/RVLD)](https://github.com/dongkwonjin/RVLD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11106-b31b1b.svg)](https://arxiv.org/abs/2308.11106) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Z0FaOqVrN5w) |
| Parametric Depth based Feature Representation Learning for Object Detection and Segmentation in Bird's-Eye View | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.04106-b31b1b.svg)](https://arxiv.org/abs/2307.04106) | :heavy_minus_sign: |
| SHIFT3D: Synthesizing Hard Inputs for Tricking 3D Detectors | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.05810-b31b1b.svg)](https://arxiv.org/abs/2309.05810) | :heavy_minus_sign: |
| Bootstrap Motion Forecasting With Self-Consistent Constraints | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2204.05859-b31b1b.svg)](https://arxiv.org/abs/2204.05859) | :heavy_minus_sign: |
| Towards Viewpoint Robustness in Bird's Eye View Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://nvlabs.github.io/viewpoint-robustness/) | [![arXiv](https://img.shields.io/badge/arXiv-2309.05192-b31b1b.svg)](https://arxiv.org/abs/2309.05192) | :heavy_minus_sign: |
| R-Pred: Two-Stage Motion Prediction via Tube-Query Attention-based Trajectory Refinement | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.08609-b31b1b.svg)](https://arxiv.org/abs/2211.08609) | :heavy_minus_sign: |
| INT2: Interactive Trajectory Prediction at Intersections | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://int2.cn/) <br /> [![GitHub](https://img.shields.io/github/stars/AIR-DISCOVER/INT2)](https://github.com/AIR-DISCOVER/INT2) | :heavy_minus_sign: | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KNkuakDvgVc) |
| MatrixVT: Efficient Multi-Camera to BEV Transformation for 3D Perception | [![GitHub](https://img.shields.io/github/stars/ZRandomize/MatrixVT)](https://github.com/ZRandomize/MatrixVT) | [![arXiv](https://img.shields.io/badge/arXiv-2211.10593-b31b1b.svg)](https://arxiv.org/abs/2211.10593) | :heavy_minus_sign: |
| Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding | [![GitHub](https://img.shields.io/github/stars/zaplm/DriverAttention)](https://github.com/zaplm/DriverAttention) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09706-b31b1b.svg)](https://arxiv.org/abs/2303.09706) | :heavy_minus_sign: |
| SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.13323-b31b1b.svg)](https://arxiv.org/abs/2308.13323) | :heavy_minus_sign: |
| MotionLM: Multi-Agent Motion Forecasting as Language Modeling | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Improving Online Lane Graph Extraction by Object-Lane Clustering | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.10947-b31b1b.svg)](https://arxiv.org/abs/2307.10947) | :heavy_minus_sign: |
| Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Self-Supervised Monocular Depth Estimation by Direction-Aware Cumulative Convolution Network | [![GitHub](https://img.shields.io/github/stars/wencheng256/DaCCN)](https://github.com/wencheng256/DaCCN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.05605-b31b1b.svg)](https://arxiv.org/abs/2308.05605) | :heavy_minus_sign: |
| Ordered Atomic Activity for Fine-Grained Interactive Traffic Scenario Understanding | :heavy_minus_sign: | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://drive.google.com/file/d/1Jwzzr0puAWte5xa-xQwOAnpAXsBsSw7f/view) | :heavy_minus_sign: |
| DistillBEV: Boosting Multi-Camera 3D Object Detection with Cross-Modal Knowledge Distillation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.vis.xyz/pub/vtd/) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04422-b31b1b.svg)](https://arxiv.org/abs/2309.04422) | :heavy_minus_sign: |
| MV-Map: Offboard HD-Map Generation with Multi-View Consistency | [![GitHub](https://img.shields.io/github/stars/ZiYang-xie/MV-Map)](https://github.com/ZiYang-xie/MV-Map) | [![arXiv](https://img.shields.io/badge/arXiv-2305.08851-b31b1b.svg)](https://arxiv.org/abs/2305.08851) | :heavy_minus_sign: |
| Towards Universal LiDAR-based 3D Object Detection by Multi-Domain Knowledge Transfer | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Forecast-MAE: Self-Supervised Pre-Training for Motion Forecasting with Masked Autoencoders | [![GitHub](https://img.shields.io/github/stars/jchengai/forecast-mae)](https://github.com/jchengai/forecast-mae) | [![arXiv](https://img.shields.io/badge/arXiv-2308.09882-b31b1b.svg)](https://arxiv.org/abs/2308.09882) | :heavy_minus_sign: |
| UniFusion: Unified Multi-View Fusion Transformer for Spatial-Temporal Representation in Bird's-Eye-View | [![GitHub](https://img.shields.io/github/stars/cfzd/UniFusion)](https://github.com/cfzd/UniFusion) | [![arXiv](https://img.shields.io/badge/arXiv-2207.08536-b31b1b.svg)](https://arxiv.org/abs/2207.08536) | :heavy_minus_sign: |
| BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images | [![GitHub](https://img.shields.io/github/stars/zjuluolun/BEVPlace)](https://github.com/zjuluolun/BEVPlace) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14325-b31b1b.svg)](https://arxiv.org/abs/2302.14325) | :heavy_minus_sign: |
| CORE: Cooperative Reconstruction for Multi-Agent Perception | [![GitHub](https://img.shields.io/github/stars/zllxot/CORE)](https://github.com/zllxot/CORE) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11514-b31b1b.svg)](https://arxiv.org/abs/2307.11514) | :heavy_minus_sign: |
| MetaBEV: Solving Sensor Failures for 3D Detection and Map Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://chongjiange.github.io/metabev.html) <br /> [![GitHub](https://img.shields.io/github/stars/ChongjianGE/MetaBEV)](https://github.com/ChongjianGE/MetaBEV) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09801-b31b1b.svg)](https://arxiv.org/abs/2304.09801) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TiEQpYq77Xo) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D from a Single Image and Shape-from-X

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Aggregating Feature Point Cloud for Depth Completion | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Coordinate Transformer: Achieving Single-Stage Multi-Person Mesh Recovery from Videos | [![GitHub](https://img.shields.io/github/stars/Li-Hao-yuan/CoordFormer)](https://github.com/Li-Hao-yuan/CoordFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10334-b31b1b.svg)](https://arxiv.org/abs/2308.10334) | :heavy_minus_sign: |
| MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.14336-b31b1b.svg)](https://arxiv.org/abs/2307.14336) | :heavy_minus_sign: |
| SlaBins: Fisheye Depth Estimation using Slanted Bins on Road Environments | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Creative Birds: Self-Supervised Single-View 3D Style Transfer | [![GitHub](https://img.shields.io/github/stars/wrk226/creative_birds)](https://github.com/wrk226/creative_birds) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14127-b31b1b.svg)](https://arxiv.org/abs/2307.14127) | :heavy_minus_sign: |
| Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vlislab22.github.io/DOT/) <br /> [![GitHub](https://img.shields.io/github/stars/hbai98/DOT)](https://github.com/hbai98/DOT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15333-b31b1b.svg)](https://arxiv.org/abs/2307.15333) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=i9MnoFhH8Ec) |
| CORE: Co-Planarity Regularized Monocular Geometry Estimation with Weak Supervision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Relightify: Relightable 3D Faces from a Single Image via Diffusion Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://foivospar.github.io/Relightify/) | [![arXiv](https://img.shields.io/badge/arXiv-2305.06077-b31b1b.svg)](https://arxiv.org/abs/2305.06077) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=N5pSN4Pc0JM) |
| GLA-GCN: Global-Local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video | [![GitHub](https://img.shields.io/github/stars/bruceyo/GLA-GCN)](https://github.com/bruceyo/GLA-GCN) | [![arXiv](https://img.shields.io/badge/arXiv-2307.05853-b31b1b.svg)](https://arxiv.org/abs/2307.05853) | :heavy_minus_sign: |
| Calibrating Panoramic Depth Estimation for Practical Localization and Mapping | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.14005-b31b1b.svg)](https://arxiv.org/abs/2308.14005) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=KXz8IwrtJWg) |
| SimNP: Learning Self-Similarity Priors between Neural Points | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.03809-b31b1b.svg)](https://arxiv.org/abs/2309.03809) | :heavy_minus_sign: |
| AGG-Net: Attention Guided Gated-Convolutional Network for Depth Image Completion | [![GitHub](https://img.shields.io/github/stars/htx0601/AGG-Net)](https://github.com/htx0601/AGG-Net) | [![arXiv](https://img.shields.io/badge/arXiv-2309.01624-b31b1b.svg)](https://arxiv.org/abs/2309.01624) | :heavy_minus_sign: |
| Viewset Diffusion: (0-)Image-Conditioned 3D Generative Models from 2D Data | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://szymanowiczs.github.io/viewset-diffusion) <br /> [![GitHub](https://img.shields.io/github/stars/szymanowiczs/viewset-diffusion)](https://github.com/szymanowiczs/viewset-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2306.07881-b31b1b.svg)](https://arxiv.org/abs/2306.07881) | :heavy_minus_sign: |
| CVSformer: Cross-View Synthesis Transformer for Semantic Scene Completion | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.07938-b31b1b.svg)](https://arxiv.org/abs/2307.07938) | :heavy_minus_sign: |
| U-RED: Unsupervised 3D Shape Retrieval and Deformation for Partial Point Clouds | [![GitHub](https://img.shields.io/github/stars/ZhangCYG/U-RED)](https://github.com/ZhangCYG/U-RED) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06383-b31b1b.svg)](https://arxiv.org/abs/2308.06383) | :heavy_minus_sign: |
| Single Depth-Image 3D Reflection Symmetry and Shape Prediction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Self-Supervised Monocular Depth Estimation: Let's Talk About the Weather | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kieran514.github.io/Robust-Depth-Project/) <br /> [![GitHub](https://img.shields.io/github/stars/kieran514/robustdepth)](https://github.com/kieran514/robustdepth) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08357-b31b1b.svg)](https://arxiv.org/abs/2307.08357) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zGXzpJAWjcQ&t=3s) |
| Mesh2Tex: Generating Mesh Textures from Image Queries | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://alexeybokhovkin.github.io/mesh2tex/) | [![arXiv](https://img.shields.io/badge/arXiv-2304.05868-b31b1b.svg)](https://arxiv.org/abs/2304.05868) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=tY6pPHN5v9Q) |
| Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.02874-b31b1b.svg)](https://arxiv.org/abs/2308.02874) | :heavy_minus_sign: |
| Learning a Room with the Occ-SDF Hybrid: Signed Distance Function Mingled with Occupancy Aids Scene Representation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://shawlyu.github.io/Occ-SDF-Hybrid/) <br /> [![GitHub](https://img.shields.io/github/stars/shawLyu/Occ-SDF-Hybrid)](https://github.com/shawLyu/Occ-SDF-Hybrid) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09152-b31b1b.svg)](https://arxiv.org/abs/2303.09152) | :heavy_minus_sign: |
| Robust Geometry-Preserving Depth Estimation using Differentiable Rendering | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.09724-b31b1b.svg)](https://arxiv.org/abs/2309.09724) | :heavy_minus_sign: |
| FeatureNeRF: Learning Generalizable NeRFs by Distilling Foundation Models | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://jianglongye.com/featurenerf/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12786-b31b1b.svg)](https://arxiv.org/abs/2303.12786) | :heavy_minus_sign: |
| One-Shot Implicit Animatable Avatars with Model-based Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://huangyangyi.github.io/ELICIT/) <br /> [![GitHub](https://img.shields.io/github/stars/huangyangyi/ELICIT)](https://github.com/huangyangyi/ELICIT) | [![arXiv](https://img.shields.io/badge/arXiv-2212.02469-b31b1b.svg)](https://arxiv.org/abs/2212.02469) | :heavy_minus_sign: |
| VeRi3D: Generative Vertex-based Radiance Fields for 3D Controllable Human Image Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xdimlab.github.io/VeRi3d/) <br /> [![GitHub](https://img.shields.io/github/stars/XinyaChen21/Veri3d)](https://github.com/XinyaChen21/Veri3d) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04800-b31b1b.svg)](https://arxiv.org/abs/2309.04800) | :heavy_minus_sign: |
| Diffuse3D: Wide-Angle 3D Photography via Bilateral Diffusion | [![GitHub](https://img.shields.io/github/stars/yutaojiang1/Diffuse3D)](https://github.com/yutaojiang1/Diffuse3D) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://csyhquan.github.io/manuscript/23-iccv-Diffuse3D%20Wide-Angle%203D%20Photography%20via%20Bilateral%20Diffusion.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=5mL6AMEvPSQ) |
| AutoSynth: Learning to Generate 3D Training Data for Object Point Cloud Registration | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2309.11170-b31b1b.svg)](https://arxiv.org/abs/2309.11170) | :heavy_minus_sign: |
| Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction | [![GitHub](https://img.shields.io/github/stars/zhangy76/KNOWN)](https://github.com/zhangy76/KNOWN) | [![arXiv](https://img.shields.io/badge/arXiv-2308.00799-b31b1b.svg)](https://arxiv.org/abs/2308.00799) | :heavy_minus_sign: |
| Accurate 3D Face Reconstruction with Facial Component Tokens | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Metric3D: Towards Zero-Shot Metric 3D Prediction from a Single Image | [![GitHub](https://img.shields.io/github/stars/YvanYin/Metric3D)](https://github.com/YvanYin/Metric3D) | [![arXiv](https://img.shields.io/badge/arXiv-2307.10984-b31b1b.svg)](https://arxiv.org/abs/2307.10984) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=I3PkukQ3_F8) |
| Reconstructing Interacting Hands with Interaction Prior from Monocular Images | [![GitHub](https://img.shields.io/github/stars/binghui-z/InterPrior_pytorch)](https://github.com/binghui-z/InterPrior_pytorch) | [![arXiv](https://img.shields.io/badge/arXiv-2308.14082-b31b1b.svg)](https://arxiv.org/abs/2308.14082) | :heavy_minus_sign: |
| SparseNeRF: Distilling Depth Ranking for Few-Shot Novel View Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sparsenerf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/Wanggcong/SparseNeRF)](https://github.com/Wanggcong/SparseNeRF) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16196-b31b1b.svg)](https://arxiv.org/abs/2303.16196) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=V0yCTakA964) |
| Beyond the Limitation of Monocular 3D Detector via Knowledge Distillation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://project-hiface.github.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11225-b31b1b.svg)](https://arxiv.org/abs/2303.11225) | :heavy_minus_sign: |
| Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://xujiacong.github.io/Animal3D/) <br /> [![GitHub](https://img.shields.io/github/stars/XuJiacong/Animal3D)](https://github.com/XuJiacong/Animal3D) | [![arXiv](https://img.shields.io/badge/arXiv-2308.11737-b31b1b.svg)](https://arxiv.org/abs/2308.11737) | :heavy_minus_sign: |
| JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery | [![GitHub](https://img.shields.io/github/stars/xljh0520/JOTR)](https://github.com/xljh0520/JOTR) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16377-b31b1b.svg)](https://arxiv.org/abs/2307.16377) | :heavy_minus_sign: |
| D-IF: Uncertainty-Aware Human Digitization via Implicit Distribution Field | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yxt7979.github.io/idf/) <br /> [![GitHub](https://img.shields.io/github/stars/psyai-net/D-IF_release)](https://github.com/psyai-net/D-IF_release) | [![arXiv](https://img.shields.io/badge/arXiv-2308.08857-b31b1b.svg)](https://arxiv.org/abs/2308.08857) | :heavy_minus_sign: |
| 3D Distillation: Improving Self-Supervised Monocular Depth Estimation on Reflective Surfaces | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DeformToon3D: Deformable Neural Radiance Fields for 3D Toonification | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/deformtoon3d/) <br /> [![GitHub](https://img.shields.io/github/stars/junzhezhang/DeformToon3D)](https://github.com/junzhezhang/DeformToon3D) | [![arXiv](https://img.shields.io/badge/arXiv-2309.04410-b31b1b.svg)](https://arxiv.org/abs/2309.04410) | :heavy_minus_sign: |
| MonoDETR: Depth-Guided Transformer for Monocular 3D Object Detection | [![GitHub](https://img.shields.io/github/stars/ZrrSkywalker/MonoDETR)](https://github.com/ZrrSkywalker/MonoDETR) | [![arXiv](https://img.shields.io/badge/arXiv-2203.13310-b31b1b.svg)](https://arxiv.org/abs/2203.13310) | :heavy_minus_sign: |
| ReLeaPS: Reinforcement Learning-based Illumination Planning for Generalized Photometric Stereo | :heavy_minus_sign: | :heavy_minus_sign: | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Tj0t19EUoUA) |
| Convex Decomposition of Indoor Scenes | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.04246-b31b1b.svg)](https://arxiv.org/abs/2307.04246) | :heavy_minus_sign: |
| NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zubair-irshad.github.io/projects/neo360.html) <br /> [![GitHub](https://img.shields.io/github/stars/zubair-irshad/NeO-360)](https://github.com/zubair-irshad/NeO-360) | [![arXiv](https://img.shields.io/badge/arXiv-2308.12967-b31b1b.svg)](https://arxiv.org/abs/2308.12967) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=avmylyL_V8c) |
| UrbanGIRAFFE: Representing Urban Scenes as Compositional Generative Neural Feature Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://lv3d.github.io/urbanGIRAFFE/) | [![arXiv](https://img.shields.io/badge/arXiv-2303.14167-b31b1b.svg)](https://arxiv.org/abs/2303.14167) | :heavy_minus_sign: |
| Efficient Converted Spiking Neural Network for 3D and 2D Classification | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distribution-Aligned Diffusion for Human Mesh Recovery | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://gongjia0208.github.io/HMDiff/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13369-b31b1b.svg)](https://arxiv.org/abs/2308.13369) | :heavy_minus_sign: |
| Towards Zero-Shot Scale-Aware Monocular Depth Estimation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/view/tri-zerodepth) <br /> [![GitHub](https://img.shields.io/github/stars/tri-ml/vidar)](https://github.com/tri-ml/vidar) | [![arXiv](https://img.shields.io/badge/arXiv-2306.17253-b31b1b.svg)](https://arxiv.org/abs/2306.17253) | :heavy_minus_sign: |
| Learning Depth Estimation for Transparent and Mirror Surfaces | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cvlab-unibo.github.io/Depth4ToM/) | [![arXiv](https://img.shields.io/badge/arXiv-2307.15052-b31b1b.svg)](https://arxiv.org/abs/2307.15052) | :heavy_minus_sign: |
| Uni-3D: A Universal Model for Panoptic 3D Scene Reconstruction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| 3D VR Sketch Guided 3D Shape Prototyping and Exploration | [![GitHub](https://img.shields.io/github/stars/Rowl1ng/3Dsketch2shape)](https://github.com/Rowl1ng/3Dsketch2shape) | [![arXiv](https://img.shields.io/badge/arXiv-2306.10830-b31b1b.svg)](https://arxiv.org/abs/2306.10830) | :heavy_minus_sign: |
| Transparent Shape from a Single View Polarization Image | [![GitHub](https://img.shields.io/github/stars/shaomq2187/TransSfP)](https://github.com/shaomq2187/TransSfP) | [![arXiv](https://img.shields.io/badge/arXiv-2204.06331-b31b1b.svg)](https://arxiv.org/abs/2204.06331) | :heavy_minus_sign: |
| Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using Pixel-Aligned Reconstruction Priors | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://x-zhangyang.github.io/2023_Get3DHuman/) <br /> [![GitHub](https://img.shields.io/github/stars/X-zhangyang/Get3DHuman)](https://github.com/X-zhangyang/Get3DHuman) | [![arXiv](https://img.shields.io/badge/arXiv-2302.01162-b31b1b.svg)](https://arxiv.org/abs/2302.01162) | :heavy_minus_sign: |
| Zero-1-to-3: Zero-Shot One Image to 3D Object | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://zero123.cs.columbia.edu/) <br /> [![GitHub](https://img.shields.io/github/stars/cvlab-columbia/zero123)](https://github.com/cvlab-columbia/zero123) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/cvlab/zero123-live) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11328-b31b1b.svg)](https://arxiv.org/abs/2303.11328) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=EzcclEHqUBI) |
| FrozenRecon: Pose-Free 3D Scene Reconstruction with Frozen Depth Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://aim-uofa.github.io/FrozenRecon/) <br /> [![GitHub](https://img.shields.io/github/stars/aim-uofa/FrozenRecon)](https://github.com/aim-uofa/FrozenRecon) | [![arXiv](https://img.shields.io/badge/arXiv-2308.05733-b31b1b.svg)](https://arxiv.org/abs/2308.05733) | :heavy_minus_sign: |
| LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction | [![GitHub](https://img.shields.io/github/stars/robotic-vision-lab/Learning-Implicitly-From-Spatial-Transformers-Network)](https://github.com/robotic-vision-lab/Learning-Implicitly-From-Spatial-Transformers-Network) | [![arXiv](https://img.shields.io/badge/arXiv-2307.12194-b31b1b.svg)](https://arxiv.org/abs/2307.12194) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=gUn5i6FgWWE) |
| 3DMiner: Discovering Shapes from Large-Scale Unannotated Image Datasets | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Nonrigid Object Contact Estimation with Regional Unwrapping Transformer | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.14074-b31b1b.svg)](https://arxiv.org/abs/2308.14074) | :heavy_minus_sign: |
| SHERF: Generalizable Human NeRF from a Single Image | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://skhu101.github.io/SHERF/) <br /> [![GitHub](https://img.shields.io/github/stars/skhu101/SHERF)](https://github.com/skhu101/SHERF) | [![arXiv](https://img.shields.io/badge/arXiv-2303.12791-b31b1b.svg)](https://arxiv.org/abs/2303.12791) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xyiv-cW6VcI) |
| Full-Body Articulated Human-Object Interaction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jnnan.github.io/project/chairs/) <br /> [![GitHub](https://img.shields.io/github/stars/jnnan/chairs)](https://github.com/jnnan/chairs) | [![arXiv](https://img.shields.io/badge/arXiv-2212.10621-b31b1b.svg)](https://arxiv.org/abs/2212.10621) | :heavy_minus_sign: |
| PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sjingjia.github.io/PlaneRecTR/) <br /> [![GitHub](https://img.shields.io/github/stars/SJingjia/PlaneRecTR)](https://github.com/SJingjia/PlaneRecTR) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13756-b31b1b.svg)](https://arxiv.org/abs/2307.13756) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=YBB7totHGJg) |
| SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://astra-vision.github.io/SceneRF/) <br /> [![GitHub](https://img.shields.io/github/stars/astra-vision/SceneRF)](https://github.com/astra-vision/SceneRF) | [![arXiv](https://img.shields.io/badge/arXiv-2212.02501-b31b1b.svg)](https://arxiv.org/abs/2212.02501) | :heavy_minus_sign: |
| 3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3dnbf.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/edz-o/3DNBF)](https://github.com/edz-o/3DNBF) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10123-b31b1b.svg)](https://arxiv.org/abs/2308.10123) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=LO80Am0Sb0Y) |
| Two-in-One Depth: Bridging the Gap between Monocular and Binocular Self-Supervised Depth Estimation | [![GitHub](https://img.shields.io/github/stars/ZM-Zhou/TiO-Depth_pytorch)](https://github.com/ZM-Zhou/TiO-Depth_pytorch) | [![arXiv](https://img.shields.io/badge/arXiv-2309.00933-b31b1b.svg)](https://arxiv.org/abs/2309.00933) | :heavy_minus_sign: |
| LRRU: Long-Short Range Recurrent Updating Networks for Depth Completion | [![GitHub](https://img.shields.io/github/stars/YufeiWang777/LRRU)](https://github.com/YufeiWang777/LRRU) | :heavy_minus_sign: | :heavy_minus_sign: |
| OccFormer: Dual-Path Transformer for Vision-based 3D Semantic Occupancy Prediction | [![GitHub](https://img.shields.io/github/stars/zhangyp15/OccFormer)](https://github.com/zhangyp15/OccFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2304.05316-b31b1b.svg)](https://arxiv.org/abs/2304.05316) | :heavy_minus_sign: |
| CHORD: Category-Level Hand-Held Object Reconstruction via Shape Deformation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://kailinli.github.io/CHORD/) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10574-b31b1b.svg)](https://arxiv.org/abs/2308.10574) | :heavy_minus_sign: |
| NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiawei-yao0812.github.io/NDC-Scene/) <br /> [![GitHub](https://img.shields.io/github/stars/Jiawei-Yao0812/NDCScene)](https://github.com/Jiawei-Yao0812/NDCScene) | [![arXiv](https://img.shields.io/badge/arXiv-2309.14616-b31b1b.svg)](https://arxiv.org/abs/2309.14616) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=hEpxgMSijUc) |
| Neural Video Depth Stabilizer | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://raymondwang987.github.io/NVDS/) <br /> [![GitHub](https://img.shields.io/github/stars/RaymondWang987/NVDS)](https://github.com/RaymondWang987/NVDS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.08695-b31b1b.svg)](https://arxiv.org/abs/2307.08695) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SNV9F-60xrE) |
| DiLiGenT-Pi: Photometric Stereo for Planar Surfaces with Rich Details - Benchmark Dataset and Beyond | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://photometricstereo.github.io/diligentpi.html) | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](https://photometricstereo.github.io/imgs/diligentpi/paper.pdf) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Motion Estimation, Matching and Tracking

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| TMR: Text-to-Motion Retrieval using Contrastive 3D Human Motion Synthesis | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://mathis.petrovich.fr/tmr/) <br /> [![GitHub](https://img.shields.io/github/stars/Mathux/TMR)](https://github.com/Mathux/TMR) <br /> [![Hugging Face](https://img.shields.io/badge/🤗-Demo-FFD21F.svg)](https://huggingface.co/spaces/Mathux/TMR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Petrovich_TMR_Text-to-Motion_Retrieval_Using_Contrastive_3D_Human_Motion_Synthesis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.00976-b31b1b.svg)](https://arxiv.org/abs/2305.00976) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FK0RukgDEtM) |
| Sequential Texts Driven Cohesive Motions Synthesis with Natural Transitions | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://druthrie.github.io/sequential-texts-to-motion/) <br /> [![GitHub](https://img.shields.io/github/stars/Druthrie/ST2M)](https://github.com/Druthrie/ST2M) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Sequential_Texts_Driven_Cohesive_Motions_Synthesis_with_Natural_Transitions_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Auxiliary Tasks Benefit 3D Skeleton-based Human Motion Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/AuxFormer)](https://github.com/MediaBrain-SJTU/AuxFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Auxiliary_Tasks_Benefit_3D_Skeleton-based_Human_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08942-b31b1b.svg)](https://arxiv.org/abs/2308.08942) | :heavy_minus_sign: |
| Explicit Motion Disentangling for Efficient Optical Flow Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Explicit_Motion_Disentangling_for_Efficient_Optical_Flow_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TrackFlow: Multi-Object tracking with Normalizing Flows | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mancusi_TrackFlow_Multi-Object_tracking_with_Normalizing_Flows_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11513-b31b1b.svg)](https://arxiv.org/abs/2308.11513) | :heavy_minus_sign: |
| HumanMAC: Masked Motion Completion for Human Motion Prediction | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://lhchen.top/Human-MAC/) <br /> [![GitHub](https://img.shields.io/github/stars/LinghaoChan/HumanMAC)](https://github.com/LinghaoChan/HumanMAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_HumanMAC_Masked_Motion_Completion_for_Human_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.03665-b31b1b.svg)](https://arxiv.org/abs/2302.03665) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vfde9GdUHBs) |
| Geometrized Transformer for Self-Supervised Homography Estimation | [![GitHub](https://img.shields.io/github/stars/ruc-aimc-lab/GeoFormer)](https://github.com/ruc-aimc-lab/GeoFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Geometrized_Transformer_for_Self-Supervised_Homography_Estimation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving | [![GitHub](https://img.shields.io/github/stars/duke-vision/semantic-unsup-flow-release)](https://github.com/duke-vision/semantic-unsup-flow-release) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yuan_SemARFlow_Injecting_Semantics_into_Unsupervised_Optical_Flow_Estimation_for_Autonomous_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06209-b31b1b.svg)](https://arxiv.org/abs/2303.06209) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=XYBTolH2S8A) |
| NeSS-ST: Detecting Good and Stable Keypoints with a Neural Stability Score and the Shi-Tomasi Detector | [![GitHub](https://img.shields.io/github/stars/KonstantinPakulev/NeSS-ST)](https://github.com/KonstantinPakulev/NeSS-ST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pakulev_NeSS-ST_Detecting_Good_and_Stable_Keypoints_with_a_Neural_Stability_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Robust Object Modeling for Visual Tracking | [![GitHub](https://img.shields.io/github/stars/dawnyc/ROMTrack)](https://github.com/dawnyc/ROMTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cai_Robust_Object_Modeling_for_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05140-b31b1b.svg)](https://arxiv.org/abs/2308.05140) | :heavy_minus_sign: |
| Social Diffusion: Long-Term Multiple Human Motion Anticipation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Exploring Lightweight Hierarchical Vision Transformers for Efficient Visual Tracking | [![GitHub](https://img.shields.io/github/stars/kangben258/HiT)](https://github.com/kangben258/HiT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kang_Exploring_Lightweight_Hierarchical_Vision_Transformers_for_Efficient_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06904-b31b1b.svg)](https://arxiv.org/abs/2308.06904) | :heavy_minus_sign: |
| HMD-NeMo: Online 3D Avatar Motion Generation from Sparse Observations | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11261-b31b1b.svg)](https://arxiv.org/abs/2308.11261) | :heavy_minus_sign: |
| Learning Fine-Grained Features for Pixel-Wise Video Correspondences | [![GitHub](https://img.shields.io/github/stars/qianduoduolr/Spa-then-Temp)](https://github.com/qianduoduolr/Spa-then-Temp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Learning_Fine-Grained_Features_for_Pixel-Wise_Video_Correspondences_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03040-b31b1b.svg)](https://arxiv.org/abs/2308.03040) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=2ZCVUoiyM0U) |
| GAFlow: Incorporating Gaussian Attention into Optical Flow | [![GitHub](https://img.shields.io/github/stars/LA30/GAFlow)](https://github.com/LA30/GAFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_GAFlow_Incorporating_Gaussian_Attention_into_Optical_Flow_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Occ<sup>2</sup>Net: Robust Image Matching based on 3D Occupancy Estimation for Occluded Regions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fan_Occ2Net_Robust_Image_Matching_Based_on_3D_Occupancy_Estimation_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16160-b31b1b.svg)](https://arxiv.org/abs/2308.16160) | :heavy_minus_sign: |
| Locomotion-Action-Manipulation: Synthesizing Human-Scene Interactions in Complex 3D Environments | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jiyewise.github.io/projects/LAMA/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Locomotion-Action-Manipulation_Synthesizing_Human-Scene_Interactions_in_Complex_3D_Environments_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.02667-b31b1b.svg)](https://arxiv.org/abs/2301.02667) | :heavy_minus_sign: |
| Trajectory Unified Transformer for Pedestrian Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/lssiair/TUTR)](https://github.com/lssiair/TUTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Trajectory_Unified_Transformer_for_Pedestrian_Trajectory_Prediction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TMA: Temporal Motion Aggregation for Event-based Optical Flow | [![GitHub](https://img.shields.io/github/stars/ispc-lab/TMA)](https://github.com/ispc-lab/TMA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_TMA_Temporal_Motion_Aggregation_for_Event-based_Optical_Flow_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11629-b31b1b.svg)](https://arxiv.org/abs/2303.11629) | :heavy_minus_sign: |
| Taming Contrast Maximization for Learning Sequential, Low-Latency, Event-based Optical Flow | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mavlab.tudelft.nl/taming_event_flow/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Paredes-Valles_Taming_Contrast_Maximization_for_Learning_Sequential_Low-latency_Event-based_Optical_Flow_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=vkYimENc494) |
| GlueStick: Robust Image Matching by Sticking Points and Lines Together | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://iago-suarez.com/gluestick/) <br /> [![GitHub](https://img.shields.io/github/stars/cvg/GlueStick)](https://github.com/cvg/GlueStick) <br /> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cvg/GlueStick/blob/main/gluestick_matching_demo.ipynb) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pautrat_GlueStick_Robust_Image_Matching_by_Sticking_Points_and_Lines_Together_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02008-b31b1b.svg)](https://arxiv.org/abs/2304.02008) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JmpddJ5pfz8) |
| DARTH: Holistic Test-Time Adaptation for Multiple Object Tracking | [![GitHub](https://img.shields.io/github/stars/mattiasegu/darth)](https://github.com/mattiasegu/darth) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Segu_DARTH_Holistic_Test-time_Adaptation_for_Multiple_Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| S-TREK: Sequential Translation and Rotation Equivariant Keypoints for Local Feature Extraction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Santellani_S-TREK_Sequential_Translation_and_Rotation_Equivariant_Keypoints_for_Local_Feature_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14598-b31b1b.svg)](https://arxiv.org/abs/2308.14598) | :heavy_minus_sign: |
| Integrating Boxes and Masks: A Multi-Object Framework for Unified Visual Tracking and Segmentation | [![GitHub](https://img.shields.io/github/stars/yoxu515/MITS)](https://github.com/yoxu515/MITS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Integrating_Boxes_and_Masks_A_Multi-Object_Framework_for_Unified_Visual_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13266-b31b1b.svg)](https://arxiv.org/abs/2308.13266) | :heavy_minus_sign: |
| Robust Frame-to-Frame Camera Rotation Estimation in Crowded Scenes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://fabiendelattre.com/robust-rotation-estimation/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Delattre_Robust_Frame-to-Frame_Camera_Rotation_Estimation_in_Crowded_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.08588-b31b1b.svg)](https://arxiv.org/abs/2309.08588) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=SL4QBedLu9Q) |
| Sparse Instance Conditioned Multimodal Trajectory Prediction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Sparse_Instance_Conditioned_Multimodal_Trajectory_Prediction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://posediffusion.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/facebookresearch/PoseDiffusion)](https://github.com/facebookresearch/PoseDiffusion) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_PoseDiffusion_Solving_Pose_Estimation_via_Diffusion-aided_Bundle_Adjustment_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.15667-b31b1b.svg)](https://arxiv.org/abs/2306.15667) | :heavy_minus_sign: |
| 3DMOTFormer: Graph Transformer for Online 3D Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/dsx0511/3DMOTFormer)](https://github.com/dsx0511/3DMOTFormer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ding_3DMOTFormer_Graph_Transformer_for_Online_3D_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06635-b31b1b.svg)](https://arxiv.org/abs/2308.06635) | :heavy_minus_sign: |
| Fast Inference and Update of Probabilistic Density Estimation on Trajectory Prediction | [![GitHub](https://img.shields.io/github/stars/meaten/FlowChain-ICCV2023)](https://github.com/meaten/FlowChain-ICCV2023) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Maeda_Fast_Inference_and_Update_of_Probabilistic_Density_Estimation_on_Trajectory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.08824-b31b1b.svg)](https://arxiv.org/abs/2308.08824) | :heavy_minus_sign: |
| Supervised Homography Learning with Realistic Dataset Generation | [![GitHub](https://img.shields.io/github/stars/JianghaiSCU/RealSH)](https://github.com/JianghaiSCU/RealSH) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_Supervised_Homography_Learning_with_Realistic_Dataset_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15353-b31b1b.svg)](https://arxiv.org/abs/2307.15353) | :heavy_minus_sign: |
| Joint-Relation Transformer for Multi-Person Motion Prediction | [![GitHub](https://img.shields.io/github/stars/MediaBrain-SJTU/JRTransformer)](https://github.com/MediaBrain-SJTU/JRTransformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Joint-Relation_Transformer_for_Multi-Person_Motion_Prediction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04808-b31b1b.svg)](https://arxiv.org/abs/2308.04808) | :heavy_minus_sign: |
| Event-based Temporally Dense Optical Flow Estimation with Sequential Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ponghiran_Event-based_Temporally_Dense_Optical_Flow_Estimation_with_Sequential_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.01244-b31b1b.svg)](https://arxiv.org/abs/2210.01244) | :heavy_minus_sign: |
| 3D Motion Magnification: Visualizing Subtle Motions from Time-Varying Radiance Fields | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://3d-motion-magnification.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/3d-motion-magnification/3d-motion-mag)](https://github.com/3d-motion-magnification/3d-motion-mag) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_3D_Motion_Magnification_Visualizing_Subtle_Motions_from_Time-Varying_Radiance_Fields_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03757-b31b1b.svg)](https://arxiv.org/abs/2308.03757) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=ljar4GAFkUk&t=146s) |
| Learning Optical Flow from Event Camera with Rendered Dataset | [![GitHub](https://img.shields.io/github/stars/boomluo02/ADMFlow)](https://github.com/boomluo02/ADMFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Learning_Optical_Flow_from_Event_Camera_with_Rendered_Dataset_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11011-b31b1b.svg)](https://arxiv.org/abs/2303.11011) | :heavy_minus_sign: |
| Persistent-Transient Duality: A Multi-Mechanism Approach for Modeling Human-Object Interaction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tran_Persistent-Transient_Duality_A_Multi-Mechanism_Approach_for_Modeling_Human-Object_Interaction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12729-b31b1b.svg)](https://arxiv.org/abs/2307.12729) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=nVOQdI8g7AY) |
| Deep Homography Mixture for Single Image Rolling Shutter Correction | [![GitHub](https://img.shields.io/github/stars/DavidYan2001/Deep_RS-HM)](https://github.com/DavidYan2001/Deep_RS-HM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Deep_Homography_Mixture_for_Single_Image_Rolling_Shutter_Correction_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Fast Neural Scene Flow | [![GitHub](https://img.shields.io/github/stars/Lilac-Lee/FastNSF)](https://github.com/Lilac-Lee/FastNSF) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Fast_Neural_Scene_Flow_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.09121-b31b1b.svg)](https://arxiv.org/abs/2304.09121) | :heavy_minus_sign: |
| RLSAC: Reinforcement Learning Enhanced Sample Consensus for End-to-End Robust Estimation | [![GitHub](https://img.shields.io/github/stars/IRMVLab/RLSAC)](https://github.com/IRMVLab/RLSAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nie_RLSAC_Reinforcement_Learning_Enhanced_Sample_Consensus_for_End-to-End_Robust_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05318-b31b1b.svg)](https://arxiv.org/abs/2308.05318) | :heavy_minus_sign: |
| MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/MeMOTR)](https://github.com/MCG-NJU/MeMOTR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_MeMOTR_Long-Term_Memory-Augmented_Transformer_for_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15700-b31b1b.svg)](https://arxiv.org/abs/2307.15700) | :heavy_minus_sign: |
| MBPTrack: Improving 3D Point Cloud Tracking with Memory Networks and Box Priors | [![GitHub](https://img.shields.io/github/stars/slothfulxtx/MBPTrack3D)](https://github.com/slothfulxtx/MBPTrack3D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_MBPTrack_Improving_3D_Point_Cloud_Tracking_with_Memory_Networks_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05071-b31b1b.svg)](https://arxiv.org/abs/2303.05071) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Zl_4LnoX_Ak) |
| SportsMOT: A Large Multi-Object Tracking Dataset in Multiple Sports Scenes | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/SportsMOT)](https://github.com/MCG-NJU/SportsMOT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cui_SportsMOT_A_Large_Multi-Object_Tracking_Dataset_in_Multiple_Sports_Scenes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.05170-b31b1b.svg)](https://arxiv.org/abs/2304.05170) | :heavy_minus_sign: |
| Heterogeneous Diversity Driven Active Learning for Multi-Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Heterogeneous_Diversity_Driven_Active_Learning_for_Multi-Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TM2D: Bimodality Driven 3D Dance Generation via Music-Text Integration | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://garfield-kh.github.io/TM2D/) <br /> [![GitHub](https://img.shields.io/github/stars/Garfield-kh/TM2D)](https://github.com/Garfield-kh/TM2D) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gong_TM2D_Bimodality_Driven_3D_Dance_Generation_via_Music-Text_Integration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02419-b31b1b.svg)](https://arxiv.org/abs/2304.02419) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=6QQFXG4s7iQ) |
| Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Synchronize_Feature_Extracting_and_Matching_A_Single_Branch_Framework_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12549-b31b1b.svg)](https://arxiv.org/abs/2308.12549) | :heavy_minus_sign: |
| Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/yolomax/ColTrack)](https://github.com/yolomax/ColTrack) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Collaborative_Tracking_Learning_for_Frame-Rate-Insensitive_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05911-b31b1b.svg)](https://arxiv.org/abs/2308.05911) | :heavy_minus_sign: |
| CiteTracker: Correlating Image and Text for Visual Tracking | [![GitHub](https://img.shields.io/github/stars/NorahGreen/CiteTracker)](https://github.com/NorahGreen/CiteTracker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_CiteTracker_Correlating_Image_and_Text_for_Visual_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11322-b31b1b.svg)](https://arxiv.org/abs/2308.11322) | :heavy_minus_sign: |
| SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sinc.is.tue.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Athanasiou_SINC_Spatial_Composition_of_3D_Human_Motions_for_Simultaneous_Action_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.10417-b31b1b.svg)](https://arxiv.org/abs/2304.10417) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=uwUriDnKTLI) |
| Uncertainty-Aware Unsupervised Multi-Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Uncertainty-aware_Unsupervised_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15409-b31b1b.svg)](https://arxiv.org/abs/2307.15409) | :heavy_minus_sign: |
| PVT++: A Simple End-to-End Latency-Aware Visual Tracking Framework | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jaraxxus-me.github.io/ICCV2023_PVTpp/) <br /> [![GitHub](https://img.shields.io/github/stars/Jaraxxus-Me/PVT_pp)](https://github.com/Jaraxxus-Me/PVT_pp) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_PVT_A_Simple_End-to-End_Latency-Aware_Visual_Tracking_Framework_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ihbae.com/publication/eigentrajectory/) <br /> [![GitHub](https://img.shields.io/github/stars/inhwanbae/EigenTrajectory)](https://github.com/inhwanbae/EigenTrajectory) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bae_EigenTrajectory_Low-Rank_Descriptors_for_Multi-Modal_Trajectory_Forecasting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09306-b31b1b.svg)](https://arxiv.org/abs/2307.09306) | :heavy_minus_sign: |
| RPEFlow: Multimodal Fusion of RGB-PointCloud-Event for Joint Optical Flow and Scene Flow Estimation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://npucvr.github.io/RPEFlow/) <br /> [![GitHub](https://img.shields.io/github/stars/danqu130/RPEFlow)](https://github.com/danqu130/RPEFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wan_RPEFlow_Multimodal_Fusion_of_RGB-PointCloud-Event_for_Joint_Optical_Flow_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15082-b31b1b.svg)](https://arxiv.org/abs/2309.15082) | :heavy_minus_sign: |
| Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud based Scene Flow Estimation | [![GitHub](https://img.shields.io/github/stars/cwc1260/MSBRN)](https://github.com/cwc1260/MSBRN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Multi-Scale_Bidirectional_Recurrent_Network_with_Hybrid_Correlation_for_Point_Cloud_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ReST: A Reconfigurable Spatial-Temporal Graph Model for Multi-Camera Multi-Object Tracking | [![GitHub](https://img.shields.io/github/stars/chengche6230/ReST)](https://github.com/chengche6230/ReST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_ReST_A_Reconfigurable_Spatial-Temporal_Graph_Model_for_Multi-Camera_Multi-Object_Tracking_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13229-b31b1b.svg)](https://arxiv.org/abs/2308.13229) | :heavy_minus_sign: |
| TAPIR: Tracking any Point with Per-Frame Initialization and Temporal Refinement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://deepmind-tapir.github.io/) <br /> [![GitHub](https://img.shields.io/github/stars/google-deepmind/tapnet)](https://github.com/google-deepmind/tapnet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Doersch_TAPIR_Tracking_Any_Point_with_Per-Frame_Initialization_and_Temporal_Refinement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.08637-b31b1b.svg)](https://arxiv.org/abs/2306.08637) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=I1DQJH3v7Nk) |
| IHNet: Iterative Hierarchical Network Guided by High-Resolution Estimated Information for Scene Flow Estimation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_IHNet_Iterative_Hierarchical_Network_Guided_by_High-Resolution_Estimated_Information_for_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Can Language Models Learn to Listen? | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/) <br /> [![GitHub](https://img.shields.io/github/stars/sanjayss34/lm-listener)](https://github.com/sanjayss34/lm-listener) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ng_Can_Language_Models_Learn_to_Listen_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10897-b31b1b.svg)](https://arxiv.org/abs/2308.10897) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=djpSOhdIU8M) |
| XVO: Generalized Visual Odometry via Cross-Modal Self-Training | [![GitHub](https://img.shields.io/github/stars/h2xlab/XVO)](https://github.com/h2xlab/XVO) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lai_XVO_Generalized_Visual_Odometry_via_Cross-Modal_Self-Training_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Distracting Downpour: Adversarial Weather Attacks for Motion Estimation | [![GitHub](https://img.shields.io/github/stars/cv-stuttgart/DistractingDownpour)](https://github.com/cv-stuttgart/DistractingDownpour) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Schmalfuss_Distracting_Downpour_Adversarial_Weather_Attacks_for_Motion_Estimation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06716-b31b1b.svg)](https://arxiv.org/abs/2305.06716) | :heavy_minus_sign: |
| Foreground-Background Distribution Modeling Transformer for Visual Object Tracking | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Foreground-Background_Distribution_Modeling_Transformer_for_Visual_Object_Tracking_ICCV_2023_paper.pdf) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Action and Event Understanding

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Weakly-Supervised Action Segmentation and Unseen Error Detection in Anomalous Instructional Videos | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ghoddoosian_Weakly-Supervised_Action_Segmentation_and_Unseen_Error_Detection_in_Anomalous_Instructional_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Diffusion Action Segmentation | [![GitHub](https://img.shields.io/github/stars/Finspire13/DiffAct)](https://github.com/Finspire13/DiffAct) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17959-b31b1b.svg)](https://arxiv.org/abs/2303.17959) | :heavy_minus_sign: |
| Audio-Visual Glance Network for Efficient Video Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nugroho_Audio-Visual_Glance_Network_for_Efficient_Video_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09322-b31b1b.svg)](https://arxiv.org/abs/2308.09322) | :heavy_minus_sign: |
| Learning from Noisy Pseudo Labels for Semi-Supervised Temporal Action Localization | [![GitHub](https://img.shields.io/github/stars/kunnxia/NPL)](https://github.com/kunnxia/NPL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xia_Learning_from_Noisy_Pseudo_Labels_for_Semi-Supervised_Temporal_Action_Localization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Video Action Recognition with Attentive Semantic Units | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Video_Action_Recognition_with_Attentive_Semantic_Units_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09756-b31b1b.svg)](https://arxiv.org/abs/2303.09756) | :heavy_minus_sign: |
| Masked Motion Predictors are Strong 3D Action Representation Learners | [![GitHub](https://img.shields.io/github/stars/maoyunyao/MAMP)](https://github.com/maoyunyao/MAMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mao_Masked_Motion_Predictors_are_Strong_3D_Action_Representation_Learners_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07092-b31b1b.svg)](https://arxiv.org/abs/2308.07092) | :heavy_minus_sign: |
| Boosting Positive Segments for Weakly-Supervised Audio-Visual Video Parsing | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Rachavarapu_Boosting_Positive_Segments_for_Weakly-Supervised_Audio-Visual_Video_Parsing_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Weakly-Supervised Action Localization by Hierarchically-Structured Latent Attention Modeling | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Weakly-Supervised_Action_Localization_by_Hierarchically-Structured_Latent_Attention_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09946-b31b1b.svg)](https://arxiv.org/abs/2308.09946) | :heavy_minus_sign: |
| Few-Shot Common Action Localization via Cross-Attentional Fusion of Context and Temporal Dynamics | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Few-Shot_Common_Action_Localization_via_Cross-Attentional_Fusion_of_Context_and_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Interaction-Aware Joint Attention Estimation using People Attributes | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.toyota-ti.ac.jp/Lab/Denshi/iim/ukita/selection/ICCV2023-PJAE.html) <br /> [![GitHub](https://img.shields.io/github/stars/chihina/PJAE)](https://github.com/chihina/PJAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nakatani_Interaction-aware_Joint_Attention_Estimation_Using_People_Attributes_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05382-b31b1b.svg)](https://arxiv.org/abs/2308.05382) | :heavy_minus_sign: |
| FineDance: A Fine-Grained Choreography Dataset for 3D Full Body Dance Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://li-ronghui.github.io/finedance) <br /> [![GitHub](https://img.shields.io/github/stars/li-ronghui/FineDance)](https://github.com/li-ronghui/FineDance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_FineDance_A_Fine-grained_Choreography_Dataset_for_3D_Full_Body_Dance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.03741-b31b1b.svg)](https://arxiv.org/abs/2212.03741) | :heavy_minus_sign: |
| SOAR: Scene-Debiasing Open-Set Action Recognition | [![GitHub](https://img.shields.io/github/stars/yhZhai/SOAR)](https://github.com/yhZhai/SOAR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_SOAR_Scene-debiasing_Open-set_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01265-b31b1b.svg)](https://arxiv.org/abs/2309.01265) | :heavy_minus_sign: |
| Leveraging Spatio-Temporal Dependency for Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/Jho-Yonsei/STC-Net)](https://github.com/Jho-Yonsei/STC-Net) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Leveraging_Spatio-Temporal_Dependency_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04761-b31b1b.svg)](https://arxiv.org/abs/2212.04761) | :heavy_minus_sign: |
| Cross-Modal Learning with 3D Deformable Attention for Action Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Cross-Modal_Learning_with_3D_Deformable_Attention_for_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.05638-b31b1b.svg)](https://arxiv.org/abs/2212.05638) | :heavy_minus_sign: |
| Generative Action Description Prompts for Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/MartinXM/GAP)](https://github.com/MartinXM/GAP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiang_Generative_Action_Description_Prompts_for_Skeleton-based_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.05318-b31b1b.svg)](https://arxiv.org/abs/2208.05318) | :heavy_minus_sign: |
| Self-Feedback DETR for Temporal Action Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Self-Feedback_DETR_for_Temporal_Action_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10570-b31b1b.svg)](https://arxiv.org/abs/2308.10570) | :heavy_minus_sign: |
| Skip-Plan: Procedure Planning in Instructional Videos via Condensed Action Space Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_Skip-Plan_Procedure_Planning_in_Instructional_Videos_via_Condensed_Action_Space_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| The Unreasonable Effectiveness of Large Language-Vision Models for Source-Free Video Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/giaczara/dallv)](https://github.com/giaczara/dallv) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zara_The_Unreasonable_Effectiveness_of_Large_Language-Vision_Models_for_Source-Free_Video_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09139-b31b1b.svg)](https://arxiv.org/abs/2308.09139) | :heavy_minus_sign: |
| Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.pinlab.org/mocodad) <br /> [![GitHub](https://img.shields.io/github/stars/aleflabo/MoCoDAD)](https://github.com/aleflabo/MoCoDAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Flaborea_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeleton-based_Video_Anomaly_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.07205-b31b1b.svg)](https://arxiv.org/abs/2307.07205) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=IuDzVez--9U) |
| Video Anomaly Detection via Sequentially Learning Multiple Pretext Tasks | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Video_Anomaly_Detection_via_Sequentially_Learning_Multiple_Pretext_Tasks_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| MiniROAD: Minimal RNN Framework for Online Action Detection | [![GitHub](https://img.shields.io/github/stars/jbistanbul/MiniROAD)](https://github.com/jbistanbul/MiniROAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/An_MiniROAD_Minimal_RNN_Framework_for_Online_Action_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| How much Temporal Long-Term Context is Needed for Action Segmentation? | [![GitHub](https://img.shields.io/github/stars/LTContext/LTContext)](https://github.com/LTContext/LTContext) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bahrami_How_Much_Temporal_Long-Term_Context_is_Needed_for_Action_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11358-b31b1b.svg)](https://arxiv.org/abs/2308.11358) | :heavy_minus_sign: |
| DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion | [![GitHub](https://img.shields.io/github/stars/sauradip/DiffusionTAD)](https://github.com/sauradip/DiffusionTAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Nag_DiffTAD_Temporal_Action_Detection_with_Proposal_Denoising_Diffusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14863-b31b1b.svg)](https://arxiv.org/abs/2303.14863) | :heavy_minus_sign: |
| STEPs: Self-Supervised Key Step Extraction and Localization from Unlabeled Procedural Videos | [![GitHub](https://img.shields.io/github/stars/anshulbshah/STEPs)](https://github.com/anshulbshah/STEPs) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shah_STEPs_Self-Supervised_Key_Step_Extraction_and_Localization_from_Unlabeled_Procedural_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.00794-b31b1b.svg)](https://arxiv.org/abs/2301.00794) | :heavy_minus_sign: |
| Efficient Video Action Detection with Token Dropout and Context Refinement | [![GitHub](https://img.shields.io/github/stars/MCG-NJU/EVAD)](https://github.com/MCG-NJU/EVAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Efficient_Video_Action_Detection_with_Token_Dropout_and_Context_Refinement_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.08451-b31b1b.svg)](https://arxiv.org/abs/2304.08451) | :heavy_minus_sign: |
| FSAR: Federated Skeleton-based Action Recognition with Adaptive Topology Structure and Knowledge Distillation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_FSAR_Federated_Skeleton-based_Action_Recognition_with_Adaptive_Topology_Structure_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.11046-b31b1b.svg)](https://arxiv.org/abs/2306.11046) | :heavy_minus_sign: |
| Exploring Predicate Visual Context in Detecting of Human-Object Interactions | [![GitHub](https://img.shields.io/github/stars/fredzzhang/pvic)](https://github.com/fredzzhang/pvic) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Exploring_Predicate_Visual_Context_in_Detecting_of_Human-Object_Interactions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06202-b31b1b.svg)](https://arxiv.org/abs/2308.06202) | :heavy_minus_sign: |
| E2E-LOAD: End-to-End Long-Form Online Action Detection | [![GitHub](https://img.shields.io/github/stars/sqiangcao99/E2E-LOAD)](https://github.com/sqiangcao99/E2E-LOAD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cao_E2E-LOAD_End-to-End_Long-form_Online_Action_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2306.07703-b31b1b.svg)](https://arxiv.org/abs/2306.07703) | :heavy_minus_sign: |
| Revisiting Foreground and Background Separation in Weakly-Supervised Temporal Action Localization: A Clustering-based Approach | [![GitHub](https://img.shields.io/github/stars/Qinying-Liu/CASE)](https://github.com/Qinying-Liu/CASE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Revisiting_Foreground_and_Background_Separation_in_Weakly-supervised_Temporal_Action_Localization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Hierarchically Decomposed Graph Convolutional Networks for Skeleton-based Action Recognition | [![GitHub](https://img.shields.io/github/stars/Jho-Yonsei/HD-GCN)](https://github.com/Jho-Yonsei/HD-GCN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Hierarchically_Decomposed_Graph_Convolutional_Networks_for_Skeleton-Based_Action_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2208.10741-b31b1b.svg)](https://arxiv.org/abs/2208.10741) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computational Imaging

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Tiled Multiplane Images for Practical 3D Photography | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_Tiled_Multiplane_Images_for_Practical_3D_Photography_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.14291-b31b1b.svg)](https://arxiv.org/abs/2309.14291) | :heavy_minus_sign: |
| Eulerian Single-Photon Vision | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://wisionlab.com/project/eulerian-single-photon-vision/) <br /> [![GitHub](https://img.shields.io/github/stars/shantanu-gupta/ESPV)](https://github.com/shantanu-gupta/ESPV) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gupta_Eulerian_Single-Photon_Vision_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| ProPainter: Improving Propagation and Transformer for Video Inpainting | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://shangchenzhou.com/projects/ProPainter/) <br /> [![GitHub](https://img.shields.io/github/stars/sczhou/ProPainter)](https://github.com/sczhou/ProPainter) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ProPainter_Improving_Propagation_and_Transformer_for_Video_Inpainting_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03897-b31b1b.svg)](https://arxiv.org/abs/2309.03897) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=92EHfgCO5-Q) |
| Global Perception based Autoregressive Neural Processes | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tai_Global_Perception_Based_Autoregressive_Neural_Processes_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DOLCE: A Model-based Probabilistic Diffusion Framework for Limited-Angle CT Reconstruction | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_DOLCE_A_Model-Based_Probabilistic_Diffusion_Framework_for_Limited-Angle_CT_Reconstruction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12340-b31b1b.svg)](https://arxiv.org/abs/2211.12340) | :heavy_minus_sign: |
| GlowGAN: Unsupervised Learning of HDR Images from LDR Images in the Wild | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://glowgan.mpi-inf.mpg.de/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_GlowGAN_Unsupervised_Learning_of_HDR_Images_from_LDR_Images_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.12352-b31b1b.svg)](https://arxiv.org/abs/2211.12352) | :heavy_minus_sign: |
| Score-based Diffusion Models as Principled Priors for Inverse Imaging | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](http://imaging.cms.caltech.edu/score_prior/) <br /> [![GitHub](https://img.shields.io/github/stars/berthyf96/score_prior)](https://github.com/berthyf96/score_prior) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Score-Based_Diffusion_Models_as_Principled_Priors_for_Inverse_Imaging_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.11751-b31b1b.svg)](https://arxiv.org/abs/2304.11751) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=FkPpQ_GDh4Y) |
| NLOS-NeuS: Non-Line-of-Sight Neural Implicit Surface | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yfujimura.github.io/nlos-neus/) <br /> [![GitHub](https://img.shields.io/github/stars/yfujimura/nlos-neus)](https://github.com/yfujimura/nlos-neus) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fujimura_NLOS-NeuS_Non-line-of-sight_Neural_Implicit_Surface_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.12280-b31b1b.svg)](https://arxiv.org/abs/2303.12280) | :heavy_minus_sign: |
| MEFLUT: Unsupervised 1D Lookup Tables for Multi-Exposure Image Fusion | [![GitHub](https://img.shields.io/github/stars/Hedlen/MEFLUT)](https://github.com/Hedlen/MEFLUT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jiang_MEFLUT_Unsupervised_1D_Lookup_Tables_for_Multi-exposure_Image_Fusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.11847-b31b1b.svg)](https://arxiv.org/abs/2309.11847) | :heavy_minus_sign: |
| Temporal-Coded Spiking Neural Networks with Dynamic Firing Threshold: Learning with Event-Driven Backpropagation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wei_Temporal-Coded_Spiking_Neural_Networks_with_Dynamic_Firing_Threshold_Learning_with_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Enhancing Non-Line-of-Sight Imaging via Learnable Inverse Kernel and Attention Mechanisms | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Enhancing_Non-line-of-sight_Imaging_via_Learnable_Inverse_Kernel_and_Attention_Mechanisms_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Aperture Diffraction for Compact Snapshot Spectral Imaging | [![GitHub](https://img.shields.io/github/stars/Krito-ex/CSST)](https://github.com/Krito-ex/CSST) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lv_Aperture_Diffraction_for_Compact_Snapshot_Spectral_Imaging_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16372-b31b1b.svg)](https://arxiv.org/abs/2309.16372) | :heavy_minus_sign: |
| Content-Aware Local GAN for Photo-Realistic Super-Resolution | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_Content-Aware_Local_GAN_for_Photo-Realistic_Super-Resolution_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| RED-PSM: Regularization by Denoising of Partially Separable Models for Dynamic Imaging | [![GitHub](https://img.shields.io/github/stars/berkiskender/RED-PSM)](https://github.com/berkiskender/RED-PSM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Iskender_RED-PSM_Regularization_by_Denoising_of_Partially_Separable_Models_for_Dynamic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.03483-b31b1b.svg)](https://arxiv.org/abs/2304.03483) | :heavy_minus_sign: |
| Self-Supervised Burst Super-Resolution | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bhat_Self-Supervised_Burst_Super-Resolution_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Coherent Event Guided Low-Light Video Enhancement | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://sherrycattt.github.io/EvLowLight/) <br /> [![GitHub](https://img.shields.io/github/stars/sherrycattt/EvLowLight)](https://github.com/sherrycattt/EvLowLight) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Coherent_Event_Guided_Low-Light_Video_Enhancement_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zLz0GTTXwZg) |
| Panoramas from Photons | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://wisionlab.com/project/panoramas-from-photons/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Jungerman_Panoramas_from_Photons_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03811-b31b1b.svg)](https://arxiv.org/abs/2309.03811) | :heavy_minus_sign: |
| Designing Phase Masks for Under-Display Cameras | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Designing_Phase_Masks_for_Under-Display_Cameras_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Xlzl3sQ9W0w) |
| Deep Optics for Video Snapshot Compressive Imaging | [![GitHub](https://img.shields.io/github/stars/pwangcs/DeepOpticsSCI)](https://github.com/pwangcs/DeepOpticsSCI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Deep_Optics_for_Video_Snapshot_Compressive_Imaging_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| TiDy-PSFs: Computational Imaging with Time-Averaged Dynamic Point-Spread-Functions | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shah_TiDy-PSFs_Computational_Imaging_with_Time-Averaged_Dynamic_Point-Spread-Functions_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17583-b31b1b.svg)](https://arxiv.org/abs/2303.17583) | :heavy_minus_sign: |
| Generalized Lightness Adaptation with Channel Selective Normalization | [![GitHub](https://img.shields.io/github/stars/mdyao/CSNorm)](https://github.com/mdyao/CSNorm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yao_Generalized_Lightness_Adaptation_with_Channel_Selective_Normalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13783-b31b1b.svg)](https://arxiv.org/abs/2308.13783) | :heavy_minus_sign: |
| Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://delinqu.github.io/QRSC/) <br /> [![GitHub](https://img.shields.io/github/stars/DelinQu/qrsc)](https://github.com/DelinQu/qrsc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Qu_Towards_Nonlinear-Motion-Aware_and_Occlusion-Robust_Rolling_Shutter_Correction_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.18125-b31b1b.svg)](https://arxiv.org/abs/2303.18125) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Or-yvKHUrZ0) |
| FCCNs: Fully Complex-Valued Convolutional Networks using Complex-Valued Color Model and Loss Function | [![GitHub](https://img.shields.io/github/stars/saurabhya/FCCNs)](https://github.com/saurabhya/FCCNs) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yadav_FCCNs_Fully_Complex-valued_Convolutional_Networks_using_Complex-valued_Color_Model_and_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Event Camera Data Pre-Training | [![GitHub](https://img.shields.io/github/stars/Yan98/Event-Camera-Data-Pre-training)](https://github.com/Yan98/Event-Camera-Data-Pre-training) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Event_Camera_Data_Pre-training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01928-b31b1b.svg)](https://arxiv.org/abs/2301.01928) | :heavy_minus_sign: |
| Improving 3D Imaging with Pre-Trained Perpendicular 2D Diffusion Models | [![GitHub](https://img.shields.io/github/stars/hyn2028/tpdm)](https://github.com/hyn2028/tpdm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Improving_3D_Imaging_with_Pre-Trained_Perpendicular_2D_Diffusion_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.08440-b31b1b.svg)](https://arxiv.org/abs/2303.08440) | :heavy_minus_sign: |
| Multiscale Structure Guided Diffusion for Image Deblurring | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ren_Multiscale_Structure_Guided_Diffusion_for_Image_Deblurring_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.01789-b31b1b.svg)](https://arxiv.org/abs/2212.01789) | :heavy_minus_sign: |
| Generalizing Event-based Motion Deblurring in Real-World Scenarios | [![GitHub](https://img.shields.io/github/stars/XiangZ-0/GEM)](https://github.com/XiangZ-0/GEM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Generalizing_Event-Based_Motion_Deblurring_in_Real-World_Scenarios_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05932-b31b1b.svg)](https://arxiv.org/abs/2308.05932) | [![OneDrive](https://img.shields.io/badge/OneDrive-0078D4.svg?style=for-the-badge&logo=microsoftonedrive&logoColor=white)](https://photos.onedrive.com/share/DE821E161E64CE08!2223?cid=DE821E161E64CE08&resId=DE821E161E64CE08!2223&authkey=!ALrfDWQod8KYAkc&ithint=video&e=KGNdnb) |
| On the Robustness of Normalizing Flows for Inverse Problems in Imaging | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hong_On_the_Robustness_of_Normalizing_Flows_for_Inverse_Problems_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.04319-b31b1b.svg)](https://arxiv.org/abs/2212.04319) | :heavy_minus_sign: |
| Learned Compressive Representations for Single-Photon 3D Imaging | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gutierrez-Barragan_Learned_Compressive_Representations_for_Single-Photon_3D_Imaging_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=E71rYGHFEYQ) |
| Recovering a Molecule's 3D Dynamics from Liquid-Phase Electron Microscopy Movies | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Recovering_a_Molecules_3D_Dynamics_from_Liquid-phase_Electron_Microscopy_Movies_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11927-b31b1b.svg)](https://arxiv.org/abs/2308.11927) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=7KllsPLHwDc) |
| NIR-Assisted Video Enhancement via Unpaired 24-Hour Data | [![GitHub](https://img.shields.io/github/stars/MyNiuuu/NVEU)](https://github.com/MyNiuuu/NVEU) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Niu_NIR-assisted_Video_Enhancement_via_Unpaired_24-hour_Data_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| SpinCam: High-Speed Imaging via a Rotating Point-Spread Function | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chan_SpinCam_High-Speed_Imaging_via_a_Rotating_Point-Spread_Function_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| RecRecNet: Rectangling Rectified Wide-Angle Images by Thin-Plate Spline Model and DoF-based Curriculum Learning | [![GitHub](https://img.shields.io/github/stars/KangLiao929/RecRecNet)](https://github.com/KangLiao929/RecRecNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liao_RecRecNet_Rectangling_Rectified_Wide-Angle_Images_by_Thin-Plate_Spline_Model_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2301.01661-b31b1b.svg)](https://arxiv.org/abs/2301.01661) | :heavy_minus_sign: |
| Affective Image Filter: Reflecting Emotions from Text to Images | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Weng_Affective_Image_Filter_Reflecting_Emotions_from_Text_to_Images_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Towards General Low-Light Raw Noise Synthesis and Modeling | [![GitHub](https://img.shields.io/github/stars/fengzhang427/LRD)](https://github.com/fengzhang427/LRD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Towards_General_Low-Light_Raw_Noise_Synthesis_and_Modeling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.16508-b31b1b.svg)](https://arxiv.org/abs/2307.16508) | :heavy_minus_sign: |
| Unsupervised Video Deraining with an Event Camera | [![GitHub](https://img.shields.io/github/stars/booker-max/Unsupervised-Deraining-with-Event-Camera)](https://github.com/booker-max/Unsupervised-Deraining-with-Event-Camera) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Unsupervised_Video_Deraining_with_An_Event_Camera_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_LoLep_Single-View_View_Synthesis_with_Locally-Learned_Planes_and_Self-Attention_Occlusion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12217-b31b1b.svg)](https://arxiv.org/abs/2307.12217) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Embodied Vision: Active Agents; Simulation

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Skill Transformer: A Monolithic Policy for Mobile Manipulation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_Skill_Transformer_A_Monolithic_Policy_for_Mobile_Manipulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09873-b31b1b.svg)](https://arxiv.org/abs/2308.09873) | :heavy_minus_sign: |
| ENTL: Embodied Navigation Trajectory Learner | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kotar_ENTL_Embodied_Navigation_Trajectory_Learner_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.02639-b31b1b.svg)](https://arxiv.org/abs/2304.02639) | :heavy_minus_sign: |
| DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation | [![GitHub](https://img.shields.io/github/stars/hanqingwangai/Dreamwalker)](https://github.com/hanqingwangai/Dreamwalker) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DREAMWALKER_Mental_Planning_for_Continuous_Vision-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07498-b31b1b.svg)](https://arxiv.org/abs/2308.07498) | :heavy_minus_sign: |
| Scene Graph Contrastive Learning for Embodied Navigation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Scene_Graph_Contrastive_Learning_for_Embodied_Navigation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Perpetual Humanoid Control for Real-Time Simulated Avatars | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zhengyiluo.github.io/PHC/) <br /> [![GitHub](https://img.shields.io/github/stars/DelinQu/qrsc)](https://github.com/DelinQu/qrsc) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_Perpetual_Humanoid_Control_for_Real-time_Simulated_Avatars_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2305.06456-b31b1b.svg)](https://arxiv.org/abs/2305.06456) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zS6Y00EW37A) |
| Grounding 3D Object Affordance from 2D Interactions in Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://yyvhang.github.io/publications/IAG/index.html) <br /> [![GitHub](https://img.shields.io/github/stars/yyvhang/IAGNet)](https://github.com/yyvhang/IAGNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Grounding_3D_Object_Affordance_from_2D_Interactions_in_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10437-b31b1b.svg)](https://arxiv.org/abs/2303.10437) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=GfCPUM1nAHI) |
| Navigating to Objects Specified by Images | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://jacobkrantz.github.io/modular_iin) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Krantz_Navigating_to_Objects_Specified_by_Images_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.01192-b31b1b.svg)](https://arxiv.org/abs/2304.01192) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=273jjBvu48s) |
| PEANUT: Predicting and Navigating to Unseen Targets | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ajzhai.github.io/PEANUT/) <br /> [![GitHub](https://img.shields.io/github/stars/ajzhai/PEANUT)](https://github.com/ajzhai/PEANUT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhai_PEANUT_Predicting_and_Navigating_to_Unseen_Targets_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.02497-b31b1b.svg)](https://arxiv.org/abs/2212.02497) | :heavy_minus_sign: |
| Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_Context-Aware_Planning_and_Environment-Aware_Memory_for_Instruction_Following_Embodied_Agents_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07241-b31b1b.svg)](https://arxiv.org/abs/2308.07241) | :heavy_minus_sign: |
| Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://hyperplane-lab.github.io/DeformableAffordance/) <br /> [![GitHub](https://img.shields.io/github/stars/TritiumR/DeformableAffordance)](https://github.com/TritiumR/DeformableAffordance) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Foresightful_Dense_Visual_Affordance_for_Deformable_Object_Manipulation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.11057-b31b1b.svg)](https://arxiv.org/abs/2303.11057) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=aYneBzwhOGs) |
| Exploiting Proximity-Aware Tasks for Embodied Social Navigation | [![GitHub](https://img.shields.io/github/stars/EnricoCancelli/ProximitySocialNav)](https://github.com/EnricoCancelli/ProximitySocialNav) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cancelli_Exploiting_Proximity-Aware_Tasks_for_Embodied_Social_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2212.00767-b31b1b.svg)](https://arxiv.org/abs/2212.00767) | :heavy_minus_sign: |
| Bird's-Eye-View Scene Graph for Vision-Language Navigation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Birds-Eye-View_Scene_Graph_for_Vision-Language_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04758-b31b1b.svg)](https://arxiv.org/abs/2308.04758) | :heavy_minus_sign: |
| Active Neural Mapping | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yan_Active_Neural_Mapping_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.16246-b31b1b.svg)](https://arxiv.org/abs/2308.16246) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=psPvanfh7SA) |
| Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation | [![GitHub](https://img.shields.io/github/stars/chenjinyubuaa/ORAN)](https://github.com/chenjinyubuaa/ORAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Omnidirectional_Information_Gathering_for_Knowledge_Transfer-Based_Audio-Visual_Navigation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10306-b31b1b.svg)](https://arxiv.org/abs/2308.10306) | :heavy_minus_sign: |
| Multi-Object Navigation with Dynamically Learned Neural Implicit Representations | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://pierremarza.github.io/projects/dynamic_implicit_representations/) <br /> [![GitHub](https://img.shields.io/github/stars/PierreMarza/dynamic_implicit_representations)](https://github.com/PierreMarza/dynamic_implicit_representations) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Marza_Multi-Object_Navigation_with_Dynamically_Learned_Neural_Implicit_Representations_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.05129-b31b1b.svg)](https://arxiv.org/abs/2210.05129) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=r_F9M80GPUI) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Recognition: Retrieval

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unsupervised Feature Representation Learning for Domain-Generalized Cross-Domain Image Retrieval | [![GitHub](https://img.shields.io/github/stars/conghui1002/DG-UCDIR)](https://github.com/conghui1002/DG-UCDIR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Unsupervised_Feature_Representation_Learning_for_Domain-generalized_Cross-domain_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DEDRIFT: Robust Similarity Search under Content Drift | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Baranchuk_DEDRIFT_Robust_Similarity_Search_under_Content_Drift_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.02752-b31b1b.svg)](https://arxiv.org/abs/2308.02752) | :heavy_minus_sign: |
| Global Features are All You Need for Image Retrieval and Reranking | [![GitHub](https://img.shields.io/github/stars/ShihaoShao-GH/SuperGlobal)](https://github.com/ShihaoShao-GH/SuperGlobal) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Global_Features_are_All_You_Need_for_Image_Retrieval_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06954-b31b1b.svg)](https://arxiv.org/abs/2308.06954) | :heavy_minus_sign: |
| HSE: Hybrid Species Embedding for Deep Metric Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_HSE_Hybrid_Species_Embedding_for_Deep_Metric_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Discrepant and Multi-Instance Proxies for Unsupervised Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zou_Discrepant_and_Multi-Instance_Proxies_for_Unsupervised_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Towards Grand Unified Representation Learning for Unsupervised Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/yangbincv/GUR)](https://github.com/yangbincv/GUR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Towards_Grand_Unified_Representation_Learning_for_Unsupervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition | [![GitHub](https://img.shields.io/github/stars/gmberton/EigenPlaces)](https://github.com/gmberton/EigenPlaces) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Berton_EigenPlaces_Training_Viewpoint_Robust_Models_for_Visual_Place_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10832-b31b1b.svg)](https://arxiv.org/abs/2308.10832) | :heavy_minus_sign: |
| Simple Baselines for Interactive Video Retrieval with Questions and Answers | [![GitHub](https://img.shields.io/github/stars/kevinliang888/IVR-QA-baselines)](https://github.com/kevinliang888/IVR-QA-baselines) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Simple_Baselines_for_Interactive_Video_Retrieval_with_Questions_and_Answers_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10402-b31b1b.svg)](https://arxiv.org/abs/2308.10402) | :heavy_minus_sign: |
| Fan-Beam Binarization Difference Projection (FB-BDP): A Novel Local Object Descriptor for Fine-Grained Leaf Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Fan-Beam_Binarization_Difference_Projection_FB-BDP_A_Novel_Local_Object_Descriptor_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Song_Conditional_Cross_Attention_Network_for_Multi-Space_Embedding_without_Entanglement_in_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.13254-b31b1b.svg)](https://arxiv.org/abs/2307.13254) | :heavy_minus_sign: |
| Learning Concordant Attention via Target-Aware Alignment for Visible-Infrared Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_Learning_Concordant_Attention_via_Target-aware_Alignment_for_Visible-Infrared_Person_Re-identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Person Re-Identification without Identification via Event Anonymization | [![GitHub](https://img.shields.io/github/stars/IIT-PAVIS/ReId_without_Id)](https://github.com/IIT-PAVIS/ReId_without_Id) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ahmad_Person_Re-Identification_without_Identification_via_Event_anonymization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04402-b31b1b.svg)](https://arxiv.org/abs/2308.04402) | :heavy_minus_sign: |
| Divide&Classify: Fine-Grained Classification for City-Wide Visual Geo-Localization | [![GitHub](https://img.shields.io/github/stars/ga1i13o/Divide-and-Classify)](https://github.com/ga1i13o/Divide-and-Classify) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Trivigno_DivideClassify_Fine-Grained_Classification_for_City-Wide_Visual_Geo-Localization_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Dark Side Augmentation: Generating Diverse Night Examples for Metric Learning | [![GitHub](https://img.shields.io/github/stars/mohwald/gandtr)](https://github.com/mohwald/gandtr) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Mohwald_Dark_Side_Augmentation_Generating_Diverse_Night_Examples_for_Metric_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16351-b31b1b.svg)](https://arxiv.org/abs/2309.16351) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=zlT-GJOcgYw) |
| PIDRo: Parallel Isomeric Attention with Dynamic Routing for Text-Video Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_PIDRo_Parallel_Isomeric_Attention_with_Dynamic_Routing_for_Text-Video_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Unified Pre-Training with Pseudo Texts for Text-to-Image Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/ZhiyinShao-H/UniPT)](https://github.com/ZhiyinShao-H/UniPT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shao_Unified_Pre-Training_with_Pseudo_Texts_for_Text-To-Image_Person_Re-Identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01420-b31b1b.svg)](https://arxiv.org/abs/2309.01420) | :heavy_minus_sign: |
| Modality Unifying Network for Visible-Infrared Person Re-Identification | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yu_Modality_Unifying_Network_for_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.06262-b31b1b.svg)](https://arxiv.org/abs/2309.06262) | :heavy_minus_sign: |
| DeepChange: A Long-Term Person Re-Identification Benchmark with Clothes Change | [![GitHub](https://img.shields.io/github/stars/PengBoXiangShang/deepchange)](https://github.com/PengBoXiangShang/deepchange) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_DeepChange_A_Long-Term_Person_Re-Identification_Benchmark_with_Clothes_Change_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| LexLIP: Lexicon-Bottlenecked Language-Image Pre-Training for Large-Scale Image-Text Sparse Retrieval | [![GitHub](https://img.shields.io/github/stars/ChiYeungLaw/LexLIP-ICCV23)](https://github.com/ChiYeungLaw/LexLIP-ICCV23) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Luo_LexLIP_Lexicon-Bottlenecked_Language-Image_Pre-Training_for_Large-Scale_Image-Text_Sparse_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2302.02908-b31b1b.svg)](https://arxiv.org/abs/2302.02908) | :heavy_minus_sign: |
| Dual Pseudo-Labels Interactive Self-Training for Semi-Supervised Visible-Infrared Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/XiangboYin/DPIS_USVLReID)](https://github.com/XiangboYin/DPIS_USVLReID) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Shi_Dual_Pseudo-Labels_Interactive_Self-Training_for_Semi-Supervised_Visible-Infrared_Person_Re-Identification_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| BT<sup>2</sup>: Backward-Compatible Training with Basis Transformation | [![GitHub](https://img.shields.io/github/stars/YifeiZhou02/BT-2)](https://github.com/YifeiZhou02/BT-2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_BT2_Backward-compatible_Training_with_Basis_Transformation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.03989-b31b1b.svg)](https://arxiv.org/abs/2211.03989) | :heavy_minus_sign: |
| Prototypical Mixing and Retrieval-based Refinement for Label Noise-Resistant Image Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Prototypical_Mixing_and_Retrieval-Based_Refinement_for_Label_Noise-Resistant_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Learning Spatial-Context-Aware Global Visual Feature Representation for Instance Image Retrieval | [![GitHub](https://img.shields.io/github/stars/Zy-Zhang/SpCa)](https://github.com/Zy-Zhang/SpCa) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Learning_Spatial-context-aware_Global_Visual_Feature_Representation_for_Instance_Image_Retrieval_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval | [![GitHub](https://img.shields.io/github/stars/bassyess/CFCD)](https://github.com/bassyess/CFCD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_Coarse-to-Fine_Learning_Compact_Discriminative_Representation_for_Single-Stage_Image_Retrieval_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04008-b31b1b.svg)](https://arxiv.org/abs/2308.04008) | :heavy_minus_sign: |
| Visible-Infrared Person Re-Identification via Semantic Alignment and Affinity Inference | [![GitHub](https://img.shields.io/github/stars/xiaoye-hhh/SAAI)](https://github.com/xiaoye-hhh/SAAI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fang_Visible-Infrared_Person_Re-Identification_via_Semantic_Alignment_and_Affinity_Inference_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Part-Aware Transformer for Generalizable Person Re-Identification | [![GitHub](https://img.shields.io/github/stars/liyuke65535/Part-Aware-Transformer)](https://github.com/liyuke65535/Part-Aware-Transformer) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ni_Part-Aware_Transformer_for_Generalizable_Person_Re-identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03322-b31b1b.svg)](https://arxiv.org/abs/2308.03322) | :heavy_minus_sign: |
| Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cmp.felk.cvut.cz/univ_emb/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ypsilantis_Towards_Universal_Image_Embeddings_A_Large-Scale_Dataset_and_Challenge_for_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.01858-b31b1b.svg)](https://arxiv.org/abs/2309.01858) | :heavy_minus_sign: |
| Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Dual_Learning_with_Dynamic_Knowledge_Distillation_for_Partially_Relevant_Video_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Fine-Grained Unsupervised Domain Adaptation for Gait Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Ma_Fine-grained_Unsupervised_Domain_Adaptation_for_Gait_Recognition_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TsWfYqz8qbk) |
| FashionNTM: Multi-Turn Fashion Image Retrieval via Cascaded Memory | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://sites.google.com/eng.ucsd.edu/fashionntm) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Pal_FashionNTM_Multi-turn_Fashion_Image_Retrieval_via_Cascaded_Memory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.10170-b31b1b.svg)](https://arxiv.org/abs/2308.10170) | :heavy_minus_sign: |
| CrossLoc3D: Aerial-Ground Cross-Source 3D Place Recognition | [![GitHub](https://img.shields.io/github/stars/rayguan97/crossloc3d)](https://github.com/rayguan97/crossloc3d) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guan_CrossLoc3D_Aerial-Ground_Cross-Source_3D_Place_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.17778-b31b1b.svg)](https://arxiv.org/abs/2303.17778) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, Continual, Long-Tail Learning

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition | [![GitHub](https://img.shields.io/github/stars/cool-xuan/Imbalanced_SAM)](https://github.com/cool-xuan/Imbalanced_SAM) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_ImbSAM_A_Closer_Look_at_Sharpness-Aware_Minimization_in_Class-Imbalanced_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07815-b31b1b.svg)](https://arxiv.org/abs/2308.07815) | :heavy_minus_sign: |
| LFS-GAN: Lifelong Few-Shot Image Generation | [![GitHub](https://img.shields.io/github/stars/JJuOn/LFS-GAN)](https://github.com/JJuOn/LFS-GAN) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Seo_LFS-GAN_Lifelong_Few-Shot_Image_Generation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11917-b31b1b.svg)](https://arxiv.org/abs/2308.11917) | :heavy_minus_sign: |
| Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection | [![GitHub](https://img.shields.io/github/stars/YuyangSunshine/ABR_IOD)](https://github.com/YuyangSunshine/ABR_IOD) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_Augmented_Box_Replay_Overcoming_Foreground_Shift_for_Incremental_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12427-b31b1b.svg)](https://arxiv.org/abs/2307.12427) | :heavy_minus_sign: |
| Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/brdav/cma)](https://github.com/brdav/cma) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bruggemann_Contrastive_Model_Adaptation_for_Cross-Condition_Robustness_in_Semantic_Segmentation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.05194-b31b1b.svg)](https://arxiv.org/abs/2303.05194) | :heavy_minus_sign: |
| Towards Effective Instance Discrimination Contrastive Loss for Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/zhyx12/EIDCo)](https://github.com/zhyx12/EIDCo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Towards_Effective_Instance_Discrimination_Contrastive_Loss_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2202.02802-b31b1b.svg)](https://arxiv.org/abs/2202.02802) | :heavy_minus_sign: |
| Adversarial Bayesian Augmentation for Single-Source Domain Generalization | [![GitHub](https://img.shields.io/github/stars/shengcheng/ABA)](https://github.com/shengcheng/ABA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Adversarial_Bayesian_Augmentation_for_Single-Source_Domain_Generalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.09520-b31b1b.svg)](https://arxiv.org/abs/2307.09520) | :heavy_minus_sign: |
| Measuring Asymmetric Gradient Discrepancy in Parallel Continual Learning | [![GitHub](https://img.shields.io/github/stars/fanlyu/maxdo)](https://github.com/fanlyu/maxdo) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lyu_Measuring_Asymmetric_Gradient_Discrepancy_in_Parallel_Continual_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| CSDA: Learning Category-Scale Joint Feature for Domain Adaptive Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_CSDA_Learning_Category-Scale_Joint_Feature_for_Domain_Adaptive_Object_Detection_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Distilling from Similar Tasks for Transfer Learning on a Budget | [![GitHub](https://img.shields.io/github/stars/Kennethborup/DistillWeighted)](https://github.com/Kennethborup/DistillWeighted) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Borup_Distilling_from_Similar_Tasks_for_Transfer_Learning_on_a_Budget_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2304.12314-b31b1b.svg)](https://arxiv.org/abs/2304.12314) | :heavy_minus_sign: |
| Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Cho_Complementary_Domain_Adaptation_and_Generalization_for_Unsupervised_Continual_Domain_Shift_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15833-b31b1b.svg)](https://arxiv.org/abs/2303.15833) | :heavy_minus_sign: |
| Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-Identification | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://cvlab.yonsei.ac.kr/projects/CaCL/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Camera-Driven_Representation_Learning_for_Unsupervised_Domain_Adaptive_Person_Re-identification_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.11901-b31b1b.svg)](https://arxiv.org/abs/2308.11901) | :heavy_minus_sign: |
| Introducing Language Guidance in Prompt-based Continual Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Khan_Introducing_Language_Guidance_in_Prompt-based_Continual_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.15827-b31b1b.svg)](https://arxiv.org/abs/2308.15827) | :heavy_minus_sign: |
| Fast and Accurate Transferability Measurement by Evaluating Intra-Class Feature Variance | [![GitHub](https://img.shields.io/github/stars/snudatalab/TMI)](https://github.com/snudatalab/TMI) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Fast_and_Accurate_Transferability_Measurement_by_Evaluating_Intra-class_Feature_Variance_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.05986-b31b1b.svg)](https://arxiv.org/abs/2308.05986) | :heavy_minus_sign: |
| A Unified Continual Learning Framework with General Parameter-Efficient Tuning | [![GitHub](https://img.shields.io/github/stars/gqk/LAE)](https://github.com/gqk/LAE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_A_Unified_Continual_Learning_Framework_with_General_Parameter-Efficient_Tuning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.10070-b31b1b.svg)](https://arxiv.org/abs/2303.10070) | :heavy_minus_sign: |
| SFHarmony: Source Free Domain Adaptation for Distributed Neuroimaging Analysis | [![GitHub](https://img.shields.io/github/stars/nkdinsdale/SFHarmony)](https://github.com/nkdinsdale/SFHarmony) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dinsdale_SFHarmony_Source_Free_Domain_Adaptation_for_Distributed_Neuroimaging_Analysis_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.15965-b31b1b.svg)](https://arxiv.org/abs/2303.15965) | :heavy_minus_sign: |
| Towards Realistic Evaluation of Industrial Continual Learning Scenarios with an Emphasis on Energy Consumption and Computational Footprint | [![GitHub](https://img.shields.io/github/stars/Vivek9Chavan/RECIL)](https://github.com/Vivek9Chavan/RECIL) <br /> [![InVar](https://img.shields.io/badge/InVar-dataset-20BEFF.svg)](https://fordatis.fraunhofer.de/handle/fordatis/329.2) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chavan_Towards_Realistic_Evaluation_of_Industrial_Continual_Learning_Scenarios_with_an_ICCV_2023_paper.pdf) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=TsWfYqz8qbk) |
| CDAC: Cross-Domain Attention Consistency in Transformer for Domain Adaptive Semantic Segmentation | [![GitHub](https://img.shields.io/github/stars/wangkaihong/CDAC)](https://github.com/wangkaihong/CDAC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_CDAC_Cross-domain_Attention_Consistency_in_Transformer_for_Domain_Adaptive_Semantic_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.14703-b31b1b.svg)](https://arxiv.org/abs/2211.14703) | :heavy_minus_sign: |
| PC-Adapter: Topology-Aware Adapter for Efficient Domain Adaption on Point Clouds with Rectified Pseudo-Label | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Park_PC-Adapter_Topology-Aware_Adapter_for_Efficient_Domain_Adaption_on_Point_Clouds_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.16936-b31b1b.svg)](https://arxiv.org/abs/2309.16936) | :heavy_minus_sign: |
| DETA: Denoised Task Adaptation for Few-Shot Learning | [![GitHub](https://img.shields.io/github/stars/JimZAI/DETA)](https://github.com/JimZAI/DETA) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_DETA_Denoised_Task_Adaptation_for_Few-Shot_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06315-b31b1b.svg)](https://arxiv.org/abs/2303.06315) | :heavy_minus_sign: |
| Activate and Reject: Towards Safe Domain Generalization under Category Shift | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Activate_and_Reject_Towards_Safe_Domain_Generalization_under_Category_Shift_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization | [![GitHub](https://img.shields.io/github/stars/zzwdx/MEDIC)](https://github.com/zzwdx/MEDIC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Generalizable_Decision_Boundaries_Dualistic_Meta-Learning_for_Open_Set_Domain_Generalization_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09391-b31b1b.svg)](https://arxiv.org/abs/2308.09391) | :heavy_minus_sign: |
| Continual Zero-Shot Learning through Semantically Guided Generative Random Walks | [![GitHub](https://img.shields.io/github/stars/wx-zhang/IGCZSL)](https://github.com/wx-zhang/IGCZSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Continual_Zero-Shot_Learning_through_Semantically_Guided_Generative_Random_Walks_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.12366-b31b1b.svg)](https://arxiv.org/abs/2308.12366) | :heavy_minus_sign: |
| Zero-Shot Point Cloud Segmentation by Semantic-Visual Aware Synthesis | [![GitHub](https://img.shields.io/github/stars/leolyj/3DPC-GZSL)](https://github.com/leolyj/3DPC-GZSL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Zero-Shot_Point_Cloud_Segmentation_by_Semantic-Visual_Aware_Synthesis_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| MDCS: More Diverse Experts with Consistency Self-Distillation for Long-Tailed Recognition | [![GitHub](https://img.shields.io/github/stars/fistyee/MDCS)](https://github.com/fistyee/MDCS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhao_MDCS_More_Diverse_Experts_with_Consistency_Self-distillation_for_Long-tailed_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09922-b31b1b.svg)](https://arxiv.org/abs/2308.09922) | :heavy_minus_sign: |
| Building a Winning Team: Selecting Source Model Ensembles using a Submodular Transferability Estimation Approach | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/B_Building_a_Winning_Team_Selecting_Source_Model_Ensembles_using_a_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.02429-b31b1b.svg)](https://arxiv.org/abs/2309.02429) | :heavy_minus_sign: |
| Confidence-based Visual Dispersal for Few-Shot Unsupervised Domain Adaptation | [![GitHub](https://img.shields.io/github/stars/Bostoncake/C-VisDiT)](https://github.com/Bostoncake/C-VisDiT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xiong_Confidence-based_Visual_Dispersal_for_Few-shot_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.15575-b31b1b.svg)](https://arxiv.org/abs/2309.15575) | :heavy_minus_sign: |
| BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_BEV-DG_Cross-Modal_Learning_under_Birds-Eye_View_for_Domain_Generalization_of_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.06530-b31b1b.svg)](https://arxiv.org/abs/2308.06530) | :heavy_minus_sign: |
| CDFSL-V: Cross-Domain Few-Shot Learning for Videos | [![GitHub](https://img.shields.io/github/stars/Sarinda251/CDFSL-V)](https://github.com/Sarinda251/CDFSL-V) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Samarasinghe_CDFSL-V_Cross-Domain_Few-Shot_Learning_for_Videos_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.03989-b31b1b.svg)](https://arxiv.org/abs/2309.03989) | :heavy_minus_sign: |
| Energy-based Self-Training and Normalization for Unsupervised Domain Adaptation | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Herath_Energy-based_Self-Training_and_Normalization_for_Unsupervised_Domain_Adaptation_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-Trained Vision-Language Models | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://wuw2019.github.io/R-AMT/) <br /> [![GitHub](https://img.shields.io/github/stars/wuw2019/R-AMT)](https://github.com/wuw2019/R-AMT) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zheng_Regularized_Mask_Tuning_Uncovering_Hidden_Knowledge_in_Pre-Trained_Vision-Language_Models_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.15049-b31b1b.svg)](https://arxiv.org/abs/2307.15049) | :heavy_minus_sign: |
| NAPA-VQ: Neighborhood-Aware Prototype Augmentation with Vector Quantization for Continual Learning | [![GitHub](https://img.shields.io/github/stars/TamashaM/NAPA-VQ)](https://github.com/TamashaM/NAPA-VQ) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Malepathirana_NAPA-VQ_Neighborhood-Aware_Prototype_Augmentation_with_Vector_Quantization_for_Continual_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09297-b31b1b.svg)](https://arxiv.org/abs/2308.09297) | :heavy_minus_sign: |
| A Sentence Speaks a Thousand Images: Domain Generalization through Distilling CLIP with Language Guidance | [![GitHub](https://img.shields.io/github/stars/OoDBag/RISE)](https://github.com/OoDBag/RISE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Huang_A_Sentence_Speaks_a_Thousand_Images_Domain_Generalization_through_Distilling_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2309.12530-b31b1b.svg)](https://arxiv.org/abs/2309.12530) | :heavy_minus_sign: |
| ViM: Vision Middleware for Unified Downstream Transferring | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_ViM_Vision_Middleware_for_Unified_Downstream_Transferring_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06911-b31b1b.svg)](https://arxiv.org/abs/2303.06911) | :heavy_minus_sign: |
| Learning to Learn: How to Continuously Teach Humans and Machines | [![GitHub](https://img.shields.io/github/stars/ZhangLab-DeepNeuroCogLab/Learning2Learn)](https://github.com/ZhangLab-DeepNeuroCogLab/Learning2Learn) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Singh_Learning_to_Learn_How_to_Continuously_Teach_Humans_and_Machines_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2211.15470-b31b1b.svg)](https://arxiv.org/abs/2211.15470) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=xz1TSRAQCN4) |
| A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://vlislab22.github.io/CTCL/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_A_Good_Student_is_Cooperative_and_Reliable_CNN-Transformer_Collaborative_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.12574-b31b1b.svg)](https://arxiv.org/abs/2307.12574) | :heavy_minus_sign: |
| Online Class Incremental Learning on Stochastic Blurry Task Boundary via Mask and Visual Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/moonjunyyy/Si-Blurry)](https://github.com/moonjunyyy/Si-Blurry) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Moon_Online_Class_Incremental_Learning_on_Stochastic_Blurry_Task_Boundary_via_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09303-b31b1b.svg)](https://arxiv.org/abs/2308.09303) | :heavy_minus_sign: |
| Heterogeneous Forgetting Compensation for Class-Incremental Learning | [![GitHub](https://img.shields.io/github/stars/JiahuaDong/HFC)](https://github.com/JiahuaDong/HFC) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Dong_Heterogeneous_Forgetting_Compensation_for_Class-Incremental_Learning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03374-b31b1b.svg)](https://arxiv.org/abs/2308.03374) | :heavy_minus_sign: |
| Disposable Transfer Learning for Selective Source Task Unlearning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Koh_Disposable_Transfer_Learning_for_Selective_Source_Task_Unlearning_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09971-b31b1b.svg)](https://arxiv.org/abs/2308.09971) | :heavy_minus_sign: |
| Online Continual Learning on Hierarchical Label Expansion | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Lee_Online_Continual_Learning_on_Hierarchical_Label_Expansion_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14374-b31b1b.svg)](https://arxiv.org/abs/2308.14374) | :heavy_minus_sign: |
| Black-Box Unsupervised Domain Adaptation with Bi-Directional Atkinson-Shiffrin Memory | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhang_Black-Box_Unsupervised_Domain_Adaptation_with_Bi-Directional_Atkinson-Shiffrin_Memory_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13236-b31b1b.svg)](https://arxiv.org/abs/2308.13236) | :heavy_minus_sign: |
| Local and Global Logit Adjustments for Long-Tailed Learning | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Tao_Local_and_Global_Logit_Adjustments_for_Long-Tailed_Learning_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| FS-DETR: Few-Shot DEtection TRansformer with Prompting and without Re-Training | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Bulat_FS-DETR_Few-Shot_DEtection_TRansformer_with_Prompting_and_without_Re-Training_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2210.04845-b31b1b.svg)](https://arxiv.org/abs/2210.04845) | :heavy_minus_sign: |
| Tuning Pre-Trained Model via Moment Probing | [![GitHub](https://img.shields.io/github/stars/mingzeG/Moment-Probing)](https://github.com/mingzeG/Moment-Probing) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Gao_Tuning_Pre-trained_Model_via_Moment_Probing_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11342-b31b1b.svg)](https://arxiv.org/abs/2307.11342) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Low-Level and Physics-based Vision

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection | [![GitHub](https://img.shields.io/github/stars/xyfJASON/HCL)](https://github.com/xyfJASON/HCL) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Feng_Hierarchical_Contrastive_Learning_for_Pattern-Generalizable_Image_Corruption_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14061-b31b1b.svg)](https://arxiv.org/abs/2308.14061) | :heavy_minus_sign: |
| DDS2M: Self-Supervised Denoising Diffusion Spatio-Spectral Model for Hyperspectral Image Restoration | [![GitHub](https://img.shields.io/github/stars/miaoyuchun/DDS2M)](https://github.com/miaoyuchun/DDS2M) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Miao_DDS2M_Self-Supervised_Denoising_Diffusion_Spatio-Spectral_Model_for_Hyperspectral_Image_Restoration_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.06682-b31b1b.svg)](https://arxiv.org/abs/2303.06682) | :heavy_minus_sign: |
| From Sky to the Ground: A Large-Scale Benchmark and Simple Baseline Towards Real Rain Removal | [![GitHub](https://img.shields.io/github/stars/yunguo224/LHP-Rain)](https://github.com/yunguo224/LHP-Rain) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Guo_From_Sky_to_the_Ground_A_Large-scale_Benchmark_and_Simple_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.03867-b31b1b.svg)](https://arxiv.org/abs/2308.03867) | :heavy_minus_sign: |
| VAPCNet: Viewpoint-Aware 3D Point Cloud Completion | [![GitHub](https://img.shields.io/github/stars/FZH92128/VAPCNet)](https://github.com/FZH92128/VAPCNet) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Fu_VAPCNet_Viewpoint-Aware_3D_Point_Cloud_Completion_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| AccFlow: Backward Accumulation for Long-Range Optical Flow | [![GitHub](https://img.shields.io/github/stars/mulns/AccFlow)](https://github.com/mulns/AccFlow) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wu_AccFlow_Backward_Accumulation_for_Long-Range_Optical_Flow_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.13133-b31b1b.svg)](https://arxiv.org/abs/2308.13133) | :heavy_minus_sign: |
| Improving Transformer-based Image Matching by Cascaded Capturing Spatially Informative Keypoints |  |  |  |
| Low-Light Image Enhancement with Multi-Stage Residue Quantization and Brightness-Aware Attention |  |  |  |
| Random Sub-Samples Generation for Self-Supervised Real Image Denoising |  |  |  |
| RSFNet: A White-Box Image Retouching Approach using Region-Specific Color Filters |  |  |  |
| Physics-Driven Turbulence Image Restoration with Stochastic Refinement |  |  |  |
| SYENet: A Simple Yet Effective Network for Multiple Low-Level Vision Tasks with Real-Time Performance on Mobile Device |  |  |  |
| Self-supervised Image Denoising with Downsampled Invariance Loss and Conditional Blind-Spot Network |  |  |  |
| Variational Degeneration to Structural Refinement: A Unified Framework for Superimposed Image Decomposition |  |  |  |
| Reconstructed Convolution Module based Look-Up Tables for Efficient Image Super-Resolution |  |  |  |
| Self-Supervised Pre-Training for Mirror Detection |  |  |  |
| Downscaled Representation Matters: Improving Image Rescaling with Collaborative Downscaled Images |  |  |  |
| Self-Supervised Monocular Underwater Depth Recovery, Image Restoration, and a Real-Sea Video Dataset |  |  |  |
| Rethinking Video Frame Interpolation from Shutter mode Induced Degradation |  |  |  |
| Single Image Deblurring with Row-Dependent Blur Magnitude |  |  |  |
| Multi-View Self-Supervised Disentanglement for General Image Denoising |  |  |  |
| Joint Demosaicing and Deghosting of Time-Varying Exposures for Single-Shot HDR Imaging |  |  |  |
| Diff-Retinex: Rethinking Low-Light Image Enhancement with a Generative Diffusion Model |  |  |  |
| Dual Aggregation Transformer for Image Super-Resolution |  |  |  |
| Video Object Segmentation-Aware Video Frame Interpolation |  |  |  |
| RawHDR: High Dynamic Range Image Reconstruction from a Single Raw Image |  |  |  |
| Multi-Scale Residual Low-Pass Filter Network for Image Deblurring |  |  |  |
| Indoor Depth Recovery based on Deep Unfolding with Non-Local Prior |  |  |  |
| Learning Correction Filter via Degradation-Adaptive Regression for Blind Single Image Super-Resolution |  |  |  |
| Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution |  |  |  |
| Both Diverse and Realism Matter: Physical Attribute and Style Alignment for Rainy Image Generation |  |  |  |
| Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion |  |  |  |
| The Devil is in the Upsampling: Architectural Decisions Made Simpler for Denoising with Deep Image Prior |  |  |  |
| SimFIR: A Simple Framework for Fisheye Image Rectification with Self-Supervised Representation Learning |  |  |  |
| Exploring Temporal Frequency Spectrum in Deep Video Deblurring |  |  |  |
| ExposureDiffusion: Learning to Expose for Low-Light Image Enhancement |  |  |  |
| High-Resolution Document Shadow Removal via a Large-Scale Real-World Dataset and a Frequency-Aware Shadow Erasing Net | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://cxh-research.github.io/DocShadow-SD7K/) <br /> [![GitHub](https://img.shields.io/github/stars/CXH-Research/DocShadow-SD7K)](https://github.com/CXH-Research/DocShadow-SD7K) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Li_High-Resolution_Document_Shadow_Removal_via_A_Large-Scale_Real-World_Dataset_and_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.14221-b31b1b.svg)](https://arxiv.org/abs/2308.14221) | :heavy_minus_sign: |
| Towards Saner Deep Image Registration |  |  |  |
| VideoFlow: Exploiting Temporal Cues for Multi-Frame Optical Flow Estimation |  |  |  |
| Scene Matters: Model-based Deep Video Compression |  |  |  |
| Non-Coaxial Event-Guided Motion Deblurring with Spatial Alignment |  |  |  |
| Retinexformer: One-Stage Retinex-based Transformer for Low-Light Image Enhancement |  |  |  |
| Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution |  |  |  |
| MVPSNet: Fast Generalizable Multi-View Photometric Stereo |  |  |  |
| FSI: Frequency and Spatial Interactive Learning for Image Restoration in Under-Display Cameras |  |  |  |
| Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution |  |  |  |
| Empowering Low-Light Image Enhancer through Customized Learnable Priors |  |  |  |
| Learning Image Harmonization in the Linear Color Space |  |  |  |
| Under-Display Camera Image Restoration with Scattering Effect |  |  |  |
| Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution | [![GitHub](https://img.shields.io/github/stars/Jiamian-Wang/Iterative-Soft-Shrinkage-SR)](https://github.com/Jiamian-Wang/Iterative-Soft-Shrinkage-SR) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_Iterative_Soft_Shrinkage_Learning_for_Efficient_Image_Super-Resolution_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.09650-b31b1b.svg)](https://arxiv.org/abs/2303.09650) | :heavy_minus_sign: |
| Single Image Defocus Deblurring via Implicit Neural Inverse Kernels |  |  |  |
| Degradation-Resistant Unfolding Network for Heterogeneous Image Fusion |  |  |  |
| Graphics2RAW: Mapping Computer Graphics Images to Sensor RAW Images |  |  |  |
| Lighting up NeRF via Unsupervised Decomposition and Enhancement |  |  |  |
| Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches |  |  |  |
| Adverse Weather Removal with Codebook Priors |  |  |  |
| MSRA-SR: Image Super-Resolution Transformer with Multi-Scale Shared Representation Acquisition |  |  |  |
| Deep Video Demoiréing via Compact Invertible Dyadic Decomposition |  |  |  |
| SILT: Shadow-Aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels |  |  |  |
| Innovating Real Fisheye Image Correction with Dual Diffusion Architecture |  |  |  |
| Adaptive Illumination Mapping for Shadow Detection in Raw Images |  |  |  |
| GEDepth: Ground Embedding for Monocular Depth Estimation |  |  |  |
| Lightweight Image Super-Resolution with Superpixel Token Interaction |  |  |  |
| Unfolding Framework with Prior of Convolution-Transformer Mixture and Uncertainty Estimation for Video Snapshot Compressive Imaging |  |  |  |
| Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors |  |  |  |
| LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction |  |  |  |
| Fine-Grained Visible Watermark Removal |  |  |  |
| SRFormer: Permuted Self-Attention for Single Image Super-Resolution |  |  |  |
| DLGSANet: Lightweight Dynamic Local and Global Self-Attention Networks for Image Super-Resolution |  |  |  |
| MB-TaylorFormer: Multi-Branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing |  |  |  |
| Multi-Frequency Representation Enhancement with Privilege Information for Video Super-Resolution |  |  |  |
| COMPASS: High-Efficiency Deep Image Compression with Arbitrary-Scale Spatial Scalability |  |  |  |
| Alignment-Free HDR Deghosting with Semantics Consistent Transformer |  |  |  |
| From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection |  |  |  |
| Towards High-Quality Specular Highlight Removal by Leveraging Large-Scale Synthetic Data |  |  |  |
| DynamicISP: Dynamically Controlled Image Signal Processor for Image Recognition |  |  |  |
| Dancing in the Dark: A Benchmark towards General Low-Light Video Enhancement |  |  |  |
| Dec-Adapter: Exploring Efficient Decoder-Side Adapter for Bridging Screen Content and Natural Image Compression |  |  |  |
| OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution |  |  |  |
| Pyramid Dual Domain Injection Network for Pan-Sharpening |  |  |  |
| Implicit Neural Representation for Cooperative Low-Light Image Enhancement |  |  |  |
| Physically-Plausible Illumination Distribution Estimation |  |  |  |
| Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising |  |  |  |
| Semantic-Aware Dynamic Parameter for Video Inpainting Transformer |  |  |  |
| Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction |  |  |  |
| Improving Lens Flare Removal with General-Purpose Pipeline and Multiple Light Sources Recovery |  |  |  |
| RFD-ECNet: Extreme Underwater Image Compression with Reference to Feature Dictionary |  |  |  |
| Learning Continuous Exposure Value Representations for Single-Image HDR Reconstruction |  |  |  |
| Focal Network for Image Restoration |  |  |  |
| CIRI: Curricular Inactivation for Residue-Aware One-Shot Video Inpainting |  |  |  |
| Beyond Image Borders: Learning Feature Extrapolation for Unbounded Image Composition |  |  |  |
| MetaF2N: Blind Image Super-Resolution by Learning Efficient Model Adaptation from Faces |  |  |  |
| Boundary-Aware Divide and Conquer: A Diffusion-based Solution for Unsupervised Shadow Removal |  |  |  |
| Leveraging Inpainting for Single-Image Shadow Removal |  |  |  |
| Hybrid Spectral Denoising Transformer with Guided Attention |  |  |  |
| Examining Autoexposure for Challenging Scenes |  |  |  |
| Self-Supervised Learning to Bring Dual Reversed Rolling Shutter Images Alive |  |  |  |
| DiffIR: Efficient Diffusion Model for Image Restoration |  |  |  |
| Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks |  |  |  |
| LMR: A Large-Scale Multi-Reference Dataset for Reference-based Super-Resolution |  |  |  |
| Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network |  |  |  |
| Single Image Reflection Separation via Component Synergy |  |  |  |
| Learning Rain Location Prior for Nighttime Deraining |  |  |  |
| Exploring Positional Characteristics of Dual-Pixel Data for Camera Autofocus |  |  |  |
| Continuously Masked Transformer for Image Inpainting |  |  |  |
| Learning Data-Driven Vector-Quantized Degradation Model for Animation Video Super-Resolution |  |  |  |
| Spatially-Adaptive Feature Modulation for Efficient Image Super-Resolution |  |  |  |
| Video Adverse-Weather-Component Suppression Network via Weather Messenger and Adversarial Backpropagation |  |  |  |
| Snow Removal in Video: a New Dataset and a Novel Method |  |  |  |
| Boosting Single Image Super-Resolution via Partial Channel Shifting |  |  |  |
| Towards Real-World Burst Image Super-Resolution: Benchmark and Method |  |  |  |
| On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement |  |  |  |
| E2NeRF: Event Enhanced Neural Radiance Fields from Blurry Images |  |  |  |
| Iterative Denoiser and Noise Estimator for Self-Supervised Image Denoising |  |  |  |
| Lighting Every Darkness in Two Pairs: a Calibration-Free Pipeline for RAW Denoising |  |  |  |
| Fingerprinting Deep Image Restoration Models |  |  |  |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Computer Vision Theory

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Femtodet: An Object Detection Baseline for Energy Versus Performance Tradeoffs | [![GitHub](https://img.shields.io/github/stars/yh-pengtu/FemtoDet)](https://github.com/yh-pengtu/FemtoDet) | [![arXiv](https://img.shields.io/badge/arXiv-2301.06719-b31b1b.svg)](https://arxiv.org/abs/2301.06719) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Video Analysis and Understanding

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Long-range Multimodal Pretraining for Movie Understanding | [![GitHub](https://img.shields.io/github/stars/dawitmureja/LMP)](https://github.com/dawitmureja/LMP) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Argaw_Long-range_Multimodal_Pretraining_for_Movie_Understanding_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.09775-b31b1b.svg)](https://arxiv.org/pdf/2308.09775.pdf) | :heavy_minus_sign: |
| Cross-view Semantic Alignment for Livestreaming Product Recognition | [![GitHub](https://img.shields.io/github/stars/adxcreative/RICE)](https://github.com/adxcreative/RICE) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Yang_Cross-view_Semantic_Alignment_for_Livestreaming_Product_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.04912-b31b1b.svg)](https://arxiv.org/abs/2308.04912) | :heavy_minus_sign: |
| HTML: Hybrid Temporal-scale Multimodal Learning Framework for Referring Video Object Segmentation | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://mingfei.info/HTML/) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Han_HTML_Hybrid_Temporal-scale_Multimodal_Learning_Framework_for_Referring_Video_Object_ICCV_2023_paper.pdf) | :heavy_minus_sign: |
| DyGait: Exploiting Dynamic Representations for High-performance Gait Recognition | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Wang_DyGait_Exploiting_Dynamic_Representations_for_High-performance_Gait_Recognition_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2303.14953-b31b1b.svg)](https://arxiv.org/abs/2303.14953)| :heavy_minus_sign: |
| Identity-Consistent Aggregation for Video Object Detection | :heavy_minus_sign: | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Deng_Identity-Consistent_Aggregation_for_Video_Object_Detection_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2308.07737-b31b1b.svg)](https://arxiv.org/abs/2308.07737)| :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Object Pose Estimation and Tracking

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### 3D Shape Modeling and Processing

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| EPiC: Ensemble of Partial Point Clouds for Robust Classification | [![GitHub](https://img.shields.io/github/stars/yossilevii100/EPiC)](https://github.com/yossilevii100/EPiC) <br /> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/gist/yossilevii100/ce4cae6c26e7c3d3358c4fb7dda3bec4/untitled5.ipynb) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11419-b31b1b.svg)](https://arxiv.org/abs/2303.11419) | :heavy_minus_sign: |
| Neural Implicit Surface Evolution | [![GitHub](https://img.shields.io/github/stars/dsilvavinicius/nise)](https://github.com/dsilvavinicius/nise) | [![arXiv](https://img.shields.io/badge/arXiv-2201.09636-b31b1b.svg)](https://arxiv.org/abs/2201.09636) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8NqwLkhaRBU) |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human Pose/Shape Estimation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Transfer, Low-Shot, and Continual Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, and Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, Meta-, Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Photogrammetry and Remote Sensing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Efficient and Scalable Vision

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| AdaNIC: Towards Practical Neural Image Compression via Dynamic Transform Routing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Vision Transformers for MobileNet Size and Speed | [![GitHub](https://img.shields.io/github/stars/snap-research/EfficientFormer)](https://github.com/snap-research/EfficientFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2212.08059-b31b1b.svg)](https://arxiv.org/abs/2212.08059) | :heavy_minus_sign: |
| DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.04383) | :heavy_minus_sign: |
| Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.13494-b31b1b.svg)](https://arxiv.org/abs/2308.13494) | :heavy_minus_sign: |
| Inherent Redundancy in Spiking Neural Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.08227-b31b1b.svg)](https://arxiv.org/abs/2308.08227) | :heavy_minus_sign: |
| Achievement-based Training Progress Balancing for Multi-Task Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04549-b31b1b.svg)](https://arxiv.org/abs/2308.04549) | :heavy_minus_sign: |
| Differentiable Transportation Pruning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08483-b31b1b.svg)](https://arxiv.org/abs/2307.08483) | :heavy_minus_sign: |
| XiNet: Efficient Neural Networks for tinyML | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/enyac-group/evol-q)](https://github.com/enyac-group/evol-q) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10814-b31b1b.svg)](https://arxiv.org/abs/2308.10814) | :heavy_minus_sign: |
| A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.13504v1) | :heavy_minus_sign: |
| Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DenseShift: Towards Accurate and Transferable Low-Bit Shift Network | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2208.09708-b31b1b.svg)](https://arxiv.org/abs/2208.09708) | :heavy_minus_sign: |
| PRANC: Pseudo RAndom Networks for Compacting deep models | [![GitHub](https://img.shields.io/github/stars/UCDvision/PRANC)](https://github.com/UCDvision/PRANC) | [![arXiv](https://img.shields.io/badge/arXiv-2206.08464-b31b1b.svg)](https://arxiv.org/abs/2206.08464) | :heavy_minus_sign: |
| Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement | [![GitHub](https://img.shields.io/github/stars/apple/ml-dr)](https://github.com/apple/ml-dr) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08983-b31b1b.svg)](https://arxiv.org/abs/2303.08983) | :heavy_minus_sign: |
| A Fast Unified System for 3D Object Detection and Tracking | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.06689-b31b1b.svg)](https://arxiv.org/abs/2308.06689) | :heavy_minus_sign: |
| I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference | [![GitHub](https://img.shields.io/github/stars/zkkli/I-ViT)](https://github.com/zkkli/I-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2207.01405-b31b1b.svg)](https://arxiv.org/abs/2207.01405) | :heavy_minus_sign: |
| EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.10554-b31b1b.svg)](https://arxiv.org/abs/2307.10554) | :heavy_minus_sign: |
| Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08809-b31b1b.svg)](https://arxiv.org/abs/2307.08809) | :heavy_minus_sign: |
| DataDAM: Efficient Dataset Distillation with Attention Matching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SAFE: Machine Unlearning With Shard Graphs | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.13169-b31b1b.svg)](https://arxiv.org/abs/2304.13169) | :heavy_minus_sign: |
| ResQ: Residual Quantization for Video Perception | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09511-b31b1b.svg)](https://arxiv.org/abs/2308.09511) | :heavy_minus_sign: |
| Efficient Computation Sharing for Multi-Task Visual Scene Understanding | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09663-b31b1b.svg)](https://arxiv.org/abs/2303.09663) | :heavy_minus_sign: |
| Essential Matrix Estimation using Convex Relaxations in Orthogonal Space | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TripLe: Revisiting Pretrained Model Reuse and Progressive Learning for Efficient Vision Transformer Scaling and Searching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DiffRate: Differentiable Compression Rate for Efficient Vision Transformers | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DiffRate)](https://github.com/OpenGVLab/DiffRate) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17997-b31b1b.svg)](https://arxiv.org/abs/2305.17997) | :heavy_minus_sign: |
| Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection | [![GitHub](https://img.shields.io/github/stars/TinyTigerPan/BCKD)](https://github.com/TinyTigerPan/BCKD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)]([https://arxiv.org/abs/2308.04383](https://arxiv.org/abs/2308.14286)) | :heavy_minus_sign: |
| From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels | [![GitHub](https://img.shields.io/github/stars/yzd-v/cls_KD)](https://github.com/yzd-v/cls_KD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13005-b31b1b.svg)](https://arxiv.org/abs/2303.13005) | :heavy_minus_sign: |
| Efficient 3D Semantic Segmentation with Superpoint Transformer | [![GitHub](https://img.shields.io/github/stars/drprojects/superpoint_transformer)](https://github.com/drprojects/superpoint_transformer) | [![arXiv](https://img.shields.io/badge/arXiv-2306.08045-b31b1b.svg)](https://arxiv.org/abs/2306.08045) | :heavy_minus_sign: |
| Dataset Quantization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.10524-b31b1b.svg)](https://arxiv.org/abs/2308.10524) | :heavy_minus_sign: |
| Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy | [![GitHub](https://img.shields.io/github/stars/JieShibo/PETL-ViT)](https://github.com/JieShibo/PETL-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16867-b31b1b.svg)](https://arxiv.org/abs/2307.16867) | :heavy_minus_sign: |
| RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/zkkli/RepQ-ViT)](https://github.com/zkkli/RepQ-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2212.08254-b31b1b.svg)](https://arxiv.org/abs/2212.08254) | :heavy_minus_sign: |
| Semantically Structured Image Compression via Irregular Group-Based Decoupling | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2305.02586-b31b1b.svg)](https://arxiv.org/abs/2305.02586) | :heavy_minus_sign: |
| SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage | [![GitHub](https://img.shields.io/github/stars/naver-ai/seit)](https://github.com/naver-ai/seit) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11114-b31b1b.svg)](https://arxiv.org/abs/2303.11114) | :heavy_minus_sign: |
| SMMix: Self-Motivated Image Mixing for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/ChenMnZ/SMMix)](https://github.com/ChenMnZ/SMMix) | [![arXiv](https://img.shields.io/badge/arXiv-2212.12977-b31b1b.svg)](https://arxiv.org/abs/2212.12977) | :heavy_minus_sign: |
| Multi-Label Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/penghui-yang/L2D)](https://github.com/penghui-yang/L2D) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06453-b31b1b.svg)](https://arxiv.org/abs/2308.06453) | :heavy_minus_sign: |
| UGC: Unified GAN Compression for Efficient Image-to-Image Translation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos with Spherical Buffers and Padded Convolutions | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2210.09887-b31b1b.svg)](https://arxiv.org/abs/2210.09887) | :heavy_minus_sign: |
| EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction | [![GitHub](https://img.shields.io/github/stars/mit-han-lab/efficientvit)](https://github.com/mit-han-lab/efficientvit) | [![arXiv](https://img.shields.io/badge/arXiv-2205.14756-b31b1b.svg)](https://arxiv.org/abs/2205.14756) | :heavy_minus_sign: |
| DREAM: Efficient Dataset Distillation by Representative Matching | [![GitHub](https://img.shields.io/github/stars/lyq312318224/DREAM)](https://github.com/lyq312318224/DREAM) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14416-b31b1b.svg)](https://arxiv.org/abs/2302.14416) | :heavy_minus_sign: |
| INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2204.07439-b31b1b.svg)](https://arxiv.org/abs/2204.07439) | :heavy_minus_sign: |
| Deep Incubation: Training Large Models by Divide-and-Conquering | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Deep-Incubation)](https://github.com/LeapLabTHU/Deep-Incubation) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04129-b31b1b.svg)](https://arxiv.org/abs/2212.04129) | :heavy_minus_sign: |
| AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Overcoming Forgetting Catastrophe in Quantization-Aware Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models | [![GitHub](https://img.shields.io/github/stars/Guoxoug/window-early-exit)](https://github.com/Guoxoug/window-early-exit) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08010-b31b1b.svg)](https://arxiv.org/abs/2303.08010) | :heavy_minus_sign: |
| ORC: Network Group-based Knowledge Distillation using Online Role Change | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2206.01186-b31b1b.svg)](https://arxiv.org/abs/2206.01186) | :heavy_minus_sign: |
| RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.06787-b31b1b.svg)](https://arxiv.org/abs/2308.06787) | :heavy_minus_sign: |
| Structural Alignment for Network Pruning through Partial Regularization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Automated Knowledge Distillation via Monte Carlo Tree Search | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications | [![GitHub](https://img.shields.io/github/stars/Amshaker/SwiftFormer)](https://github.com/Amshaker/SwiftFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15446-b31b1b.svg)](https://arxiv.org/abs/2303.15446) | :heavy_minus_sign: |
| Causal-DFQ: Causality Guided Data-Free Network Quantization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Automatic Network Pruning via Hilbert-Schmidt Independence Criterion Lasso under Information Bottleneck Principle | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distribution Shift Matters for Knowledge Distillation with Webly Collected Images | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.11469-b31b1b.svg)](https://arxiv.org/abs/2307.11469) | :heavy_minus_sign: |
| FastRecon: Few-shot Industrial Anomaly Detection via Fast Feature Reconstruction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/ChengHan111/E2VPT)](https://github.com/ChengHan111/E2VPT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13770-b31b1b.svg)](https://arxiv.org/abs/2307.13770) | :heavy_minus_sign: |
| Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/kkakkkka/ETRIS)](https://github.com/kkakkkka/ETRIS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.11545-b31b1b.svg)](https://arxiv.org/abs/2307.11545) | :heavy_minus_sign: |
| SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Efficient Deep Space Filling Curve | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Q-Diffusion: Quantizing Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion)](https://github.com/Xiuyu-Li/q-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2302.04304-b31b1b.svg)](https://arxiv.org/abs/2302.04304) | :heavy_minus_sign: |
| Lossy and Lossless (L2) Post-training Model Size Compression | [![GitHub](https://img.shields.io/github/stars/ModelTC/L2_Compression)](https://github.com/ModelTC/L2_Compression) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04269-b31b1b.svg)](https://arxiv.org/abs/2308.04269) | :heavy_minus_sign: |
| Robustifying Token Attention for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/guoyongcs/TAPADL)](https://github.com/guoyongcs/TAPADL) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11126-b31b1b.svg)](https://arxiv.org/abs/2303.11126) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning (other than Deep Learning)

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Document Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Biometrics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Datasets and Evaluation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Faces and Gestures

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| DeePoint: Visual Pointing Recognition and Direction Estimation | [![GitHub](https://img.shields.io/github/stars/kyotovision-public/deepoint)](https://github.com/kyotovision-public/deepoint) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06977-b31b1b.svg)](https://arxiv.org/abs/2304.06977) | :heavy_minus_sign: |
| Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ContactGen: Generative Contact Modeling for Grasp Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stevenlsw.github.io/contactgen/) <br /> [![GitHub](https://img.shields.io/github/stars/stevenlsw/contactgen)](https://github.com/stevenlsw/contactgen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.pdf) <br />  [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2310.03740) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pBgaQdMdB3Q) |
| Imitator: Personalized Speech-Driven 3D Facial Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://balamuruganthambiraja.github.io/Imitator/) | [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2301.00023) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JhXTdjiUCUw) |
| DVGaze: Dual-View Gaze Estimation | [![GitHub](https://img.shields.io/github/stars/yihuacheng/DVGaze)](https://github.com/yihuacheng/DVGaze) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10310-b31b1b.svg)](https://arxiv.org/abs/2308.10310) | :heavy_minus_sign: |
| TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective | [![GitHub](https://img.shields.io/github/stars/DanJun6737/TransFace)](https://github.com/DanJun6737/TransFace) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10133-b31b1b.svg)](https://arxiv.org/abs/2308.10133) | :heavy_minus_sign: |
| Towards Unsupervised Domain Generalization for Face Anti-Spoofing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Reinforced Disentanglement for Face Swapping without Skip Connection | [![GitHub](https://img.shields.io/github/stars/alaist/RD-FS)](https://github.com/alaist/RD-FS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07928-b31b1b.svg)](https://arxiv.org/abs/2307.07928) | :heavy_minus_sign: |
| CoSign: Exploring Co-Occurrence Signals in Skeleton-based Continuous Sign Language Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqiaopeng.github.io/emotalk/) <br /> [![GitHub](https://img.shields.io/github/stars/psyai-net/EmoTalk_release)](https://github.com/psyai-net/EmoTalk_release) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11089-b31b1b.svg)](https://arxiv.org/abs/2303.11089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0uV2B1m-XjI) |
| LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.09023-b31b1b.svg)](https://arxiv.org/abs/2307.09023) | :heavy_minus_sign: |
| ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling | [![GitHub](https://img.shields.io/github/stars/LiuLinyun/ASM-unofficial)](https://github.com/LiuLinyun/ASM-unofficial) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09423-b31b1b.svg)](https://arxiv.org/abs/2304.09423) | :heavy_minus_sign: |
| Troubleshooting Ethnic Quality Bias with Curriculum Domain Adaptation for Face Image Quality Assessment | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| UniFace: Unified Cross-Entropy Loss for Deep Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Human Part-Wise 3D Motion Context Learning for Sign Language Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09305-b31b1b.svg)](https://arxiv.org/abs/2308.09305) | :heavy_minus_sign: |
| Weakly-Supervised Text-Driven Contrastive Learning for Facial Behavior Understanding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.00058-b31b1b.svg)](https://arxiv.org/abs/2304.00058) | :heavy_minus_sign: |
| HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zxz267.github.io/HaMuCo/) <br /> [![GitHub](https://img.shields.io/github/stars/zxz267/HaMuCo)](https://github.com/zxz267/HaMuCo) | [![arXiv](https://img.shields.io/badge/arXiv-2302.00988-b31b1b.svg)](https://arxiv.org/abs/2302.00988) | :heavy_minus_sign: |
| ReactioNet: Learning High-Order Facial Behavior from Universal Stimulus-Reaction by Dyadic Relation Reasoning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Human Dynamics in Autonomous Driving Scenarios | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation | [![GitHub](https://img.shields.io/github/stars/zyhbili/LivelySpeaker)](https://github.com/zyhbili/LivelySpeaker) | [![arXiv](https://img.shields.io/badge/arXiv-2309.09294-b31b1b.svg)](https://arxiv.org/abs/2309.09294) | :heavy_minus_sign: |
| Controllable Guide-Space for Generalizable Face Forgery Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.14039-b31b1b.svg)](https://arxiv.org/abs/2307.14039) | :heavy_minus_sign: |
| Unpaired Multi-Domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map | [![GitHub](https://img.shields.io/github/stars/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap)](https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13245-b31b1b.svg)](https://arxiv.org/abs/2308.13245) | :heavy_minus_sign: |
| Emotional Listener Portrait: Neural Listener Head Generation with Emotion | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Invariant Feature Regularization for Fair Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Gloss-Free Sign Language Translation: Improving from Visual-Language Pretraining | [![GitHub](https://img.shields.io/github/stars/zhoubenjia/GFSLT-VLP)](https://github.com/zhoubenjia/GFSLT-VLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14768-b31b1b.svg)](https://arxiv.org/abs/2307.14768) | :heavy_minus_sign: |
| Contrastive Pseudo Learning for Open-World DeepFake Attribution | [![GitHub](https://img.shields.io/github/stars/TencentYoutuResearch/OpenWorld-DeepFakeAttribution)](https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution) | [![arXiv](https://img.shields.io/badge/arXiv-2309.11132-b31b1b.svg)](https://arxiv.org/abs/2309.11132) | :heavy_minus_sign: |
| Continual Learning for Personalized Co-Speech Gesture Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HandR2N2: Iterative 3D Hand Pose Estimation using a Residual Recurrent Neural Network | [![GitHub](https://img.shields.io/github/stars/cwc1260/HandR2N2)](https://github.com/cwc1260/HandR2N2) | :heavy_minus_sign: | :heavy_minus_sign: |
| SPACE: Speech-Driven Portrait Animation with Controllable Expression | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.09809-b31b1b.svg)](https://arxiv.org/abs/2211.09809) | :heavy_minus_sign: |
| How to Boost Face Recognition with StyleGAN? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://seva100.github.io/stylegan-for-facerec) <br /> [![GitHub](https://img.shields.io/github/stars/seva100/stylegan-for-facerec)](https://github.com/seva100/stylegan-for-facerec) | [![arXiv](https://img.shields.io/badge/arXiv-2210.10090-b31b1b.svg)](https://arxiv.org/abs/2210.10090) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Bsi0RMTdEaI) |
| ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.idiap.ch/en/dataset/childplay-gaze) <br /> [![Zenodo](https://img.shields.io/badge/Zenodo-dataset-FFD1BF.svg)](https://zenodo.org/record/8252535) | [![arXiv](https://img.shields.io/badge/arXiv-2307.01630-b31b1b.svg)](https://arxiv.org/abs/2307.01630) | :heavy_minus_sign: |
| Robust One-Shot Face Video Re-Enactment using Hybrid Latent Spaces of StyleGAN2 | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://trevineoorloff.github.io/FaceVideoReenactment_HybridLatents.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2302.07848-b31b1b.svg)](https://arxiv.org/abs/2302.07848) | :heavy_minus_sign: |
| Data-Free Class-Incremental Hand Gesture Recognition | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://humansensing.cs.cmu.edu/sites/default/files/Data-Free%20Class-Incremental%20Hand%20Gesture%20Recognition_0.pdf) | [![GitHub](https://img.shields.io/github/stars/humansensinglab/dfcil-hgr)](https://github.com/humansensinglab/dfcil-hgr) | :heavy_minus_sign: |
| Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition | [![GitHub](https://img.shields.io/github/stars/Carpumpkin/InBoMem)](https://github.com/Carpumpkin/InBoMem) | :heavy_minus_sign: | :heavy_minus_sign: |
| Knowledge-Spreader: Learning Semi-Supervised Facial Action Dynamics by Consistifying Knowledge Granularity | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Face Clustering via Graph Convolutional Networks with Confidence Edges | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| StyleGANEX: StyleGAN-based Manipulation Beyond Cropped Aligned Faces | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/styleganex/) <br /> [![GitHub](https://img.shields.io/github/stars/williamyang1991/StyleGANEX)](https://github.com/williamyang1991/StyleGANEX) | [![arXiv](https://img.shields.io/badge/arXiv-2303.06146-b31b1b.svg)](https://arxiv.org/abs/2303.06146) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8oK0TXQmxg8) |
| SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes | [![GitHub](https://img.shields.io/github/stars/anonymous-author-sub/seeable)](https://github.com/anonymous-author-sub/seeable) | [![arXiv](https://img.shields.io/badge/arXiv-2211.11296-b31b1b.svg)](https://arxiv.org/abs/2211.11296) | :heavy_minus_sign: |
| Adaptive Nonlinear Latent Transformation for Conditional Face Editing | [![GitHub](https://img.shields.io/github/stars/Hzzone/AdaTrans)](https://github.com/Hzzone/AdaTrans) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07790-b31b1b.svg)](https://arxiv.org/abs/2307.07790) | :heavy_minus_sign: |
| Semi-Supervised Speech-Driven 3D Facial Animation via Cross-Modal Encoding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ICD-Face: Intra-Class Compactness Distillation for Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| C2ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Medical and Biological Vision; Cell Microscopy

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| BoMD: Bag of Multi-Label Local Descriptors for Noisy Chest X-Ray Classification | [![GitHub](https://img.shields.io/github/stars/cyh-0/BoMD)](https://github.com/cyh-0/BoMD) | [![arXiv](https://img.shields.io/badge/arXiv-2203.01937-b31b1b.svg)](https://arxiv.org/abs/2203.01937) | :heavy_minus_sign: |
| CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection | [![GitHub](https://img.shields.io/github/stars/ljwztc/CLIP-Driven-Universal-Model)](https://github.com/ljwztc/CLIP-Driven-Universal-Model) | [![arXiv](https://img.shields.io/badge/arXiv-2301.00785-b31b1b.svg)](https://arxiv.org/abs/2301.00785) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Scene Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Multimodal Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human-in-the-Loop Computer Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Forensics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Geometric Deep Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision Applications and Systems

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing | [![GitHub](https://img.shields.io/github/stars/aimagelab/multimodal-garment-designer)](https://github.com/aimagelab/multimodal-garment-designer) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02051-b31b1b.svg)](https://arxiv.org/abs/2304.02051) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning and Dataset

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unmasked Teacher: Towards Training-Efficient Video Foundation Models | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/unmasked_teacher)](https://github.com/OpenGVLab/unmasked_teacher) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16058-b31b1b.svg)](https://arxiv.org/abs/2303.16058) | :heavy_minus_sign: |

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/ICCV-2023-Papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/ICCV-2023-Papers&type=Date" alt="Star History Chart">
    </a>
<p>
