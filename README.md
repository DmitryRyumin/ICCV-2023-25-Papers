# ICCV-2023-Papers

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![Conference](http://img.shields.io/badge/ICCV-2023-7395C5.svg)](https://iccv2023.thecvf.com)
![Version](https://img.shields.io/badge/version-v0.0.0-rc0)
![GitHub repo size](https://img.shields.io/github/repo-size/DmitryRyumin/ICCV-2023-Papers)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/LICENSE)
[![Contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/README.md)
![GitHub contributors](https://img.shields.io/github/contributors/dmitryryumin/ICCV-2023-Papers)
![GitHub commit activity (branch)](https://img.shields.io/github/commit-activity/t/dmitryryumin/ICCV-2023-Papers)
![GitHub closed issues](https://img.shields.io/github/issues-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub issues](https://img.shields.io/github/issues/DmitryRyumin/ICCV-2023-Papers)
![GitHub closed pull requests](https://img.shields.io/github/issues-pr-closed/DmitryRyumin/ICCV-2023-Papers)
![GitHub pull requests](https://img.shields.io/github/issues-pr/dmitryryumin/ICCV-2023-Papers)
![GitHub last commit](https://img.shields.io/github/last-commit/DmitryRyumin/ICCV-2023-Papers)
![GitHub watchers](https://img.shields.io/github/watchers/dmitryryumin/ICCV-2023-Papers)
![GitHub forks](https://img.shields.io/github/forks/dmitryryumin/ICCV-2023-Papers)
![GitHub Repo stars](https://img.shields.io/github/stars/dmitryryumin/ICCV-2023-Papers)
![Visitors](https://api.visitorbadge.io/api/combined?path=https%3A%2F%2Fgithub.com%2FDmitryRyumin%2FICCV-2023-Papers&label=Visitors&countColor=%23263759&style=flat)

<div style="float:left;">
  <img src="https://geps.dev/progress/59?successColor=006600" />
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/completed_checkmark_done.svg" width="25" />
</div>

---

ICCV 2023 Papers: Explore a comprehensive collection of cutting-edge research papers presented at [*ICCV 2023*](https://iccv2023.thecvf.com/), the premier computer vision conference. Keep up to date with the latest advances in computer vision and deep learning. Code implementations included. :star: the repository for the development of visual intelligence!

<p align="center">
    <a href="https://iccv2023.thecvf.com/" target="_blank">
        <img width="600" src="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/images/ICCV2023-banner.jpg" alt="ICCV 2023">
    </a>
<p>

---

[*The online version of the ICCV 2023 Conference Programme*](https://iccv2023.thecvf.com/main.conference.program-107.php), comprises a list of all accepted full papers, their presentation order, as well as the designated presentation times.

---

<a href="https://github.com/DmitryRyumin/NewEraAI-Papers" style="float:left;">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arrow_click_cursor_pointer.png" width="25" />
  Other collections of the best AI conferences
</a>

<br />
<br />

> :exclamation: Conference table will be up to date all the time.

<table>
    <tr>
        <td><strong>Conference</strong></td>
        <td colspan="1" align="center"><strong>Year</strong></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Computer Vision (CV)</i></td>
    </tr>
    <tr>
        <td>CVPR</td>
        <td><a href="https://github.com/DmitryRyumin/CVPR-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
      <td colspan="2" align="center"><i>Speech (SP)</i></td>
    </tr>
    <tr>
        <td>ICASSP</td>
        <td><a href="https://github.com/DmitryRyumin/ICASSP-2023-Papers" target="_blank">2023</a></td>
    </tr>
    <tr>
        <td>INTERSPEECH</td>
        <td><a href="https://github.com/DmitryRyumin/INTERSPEECH-2023-Papers" target="_blank">2023</a></td>
    </tr>
</table>

---

## Contributors

<a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/graphs/contributors">
  <img src="http://contributors.nn.ci/api?repo=DmitryRyumin/ICCV-2023-Papers" />
</a>

<br />
<br />

Contributions to improve the completeness of this list are greatly appreciated. If you come across any overlooked papers, please **feel free to [*create pull requests*](https://github.com/DmitryRyumin/ICCV-2023-Papers/pulls), [*open issues*](https://github.com/DmitryRyumin/ICCV-2023-Papers/issues) or contact me via [*email*](mailto:neweraairesearch@gmail.com)**. Your participation is crucial to making this repository even better.

---

## [Papers](https://openaccess.thecvf.com/ICCV2023?day=all) <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/ai.svg" width="30" />

<!-- > :exclamation: Final paper links will be added post-conference. -->

<table>
    <thead>
        <tr>
            <th scope="col">Section</th>
            <th scope="col">Papers</th>
            <th scope="col"><img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/arxiv-logo.svg" width="45" /></th>
            <th scope="col"><img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/github_code_developer.svg" width="27" /></th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md">3D from Multi-View and Sensors</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md"><img src="https://img.shields.io/badge/69-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md"><img src="https://img.shields.io/badge/49-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-multi-view-and-sensors.md"><img src="https://img.shields.io/badge/36-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md">Adversarial Attack and Defense</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md"><img src="https://img.shields.io/badge/53-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md"><img src="https://img.shields.io/badge/39-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/adversarial-attack-and-defense.md"><img src="https://img.shields.io/badge/29-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-robotics.md">Vision and Robotics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-robotics.md"><img src="https://img.shields.io/badge/11-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-robotics.md"><img src="https://img.shields.io/badge/6-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-robotics.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-graphics.md">Vision and Graphics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-graphics.md"><img src="https://img.shields.io/badge/22-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-graphics.md"><img src="https://img.shields.io/badge/17-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-graphics.md"><img src="https://img.shields.io/badge/13-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/segmentation-grouping-and-shape-analysis.md">Segmentation, Grouping and Shape Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/segmentation-grouping-and-shape-analysis.md"><img src="https://img.shields.io/badge/72-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/segmentation-grouping-and-shape-analysis.md"><img src="https://img.shields.io/badge/49-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/segmentation-grouping-and-shape-analysis.md"><img src="https://img.shields.io/badge/37-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-categorization.md">Recognition: Categorization</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-categorization.md"><img src="https://img.shields.io/badge/50-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-categorization.md"><img src="https://img.shields.io/badge/34-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-categorization.md"><img src="https://img.shields.io/badge/25-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/explainable-ai-for-cv.md">Explainable AI for CV</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/explainable-ai-for-cv.md"><img src="https://img.shields.io/badge/21-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/explainable-ai-for-cv.md"><img src="https://img.shields.io/badge/15-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/explainable-ai-for-cv.md"><img src="https://img.shields.io/badge/12-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/neural-generative-models.md">Neural Generative Models</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/neural-generative-models.md"><img src="https://img.shields.io/badge/34-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/neural-generative-models.md"><img src="https://img.shields.io/badge/25-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/neural-generative-models.md"><img src="https://img.shields.io/badge/16-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-language.md">Vision and Language</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-language.md"><img src="https://img.shields.io/badge/64-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-language.md"><img src="https://img.shields.io/badge/49-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-language.md"><img src="https://img.shields.io/badge/36-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-graphics-and-robotics.md">Vision, Graphics, and Robotics</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-graphics-and-robotics.md"><img src="https://img.shields.io/badge/8-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-graphics-and-robotics.md"><img src="https://img.shields.io/badge/8-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-graphics-and-robotics.md"><img src="https://img.shields.io/badge/7-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/privacy-security-fairness-and-explainability.md">Privacy, Security, Fairness, and Explainability</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/privacy-security-fairness-and-explainability.md"><img src="https://img.shields.io/badge/8-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/privacy-security-fairness-and-explainability.md"><img src="https://img.shields.io/badge/8-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/privacy-security-fairness-and-explainability.md"><img src="https://img.shields.io/badge/6-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/fairness-privacy-ethics-social-good-transparency-accountability-in-vision.md">Fairness, Privacy, Ethics, Social-good, Transparency, Accountability in Vision</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/fairness-privacy-ethics-social-good-transparency-accountability-in-vision.md"><img src="https://img.shields.io/badge/41-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/fairness-privacy-ethics-social-good-transparency-accountability-in-vision.md"><img src="https://img.shields.io/badge/26-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/fairness-privacy-ethics-social-good-transparency-accountability-in-vision.md"><img src="https://img.shields.io/badge/16-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/first-person-egocentric-vision.md">First Person (Egocentric) Vision</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/first-person-egocentric-vision.md"><img src="https://img.shields.io/badge/7-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/first-person-egocentric-vision.md"><img src="https://img.shields.io/badge/6-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/first-person-egocentric-vision.md"><img src="https://img.shields.io/badge/1-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/representation-learning.md">Representation Learning</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/representation-learning.md"><img src="https://img.shields.io/badge/40-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/representation-learning.md"><img src="https://img.shields.io/badge/28-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/representation-learning.md"><img src="https://img.shields.io/badge/20-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/deep-learning-architectures.md">Deep Learning Architectures</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/deep-learning-architectures.md"><img src="https://img.shields.io/badge/45-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/deep-learning-architectures.md"><img src="https://img.shields.io/badge/30-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/deep-learning-architectures.md"><img src="https://img.shields.io/badge/18-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-detection.md">Recognition: Detection</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-detection.md"><img src="https://img.shields.io/badge/73-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-detection.md"><img src="https://img.shields.io/badge/54-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-detection.md"><img src="https://img.shields.io/badge/38-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/image-and-video-synthesis.md">Image and Video Synthesis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/image-and-video-synthesis.md"><img src="https://img.shields.io/badge/71-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/image-and-video-synthesis.md"><img src="https://img.shields.io/badge/58-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/image-and-video-synthesis.md"><img src="https://img.shields.io/badge/44-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-audio.md">Vision and Audio</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-audio.md"><img src="https://img.shields.io/badge/12-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-audio.md"><img src="https://img.shields.io/badge/11-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/vision-and-audio.md"><img src="https://img.shields.io/badge/3-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-segmentation-and-shape-analysis.md">Recognition, Segmentation, and Shape Analysis</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-segmentation-and-shape-analysis.md"><img src="https://img.shields.io/badge/12-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-segmentation-and-shape-analysis.md"><img src="https://img.shields.io/badge/10-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-segmentation-and-shape-analysis.md"><img src="https://img.shields.io/badge/9-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/generative-ai.md">Generative AI</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/generative-ai.md"><img src="https://img.shields.io/badge/14-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/generative-ai.md"><img src="https://img.shields.io/badge/13-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/generative-ai.md"><img src="https://img.shields.io/badge/10-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/humans-3d-modeling-and-driving.md">Humans, 3D Modeling, and Driving</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/humans-3d-modeling-and-driving.md"><img src="https://img.shields.io/badge/12-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/humans-3d-modeling-and-driving.md"><img src="https://img.shields.io/badge/10-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/humans-3d-modeling-and-driving.md"><img src="https://img.shields.io/badge/7-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-vision-and-theory.md">Low-Level Vision and Theory</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-vision-and-theory.md"><img src="https://img.shields.io/badge/12-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-vision-and-theory.md"><img src="https://img.shields.io/badge/6-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-vision-and-theory.md"><img src="https://img.shields.io/badge/7-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/navigation-and-autonomous-driving.md">Navigation and Autonomous Driving</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/navigation-and-autonomous-driving.md"><img src="https://img.shields.io/badge/51-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/navigation-and-autonomous-driving.md"><img src="https://img.shields.io/badge/40-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/navigation-and-autonomous-driving.md"><img src="https://img.shields.io/badge/27-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-a-single-image-and-shape-from-x.md">3D from a Single Image and Shape-from-X</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-a-single-image-and-shape-from-x.md"><img src="https://img.shields.io/badge/68-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-a-single-image-and-shape-from-x.md"><img src="https://img.shields.io/badge/56-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-from-a-single-image-and-shape-from-x.md"><img src="https://img.shields.io/badge/39-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/motion-estimation-matching-and-tracking.md">Motion Estimation, Matching and Tracking</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/motion-estimation-matching-and-tracking.md"><img src="https://img.shields.io/badge/59-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/motion-estimation-matching-and-tracking.md"><img src="https://img.shields.io/badge/42-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/motion-estimation-matching-and-tracking.md"><img src="https://img.shields.io/badge/41-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/action-and-event-understanding.md">Action and Event Understanding</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/action-and-event-understanding.md"><img src="https://img.shields.io/badge/30-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/action-and-event-understanding.md"><img src="https://img.shields.io/badge/22-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/action-and-event-understanding.md"><img src="https://img.shields.io/badge/19-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computational-imaging.md">Computational Imaging</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computational-imaging.md"><img src="https://img.shields.io/badge/37-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computational-imaging.md"><img src="https://img.shields.io/badge/22-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computational-imaging.md"><img src="https://img.shields.io/badge/19-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/embodied-vision-active-agents-simulation.md">Embodied Vision: Active Agents; Simulation</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/embodied-vision-active-agents-simulation.md"><img src="https://img.shields.io/badge/15-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/embodied-vision-active-agents-simulation.md"><img src="https://img.shields.io/badge/14-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/embodied-vision-active-agents-simulation.md"><img src="https://img.shields.io/badge/8-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-retrieval.md">Recognition: Retrieval</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-retrieval.md"><img src="https://img.shields.io/badge/31-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-retrieval.md"><img src="https://img.shields.io/badge/16-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/recognition-retrieval.md"><img src="https://img.shields.io/badge/18-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/transfer-low-shot-continual-long-tail-learning.md">Transfer, Low-Shot, Continual, Long-Tail Learning</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/transfer-low-shot-continual-long-tail-learning.md"><img src="https://img.shields.io/badge/43-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/transfer-low-shot-continual-long-tail-learning.md"><img src="https://img.shields.io/badge/36-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/transfer-low-shot-continual-long-tail-learning.md"><img src="https://img.shields.io/badge/27-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-and-physics-based-vision.md">Low-Level and Physics-based Vision</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-and-physics-based-vision.md"><img src="https://img.shields.io/badge/115-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-and-physics-based-vision.md"><img src="https://img.shields.io/badge/71-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/low-level-and-physics-based-vision.md"><img src="https://img.shields.io/badge/78-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computer-vision-theory.md">Computer Vision Theory</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computer-vision-theory.md"><img src="https://img.shields.io/badge/9-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computer-vision-theory.md"><img src="https://img.shields.io/badge/5-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/computer-vision-theory.md"><img src="https://img.shields.io/badge/5-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/video-analysis-and-understanding.md">Video Analysis and Understanding</a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/video-analysis-and-understanding.md"><img src="https://img.shields.io/badge/51-42BA16" alt="Papers"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/video-analysis-and-understanding.md"><img src="https://img.shields.io/badge/38-b31b1b" alt="Preprints"></a>
            </td>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/video-analysis-and-understanding.md"><img src="https://img.shields.io/badge/32-1D7FBF" alt="Open Code"></a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/object-pose-estimation-and-tracking.md">Object Pose Estimation and Tracking</a>
            </td>
            <td colspan="3" rowspan=3 align="center"><i">Will soon be added</i></td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/3d-shape-modeling-and-processing.md">3D Shape Modeling and Processing</a>
            </td>
        </tr>
        <tr>
            <td>
                <a href="https://github.com/DmitryRyumin/ICCV-2023-Papers/blob/main/sections/human-poseshape-estimation.md">3D Human Pose/Shape Estimation</a>
            </td>
        </tr>
    </tbody>
</table>

<!-- <details open>
<summary>List of sections<a id="sections"></a></summary>

- [Transfer, Low-Shot, and Continual Learning](#transfer-low-shot-and-continual-learning)
- [Self-, Semi-, and Unsupervised Learning](#self--semi--and-unsupervised-learning)
- [Self-, Semi-, Meta-, Unsupervised Learning](#self--semi--meta--unsupervised-learning)
- [Photogrammetry and Remote Sensing](#photogrammetry-and-remote-sensing)
- [Efficient and Scalable Vision](#efficient-and-scalable-vision)
- [Machine Learning (other than Deep Learning)](#machine-learning-other-than-deep-learning)
- [Document Analysis and Understanding](#document-analysis-and-understanding)
- [Biometrics](#biometrics)
- [Datasets and Evaluation](#datasets-and-evaluation)
- [Faces and Gestures](#faces-and-gestures)
- [Medical and Biological Vision; Cell Microscopy](#medical-and-biological-vision-cell-microscopy)
- [Scene Analysis and Understanding](#scene-analysis-and-understanding)
- [Multimodal Learning](#multimodal-learning)
- [Human-in-the-Loop Computer Vision](#human-in-the-loop-computer-vision)
- [Image and Video Forensics](#image-and-video-forensics)
- [Geometric Deep Learning](#geometric-deep-learning)
- [Vision Applications and Systems](#vision-applications-and-systems)
- [Machine Learning and Dataset](#machine-learning-and-dataset)

</details> -->

<!--

### Transfer, Low-Shot, and Continual Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, and Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Self-, Semi-, Meta-, Unsupervised Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Photogrammetry and Remote Sensing

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Efficient and Scalable Vision

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| AdaNIC: Towards Practical Neural Image Compression via Dynamic Transform Routing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Rethinking Vision Transformers for MobileNet Size and Speed | [![GitHub](https://img.shields.io/github/stars/snap-research/EfficientFormer)](https://github.com/snap-research/EfficientFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2212.08059-b31b1b.svg)](https://arxiv.org/abs/2212.08059) | :heavy_minus_sign: |
| DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.04383) | :heavy_minus_sign: |
| Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.13494-b31b1b.svg)](https://arxiv.org/abs/2308.13494) | :heavy_minus_sign: |
| Inherent Redundancy in Spiking Neural Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.08227-b31b1b.svg)](https://arxiv.org/abs/2308.08227) | :heavy_minus_sign: |
| Achievement-based Training Progress Balancing for Multi-Task Learning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04549-b31b1b.svg)](https://arxiv.org/abs/2308.04549) | :heavy_minus_sign: |
| Differentiable Transportation Pruning | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08483-b31b1b.svg)](https://arxiv.org/abs/2307.08483) | :heavy_minus_sign: |
| XiNet: Efficient Neural Networks for tinyML | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Jumping through Local Minima: Quantization in the Loss Landscape of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/enyac-group/evol-q)](https://github.com/enyac-group/evol-q) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10814-b31b1b.svg)](https://arxiv.org/abs/2308.10814) | :heavy_minus_sign: |
| A2Q: Accumulator-Aware Quantization with Guaranteed Overflow Avoidance | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)](https://arxiv.org/abs/2308.13504v1) | :heavy_minus_sign: |
| Workie-Talkie: Accelerating Federated Learning by Overlapping Computing and Communications via Contrastive Regularization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DenseShift: Towards Accurate and Transferable Low-Bit Shift Network | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2208.09708-b31b1b.svg)](https://arxiv.org/abs/2208.09708) | :heavy_minus_sign: |
| PRANC: Pseudo RAndom Networks for Compacting deep models | [![GitHub](https://img.shields.io/github/stars/UCDvision/PRANC)](https://github.com/UCDvision/PRANC) | [![arXiv](https://img.shields.io/badge/arXiv-2206.08464-b31b1b.svg)](https://arxiv.org/abs/2206.08464) | :heavy_minus_sign: |
| Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement | [![GitHub](https://img.shields.io/github/stars/apple/ml-dr)](https://github.com/apple/ml-dr) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08983-b31b1b.svg)](https://arxiv.org/abs/2303.08983) | :heavy_minus_sign: |
| A Fast Unified System for 3D Object Detection and Tracking | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.06689-b31b1b.svg)](https://arxiv.org/abs/2308.06689) | :heavy_minus_sign: |
| I-ViT: Integer-only Quantization for Efficient Vision Transformer Inference | [![GitHub](https://img.shields.io/github/stars/zkkli/I-ViT)](https://github.com/zkkli/I-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2207.01405-b31b1b.svg)](https://arxiv.org/abs/2207.01405) | :heavy_minus_sign: |
| EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.10554-b31b1b.svg)](https://arxiv.org/abs/2307.10554) | :heavy_minus_sign: |
| Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.08809-b31b1b.svg)](https://arxiv.org/abs/2307.08809) | :heavy_minus_sign: |
| DataDAM: Efficient Dataset Distillation with Attention Matching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SAFE: Machine Unlearning With Shard Graphs | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.13169-b31b1b.svg)](https://arxiv.org/abs/2304.13169) | :heavy_minus_sign: |
| ResQ: Residual Quantization for Video Perception | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09511-b31b1b.svg)](https://arxiv.org/abs/2308.09511) | :heavy_minus_sign: |
| Efficient Computation Sharing for Multi-Task Visual Scene Understanding | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2303.09663-b31b1b.svg)](https://arxiv.org/abs/2303.09663) | :heavy_minus_sign: |
| Essential Matrix Estimation using Convex Relaxations in Orthogonal Space | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| TripLe: Revisiting Pretrained Model Reuse and Progressive Learning for Efficient Vision Transformer Scaling and Searching | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| DiffRate: Differentiable Compression Rate for Efficient Vision Transformers | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/DiffRate)](https://github.com/OpenGVLab/DiffRate) | [![arXiv](https://img.shields.io/badge/arXiv-2305.17997-b31b1b.svg)](https://arxiv.org/abs/2305.17997) | :heavy_minus_sign: |
| Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection | [![GitHub](https://img.shields.io/github/stars/TinyTigerPan/BCKD)](https://github.com/TinyTigerPan/BCKD) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04383-b31b1b.svg)]([https://arxiv.org/abs/2308.04383](https://arxiv.org/abs/2308.14286)) | :heavy_minus_sign: |
| From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels | [![GitHub](https://img.shields.io/github/stars/yzd-v/cls_KD)](https://github.com/yzd-v/cls_KD) | [![arXiv](https://img.shields.io/badge/arXiv-2303.13005-b31b1b.svg)](https://arxiv.org/abs/2303.13005) | :heavy_minus_sign: |
| Efficient 3D Semantic Segmentation with Superpoint Transformer | [![GitHub](https://img.shields.io/github/stars/drprojects/superpoint_transformer)](https://github.com/drprojects/superpoint_transformer) | [![arXiv](https://img.shields.io/badge/arXiv-2306.08045-b31b1b.svg)](https://arxiv.org/abs/2306.08045) | :heavy_minus_sign: |
| Dataset Quantization | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.10524-b31b1b.svg)](https://arxiv.org/abs/2308.10524) | :heavy_minus_sign: |
| Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy | [![GitHub](https://img.shields.io/github/stars/JieShibo/PETL-ViT)](https://github.com/JieShibo/PETL-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.16867-b31b1b.svg)](https://arxiv.org/abs/2307.16867) | :heavy_minus_sign: |
| RepQ-ViT: Scale Reparameterization for Post-Training Quantization of Vision Transformers | [![GitHub](https://img.shields.io/github/stars/zkkli/RepQ-ViT)](https://github.com/zkkli/RepQ-ViT) | [![arXiv](https://img.shields.io/badge/arXiv-2212.08254-b31b1b.svg)](https://arxiv.org/abs/2212.08254) | :heavy_minus_sign: |
| Semantically Structured Image Compression via Irregular Group-Based Decoupling | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2305.02586-b31b1b.svg)](https://arxiv.org/abs/2305.02586) | :heavy_minus_sign: |
| SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage | [![GitHub](https://img.shields.io/github/stars/naver-ai/seit)](https://github.com/naver-ai/seit) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11114-b31b1b.svg)](https://arxiv.org/abs/2303.11114) | :heavy_minus_sign: |
| SMMix: Self-Motivated Image Mixing for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/ChenMnZ/SMMix)](https://github.com/ChenMnZ/SMMix) | [![arXiv](https://img.shields.io/badge/arXiv-2212.12977-b31b1b.svg)](https://arxiv.org/abs/2212.12977) | :heavy_minus_sign: |
| Multi-Label Knowledge Distillation | [![GitHub](https://img.shields.io/github/stars/penghui-yang/L2D)](https://github.com/penghui-yang/L2D) | [![arXiv](https://img.shields.io/badge/arXiv-2308.06453-b31b1b.svg)](https://arxiv.org/abs/2308.06453) | :heavy_minus_sign: |
| UGC: Unified GAN Compression for Efficient Image-to-Image Translation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| MotionDeltaCNN: Sparse CNN Inference of Frame Differences in Moving Camera Videos with Spherical Buffers and Padded Convolutions | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2210.09887-b31b1b.svg)](https://arxiv.org/abs/2210.09887) | :heavy_minus_sign: |
| EfficientViT: Lightweight Multi-Scale Attention for High-Resolution Dense Prediction | [![GitHub](https://img.shields.io/github/stars/mit-han-lab/efficientvit)](https://github.com/mit-han-lab/efficientvit) | [![arXiv](https://img.shields.io/badge/arXiv-2205.14756-b31b1b.svg)](https://arxiv.org/abs/2205.14756) | :heavy_minus_sign: |
| DREAM: Efficient Dataset Distillation by Representative Matching | [![GitHub](https://img.shields.io/github/stars/lyq312318224/DREAM)](https://github.com/lyq312318224/DREAM) | [![arXiv](https://img.shields.io/badge/arXiv-2302.14416-b31b1b.svg)](https://arxiv.org/abs/2302.14416) | :heavy_minus_sign: |
| INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2204.07439-b31b1b.svg)](https://arxiv.org/abs/2204.07439) | :heavy_minus_sign: |
| Deep Incubation: Training Large Models by Divide-and-Conquering | [![GitHub](https://img.shields.io/github/stars/LeapLabTHU/Deep-Incubation)](https://github.com/LeapLabTHU/Deep-Incubation) | [![arXiv](https://img.shields.io/badge/arXiv-2212.04129-b31b1b.svg)](https://arxiv.org/abs/2212.04129) | :heavy_minus_sign: |
| AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Overcoming Forgetting Catastrophe in Quantization-Aware Training | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models | [![GitHub](https://img.shields.io/github/stars/Guoxoug/window-early-exit)](https://github.com/Guoxoug/window-early-exit) | [![arXiv](https://img.shields.io/badge/arXiv-2303.08010-b31b1b.svg)](https://arxiv.org/abs/2303.08010) | :heavy_minus_sign: |
| ORC: Network Group-based Knowledge Distillation using Online Role Change | [![GitHub](https://img.shields.io/github/stars/IRMVLab/DELFlow)](https://github.com/IRMVLab/DELFlow) | [![arXiv](https://img.shields.io/badge/arXiv-2206.01186-b31b1b.svg)](https://arxiv.org/abs/2206.01186) | :heavy_minus_sign: |
| RMP-Loss: Regularizing Membrane Potential Distribution for Spiking Neural Networks | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.06787-b31b1b.svg)](https://arxiv.org/abs/2308.06787) | :heavy_minus_sign: |
| Structural Alignment for Network Pruning through Partial Regularization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Automated Knowledge Distillation via Monte Carlo Tree Search | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications | [![GitHub](https://img.shields.io/github/stars/Amshaker/SwiftFormer)](https://github.com/Amshaker/SwiftFormer) | [![arXiv](https://img.shields.io/badge/arXiv-2303.15446-b31b1b.svg)](https://arxiv.org/abs/2303.15446) | :heavy_minus_sign: |
| Causal-DFQ: Causality Guided Data-Free Network Quantization | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Efficient Joint Optimization of Layer-Adaptive Weight Pruning in Deep Neural Networks | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Automatic Network Pruning via Hilbert-Schmidt Independence Criterion Lasso under Information Bottleneck Principle | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Distribution Shift Matters for Knowledge Distillation with Webly Collected Images | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.11469-b31b1b.svg)](https://arxiv.org/abs/2307.11469) | :heavy_minus_sign: |
| FastRecon: Few-shot Industrial Anomaly Detection via Fast Feature Reconstruction | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| E<sup>2</sup>VPT: An Effective and Efficient Approach for Visual Prompt Tuning | [![GitHub](https://img.shields.io/github/stars/ChengHan111/E2VPT)](https://github.com/ChengHan111/E2VPT) | [![arXiv](https://img.shields.io/badge/arXiv-2307.13770-b31b1b.svg)](https://arxiv.org/abs/2307.13770) | :heavy_minus_sign: |
| Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation | [![GitHub](https://img.shields.io/github/stars/kkakkkka/ETRIS)](https://github.com/kkakkkka/ETRIS) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Xu_Bridging_Vision_and_Language_Encoders_Parameter-Efficient_Tuning_for_Referring_Image_ICCV_2023_paper.pdf) <br /> [![arXiv](https://img.shields.io/badge/arXiv-2307.11545-b31b1b.svg)](https://arxiv.org/abs/2307.11545) | :heavy_minus_sign: |
| SHACIRA: Scalable HAsh-grid Compression for Implicit Neural Representations | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Efficient Deep Space Filling Curve | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Q-Diffusion: Quantizing Diffusion Models | [![GitHub](https://img.shields.io/github/stars/Xiuyu-Li/q-diffusion)](https://github.com/Xiuyu-Li/q-diffusion) | [![arXiv](https://img.shields.io/badge/arXiv-2302.04304-b31b1b.svg)](https://arxiv.org/abs/2302.04304) | :heavy_minus_sign: |
| Lossy and Lossless (L2) Post-training Model Size Compression | [![GitHub](https://img.shields.io/github/stars/ModelTC/L2_Compression)](https://github.com/ModelTC/L2_Compression) | [![arXiv](https://img.shields.io/badge/arXiv-2308.04269-b31b1b.svg)](https://arxiv.org/abs/2308.04269) | :heavy_minus_sign: |
| Robustifying Token Attention for Vision Transformers | [![GitHub](https://img.shields.io/github/stars/guoyongcs/TAPADL)](https://github.com/guoyongcs/TAPADL) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11126-b31b1b.svg)](https://arxiv.org/abs/2303.11126) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning (other than Deep Learning)

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Document Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Biometrics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Datasets and Evaluation

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Faces and Gestures

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| DeePoint: Visual Pointing Recognition and Direction Estimation | [![GitHub](https://img.shields.io/github/stars/kyotovision-public/deepoint)](https://github.com/kyotovision-public/deepoint) | [![arXiv](https://img.shields.io/badge/arXiv-2304.06977-b31b1b.svg)](https://arxiv.org/abs/2304.06977) | :heavy_minus_sign: |
| Contactless Pulse Estimation Leveraging Pseudo Labels and Self-Supervision | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Most Important Person-Guided Dual-Branch Cross-Patch Attention for Group Affect Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ContactGen: Generative Contact Modeling for Grasp Generation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://stevenlsw.github.io/contactgen/) <br /> [![GitHub](https://img.shields.io/github/stars/stevenlsw/contactgen)](https://github.com/stevenlsw/contactgen) | [![thecvf](https://img.shields.io/badge/pdf-thecvf-7395C5.svg)](https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_ContactGen_Generative_Contact_Modeling_for_Grasp_Generation_ICCV_2023_paper.pdf) <br />  [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2310.03740) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=pBgaQdMdB3Q) |
| Imitator: Personalized Speech-Driven 3D Facial Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://balamuruganthambiraja.github.io/Imitator/) | [![arXiv](https://img.shields.io/badge/arXiv-2301.00023-b31b1b.svg)](https://arxiv.org/abs/2301.00023) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=JhXTdjiUCUw) |
| DVGaze: Dual-View Gaze Estimation | [![GitHub](https://img.shields.io/github/stars/yihuacheng/DVGaze)](https://github.com/yihuacheng/DVGaze) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10310-b31b1b.svg)](https://arxiv.org/abs/2308.10310) | :heavy_minus_sign: |
| TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective | [![GitHub](https://img.shields.io/github/stars/DanJun6737/TransFace)](https://github.com/DanJun6737/TransFace) | [![arXiv](https://img.shields.io/badge/arXiv-2308.10133-b31b1b.svg)](https://arxiv.org/abs/2308.10133) | :heavy_minus_sign: |
| Towards Unsupervised Domain Generalization for Face Anti-Spoofing | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Reinforced Disentanglement for Face Swapping without Skip Connection | [![GitHub](https://img.shields.io/github/stars/alaist/RD-FS)](https://github.com/alaist/RD-FS) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07928-b31b1b.svg)](https://arxiv.org/abs/2307.07928) | :heavy_minus_sign: |
| CoSign: Exploring Co-Occurrence Signals in Skeleton-based Continuous Sign Language Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://ziqiaopeng.github.io/emotalk/) <br /> [![GitHub](https://img.shields.io/github/stars/psyai-net/EmoTalk_release)](https://github.com/psyai-net/EmoTalk_release) | [![arXiv](https://img.shields.io/badge/arXiv-2303.11089-b31b1b.svg)](https://arxiv.org/abs/2303.11089) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=0uV2B1m-XjI) |
| LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.09023-b31b1b.svg)](https://arxiv.org/abs/2307.09023) | :heavy_minus_sign: |
| ASM: Adaptive Skinning Model for High-Quality 3D Face Modeling | [![GitHub](https://img.shields.io/github/stars/LiuLinyun/ASM-unofficial)](https://github.com/LiuLinyun/ASM-unofficial) | [![arXiv](https://img.shields.io/badge/arXiv-2304.09423-b31b1b.svg)](https://arxiv.org/abs/2304.09423) | :heavy_minus_sign: |
| Troubleshooting Ethnic Quality Bias with Curriculum Domain Adaptation for Face Image Quality Assessment | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| UniFace: Unified Cross-Entropy Loss for Deep Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Human Part-Wise 3D Motion Context Learning for Sign Language Recognition | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2308.09305-b31b1b.svg)](https://arxiv.org/abs/2308.09305) | :heavy_minus_sign: |
| Weakly-Supervised Text-Driven Contrastive Learning for Facial Behavior Understanding | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2304.00058-b31b1b.svg)](https://arxiv.org/abs/2304.00058) | :heavy_minus_sign: |
| HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://zxz267.github.io/HaMuCo/) <br /> [![GitHub](https://img.shields.io/github/stars/zxz267/HaMuCo)](https://github.com/zxz267/HaMuCo) | [![arXiv](https://img.shields.io/badge/arXiv-2302.00988-b31b1b.svg)](https://arxiv.org/abs/2302.00988) | :heavy_minus_sign: |
| ReactioNet: Learning High-Order Facial Behavior from Universal Stimulus-Reaction by Dyadic Relation Reasoning | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| CLIP-Cluster: CLIP-Guided Attribute Hallucination for Face Clustering | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Learning Human Dynamics in Autonomous Driving Scenarios | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation | [![GitHub](https://img.shields.io/github/stars/zyhbili/LivelySpeaker)](https://github.com/zyhbili/LivelySpeaker) | [![arXiv](https://img.shields.io/badge/arXiv-2309.09294-b31b1b.svg)](https://arxiv.org/abs/2309.09294) | :heavy_minus_sign: |
| Controllable Guide-Space for Generalizable Face Forgery Detection | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2307.14039-b31b1b.svg)](https://arxiv.org/abs/2307.14039) | :heavy_minus_sign: |
| Unpaired Multi-Domain Attribute Translation of 3D Facial Shapes with a Square and Symmetric Geometric Map | [![GitHub](https://img.shields.io/github/stars/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap)](https://github.com/NaughtyZZ/3D_facial_shape_attribute_translation_ssgmap) | [![arXiv](https://img.shields.io/badge/arXiv-2308.13245-b31b1b.svg)](https://arxiv.org/abs/2308.13245) | :heavy_minus_sign: |
| Emotional Listener Portrait: Neural Listener Head Generation with Emotion | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Steered Diffusion: A Generalized Framework for Plug-and-Play Conditional Image Synthesis | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Invariant Feature Regularization for Fair Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Gloss-Free Sign Language Translation: Improving from Visual-Language Pretraining | [![GitHub](https://img.shields.io/github/stars/zhoubenjia/GFSLT-VLP)](https://github.com/zhoubenjia/GFSLT-VLP) | [![arXiv](https://img.shields.io/badge/arXiv-2307.14768-b31b1b.svg)](https://arxiv.org/abs/2307.14768) | :heavy_minus_sign: |
| Contrastive Pseudo Learning for Open-World DeepFake Attribution | [![GitHub](https://img.shields.io/github/stars/TencentYoutuResearch/OpenWorld-DeepFakeAttribution)](https://github.com/TencentYoutuResearch/OpenWorld-DeepFakeAttribution) | [![arXiv](https://img.shields.io/badge/arXiv-2309.11132-b31b1b.svg)](https://arxiv.org/abs/2309.11132) | :heavy_minus_sign: |
| Continual Learning for Personalized Co-Speech Gesture Generation | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| HandR2N2: Iterative 3D Hand Pose Estimation using a Residual Recurrent Neural Network | [![GitHub](https://img.shields.io/github/stars/cwc1260/HandR2N2)](https://github.com/cwc1260/HandR2N2) | :heavy_minus_sign: | :heavy_minus_sign: |
| SPACE: Speech-Driven Portrait Animation with Controllable Expression | :heavy_minus_sign: | [![arXiv](https://img.shields.io/badge/arXiv-2211.09809-b31b1b.svg)](https://arxiv.org/abs/2211.09809) | :heavy_minus_sign: |
| How to Boost Face Recognition with StyleGAN? | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://seva100.github.io/stylegan-for-facerec) <br /> [![GitHub](https://img.shields.io/github/stars/seva100/stylegan-for-facerec)](https://github.com/seva100/stylegan-for-facerec) | [![arXiv](https://img.shields.io/badge/arXiv-2210.10090-b31b1b.svg)](https://arxiv.org/abs/2210.10090) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=Bsi0RMTdEaI) |
| ChildPlay: A New Benchmark for Understanding Children's Gaze Behaviour | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.idiap.ch/en/dataset/childplay-gaze) <br /> [![Zenodo](https://img.shields.io/badge/Zenodo-dataset-FFD1BF.svg)](https://zenodo.org/record/8252535) | [![arXiv](https://img.shields.io/badge/arXiv-2307.01630-b31b1b.svg)](https://arxiv.org/abs/2307.01630) | :heavy_minus_sign: |
| Robust One-Shot Face Video Re-Enactment using Hybrid Latent Spaces of StyleGAN2 | [![GitHub Page](https://img.shields.io/badge/GitHub-Page-159957.svg)](https://trevineoorloff.github.io/FaceVideoReenactment_HybridLatents.io/) | [![arXiv](https://img.shields.io/badge/arXiv-2302.07848-b31b1b.svg)](https://arxiv.org/abs/2302.07848) | :heavy_minus_sign: |
| Data-Free Class-Incremental Hand Gesture Recognition | [![Pdf](https://img.shields.io/badge/pdf-version-003B10.svg)](http://humansensing.cs.cmu.edu/sites/default/files/Data-Free%20Class-Incremental%20Hand%20Gesture%20Recognition_0.pdf) | [![GitHub](https://img.shields.io/github/stars/humansensinglab/dfcil-hgr)](https://github.com/humansensinglab/dfcil-hgr) | :heavy_minus_sign: |
| Learning Robust Representations with Information Bottleneck and Memory Network for RGB-D-based Gesture Recognition | [![GitHub](https://img.shields.io/github/stars/Carpumpkin/InBoMem)](https://github.com/Carpumpkin/InBoMem) | :heavy_minus_sign: | :heavy_minus_sign: |
| Knowledge-Spreader: Learning Semi-Supervised Facial Action Dynamics by Consistifying Knowledge Granularity | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| Face Clustering via Graph Convolutional Networks with Confidence Edges | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| StyleGANEX: StyleGAN-based Manipulation Beyond Cropped Aligned Faces | [![WEB Page](https://img.shields.io/badge/WEB-Page-159957.svg)](https://www.mmlab-ntu.com/project/styleganex/) <br /> [![GitHub](https://img.shields.io/github/stars/williamyang1991/StyleGANEX)](https://github.com/williamyang1991/StyleGANEX) | [![arXiv](https://img.shields.io/badge/arXiv-2303.06146-b31b1b.svg)](https://arxiv.org/abs/2303.06146) | [![YouTube](https://img.shields.io/badge/YouTube-%23FF0000.svg?style=for-the-badge&logo=YouTube&logoColor=white)](https://www.youtube.com/watch?v=8oK0TXQmxg8) |
| SeeABLE: Soft Discrepancies and Bounded Contrastive Learning for Exposing Deepfakes | [![GitHub](https://img.shields.io/github/stars/anonymous-author-sub/seeable)](https://github.com/anonymous-author-sub/seeable) | [![arXiv](https://img.shields.io/badge/arXiv-2211.11296-b31b1b.svg)](https://arxiv.org/abs/2211.11296) | :heavy_minus_sign: |
| Adaptive Nonlinear Latent Transformation for Conditional Face Editing | [![GitHub](https://img.shields.io/github/stars/Hzzone/AdaTrans)](https://github.com/Hzzone/AdaTrans) | [![arXiv](https://img.shields.io/badge/arXiv-2307.07790-b31b1b.svg)](https://arxiv.org/abs/2307.07790) | :heavy_minus_sign: |
| Semi-Supervised Speech-Driven 3D Facial Animation via Cross-Modal Encoding | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| ICD-Face: Intra-Class Compactness Distillation for Face Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |
| C2ST: Cross-Modal Contextualized Sequence Transduction for Continuous Sign Language Recognition | :heavy_minus_sign: | :heavy_minus_sign: | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Medical and Biological Vision; Cell Microscopy

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| BoMD: Bag of Multi-Label Local Descriptors for Noisy Chest X-Ray Classification | [![GitHub](https://img.shields.io/github/stars/cyh-0/BoMD)](https://github.com/cyh-0/BoMD) | [![arXiv](https://img.shields.io/badge/arXiv-2203.01937-b31b1b.svg)](https://arxiv.org/abs/2203.01937) | :heavy_minus_sign: |
| CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection | [![GitHub](https://img.shields.io/github/stars/ljwztc/CLIP-Driven-Universal-Model)](https://github.com/ljwztc/CLIP-Driven-Universal-Model) | [![arXiv](https://img.shields.io/badge/arXiv-2301.00785-b31b1b.svg)](https://arxiv.org/abs/2301.00785) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Scene Analysis and Understanding

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Multimodal Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Human-in-the-Loop Computer Vision

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Image and Video Forensics

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Geometric Deep Learning

> Will soon be added

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Vision Applications and Systems

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing | [![GitHub](https://img.shields.io/github/stars/aimagelab/multimodal-garment-designer)](https://github.com/aimagelab/multimodal-garment-designer) | [![arXiv](https://img.shields.io/badge/arXiv-2304.02051-b31b1b.svg)](https://arxiv.org/abs/2304.02051) | :heavy_minus_sign: |

<a href="#sections">
  <img src="https://cdn.jsdelivr.net/gh/DmitryRyumin/NewEraAI-Papers@main/images/top.svg" />
</a>

### Machine Learning and Dataset

| **Title** | **Repo** | **Paper** | **Video** |
|-----------|:--------:|:---------:|:---------:|
| Unmasked Teacher: Towards Training-Efficient Video Foundation Models | [![GitHub](https://img.shields.io/github/stars/OpenGVLab/unmasked_teacher)](https://github.com/OpenGVLab/unmasked_teacher) | [![arXiv](https://img.shields.io/badge/arXiv-2303.16058-b31b1b.svg)](https://arxiv.org/abs/2303.16058) | :heavy_minus_sign: | -->

---

## Star History

<p align="center">
    <a href="https://star-history.com/#Dmitryryumin/ICCV-2023-Papers&Date" target="_blank">
        <img width="500" src="https://api.star-history.com/svg?repos=Dmitryryumin/ICCV-2023-Papers&type=Date" alt="Star History Chart">
    </a>
<p>
